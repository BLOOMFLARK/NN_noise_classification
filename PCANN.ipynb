{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "PCANN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BLOOMFLARK/NN_noise_classification/blob/master/PCANN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8az9TUBPNn3",
        "colab_type": "code",
        "outputId": "b948f831-ac03-4ab9-c9dc-5acca6d53da7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZOtN3-sW09u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# написать функцию fit в класс PCANet, который бы обучал модель"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZOKuZLzPKT1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn.functional as F\n",
        "from sklearn.decomposition import PCA, IncrementalPCA\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torchvision import transforms, models, datasets\n",
        "\n",
        "import random\n",
        "\n",
        "\n",
        "class PCANet(torch.nn.Module):\n",
        "    def __init__(self, num_filters: list, filters_sizes: list, batch_size=256):\n",
        "        super(PCANet, self).__init__()\n",
        "        self.params = {\n",
        "            'num_filters': num_filters,\n",
        "            'filters_sizes': filters_sizes,\n",
        "        }\n",
        "        self.W_1 = None\n",
        "        self.pool1 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.act1 = torch.nn.ReLU()\n",
        "        self.W_2 = None\n",
        "        # self.pool2 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc = torch.nn.Linear(90, 10, bias=True)\n",
        "        #self.fc = torch.nn.Linear(30250, 2, bias=True)\n",
        "        # self.fc2 = torch.nn.Linear(100, 2, bias=True)\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.conv2d(x, self.W_1)\n",
        "        x = self.act1(x)\n",
        "        x = self.pool1(x)\n",
        "        N1, C1, H1, W1 = x.shape\n",
        "\n",
        "        x = F.conv2d(x, self.W_2)\n",
        "        x = self.act1(x)\n",
        "        x = self.pool1(x)\n",
        "        N, C, H, W = x.shape\n",
        "\n",
        "        x_flat = x.view(N, C * H * W)\n",
        "\n",
        "        x_flat = self.fc(x_flat)\n",
        "        # x_flat = self.act1(x_flat)\n",
        "        # x_flat = self.fc2(x_flat)\n",
        "        return x_flat\n",
        "            \n",
        "    @staticmethod        \n",
        "    def _extract_image_patches(imgs: torch.Tensor, filter_size, stride=1, remove_mean=True):\n",
        "        # imgs.shape = (N, C, H, W) -> (N, 1, H, W) \n",
        "        # так должно быть, но сюда могут прийти не grayscale изображения первого шага, а со второго\n",
        "        # на котором применено L1 фильтров -> L1 каналов\n",
        "        N, n_channels, H, W = imgs.shape\n",
        "        \n",
        "        if n_channels > 1:\n",
        "            # изображение вида (N, C, H, W) - N C-канальных изображений\n",
        "            # приводим к виду (N*C, 1, H, W) - N*C одно-канальных изображений\n",
        "            imgs = imgs.view(-1, 1, H, W)\n",
        "        print('images shape', imgs.shape)\n",
        "            \n",
        "        k = filter_size\n",
        "        patches = torch.nn.functional.unfold(imgs, k, padding=k//2) # (N, k^2, H*W)\n",
        "        print('patches_shape, ', patches.shape)\n",
        "        print('should be patches shape, ', (imgs.shape[0], k**2, H*W))\n",
        "        \n",
        "        if remove_mean:\n",
        "            patches -= patches.mean(dim=1, keepdim=True) # последнее измерение - количество патчей\n",
        "        \n",
        "        print('filter_size', k)\n",
        "        X = patches.view(k**2, -1) # (k^2, N*H*W)\n",
        "\n",
        "        return X\n",
        "    \n",
        "    def _convolve(self, imgs: torch.Tensor, filter_bank: torch.Tensor) -> torch.Tensor:\n",
        "        weight = filter_bank\n",
        "        output = F.conv2d(imgs, weight) #, padding=padding)\n",
        "        return output\n",
        "    \n",
        "    def _first_stage(self, imgs: torch.Tensor, train: bool) -> torch.Tensor:\n",
        "        # (N, C, H, W) image\n",
        "        # (train_size, 1, H, W) - grayscale\n",
        "        assert imgs.dim() == 4 and imgs.nelement() > 0\n",
        "\n",
        "        print('PCANet first stage...')\n",
        "\n",
        "        if train:\n",
        "            # достаем все патчи из всех N изображений\n",
        "            filter_size1 = self.params['filters_sizes'][0]\n",
        "            X = self._extract_image_patches(\n",
        "                imgs, filter_size1)\n",
        "            \n",
        "            n_filters = self.params['num_filters'][0]\n",
        "            \n",
        "            eigenvectors = self.get_pca_eigenvectors(X, n_components=n_filters, batch_size=self.batch_size)\n",
        "            self.W_1 = torch.FloatTensor(eigenvectors).view(n_filters, 1, filter_size1, filter_size1)\n",
        "         \n",
        "        I = self._convolve(imgs, self.W_1)  # (N, 1, H, W) * (L1, k1, k1) -> (N, L1, H', W')\n",
        "        return I\n",
        "    \n",
        "    @staticmethod\n",
        "    def conv_output_size(w, filter_size, padding=0, stride=1):\n",
        "        return int((w - filter_size + 2 * padding) / stride + 1)\n",
        "    \n",
        "    @staticmethod\n",
        "    def get_pca_eigenvectors(X, n_components, batch_size=100):\n",
        "        ipca = IncrementalPCA(n_components=n_components, batch_size=batch_size)\n",
        "        print('pca fitting ...')\n",
        "        ipca.fit(X @ X.t())\n",
        "        eigenvectors = ipca.components_\n",
        "        print('eigenvectors shape:', eigenvectors.shape)\n",
        "        return eigenvectors\n",
        "        \n",
        "    def _second_stage(self, I: torch.Tensor, train):\n",
        "        print('PCANet second stage...')\n",
        "        # I: (N, L1, H, W)\n",
        "        if train:\n",
        "            N, L1, H, W = I.shape\n",
        "            filter_size2 = self.params['filters_sizes'][1]\n",
        "            n_filters2 = self.params['num_filters'][1]\n",
        "            n_filters1 = self.params['num_filters'][0]\n",
        "            \n",
        "            H_new = self.conv_output_size(I.shape[2], filter_size2)\n",
        "            W_new = self.conv_output_size(I.shape[3], filter_size2)\n",
        "            \n",
        "            X = self._extract_image_patches(I, filter_size2)\n",
        "            print('X_SHAPE ', X.shape)\n",
        "            eigenvectors = self.get_pca_eigenvectors(X, n_components=n_filters2, batch_size=self.batch_size)\n",
        "            W_2 = torch.FloatTensor(eigenvectors).view(n_filters2, 1, filter_size2, filter_size2) # (L2, 1, k2, k2)\n",
        "            self.W_2 = W_2.repeat(1, n_filters1, 1, 1) # (L2, L1, k2, k2) - повторяет L1 раз для конкретного l из L2\n",
        "        return self._convolve(I, self.W_2)\n",
        "    \n",
        "    def run(self, images):\n",
        "        # Создаем фильтры\n",
        "        # images: (N, 1, H, W)\n",
        "        I = self._first_stage(images, train=True)\n",
        "        print(\"I \", I.shape)\n",
        "        II = self._second_stage(I, train=True)\n",
        "        N, C, H, W = II.shape\n",
        "        # self.fc = torch.nn.Linear(H * W, 2, bias=True)\n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvnnipQMXgkv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f45b2cee-1af4-4454-d7aa-e4d35fe15dfe"
      },
      "source": [
        "os.listdir()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config', 'drive', 'sample_data']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6otLLXYXbuF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MNIST_train = datasets.MNIST('sample_data', train=True, transform=None, target_transform=None, download=True)\n",
        "MNIST_test = datasets.MNIST('sample_data', train=False, transform=None, target_transform=None, download=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWuAKLOkYkqI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = MNIST_train.data[:11000, :, :]\n",
        "y_train = MNIST_train.targets[:11000]\n",
        "X_test = MNIST_test.data[:1650, :, :]\n",
        "y_test = MNIST_test.targets[:1650]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MknGL-6ZpiU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.unsqueeze(1).float()\n",
        "X_test = X_test.unsqueeze(1).float()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhrhsXknZBsU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "03ffdc33-0955-4045-8667-978f0ab32081"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([11000, 1, 28, 28])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVE991rqaFmq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fb89aae7-abd7-406d-ad7b-b1f96f637539"
      },
      "source": [
        "X_test.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1650, 1, 28, 28])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epZV8ZyHaJyw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aed3e423-6051-4aec-ede0-b25457bf3f30"
      },
      "source": [
        "y_test.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1650])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ag7yTvPNPKT7",
        "colab_type": "code",
        "outputId": "1b8615e8-65ae-4348-b4e8-910b8da3b9aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "net = PCANet([5, 10], [3, 8])\n",
        "net.params\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "net = net.to(device)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9cpvOhiPbwl",
        "colab_type": "code",
        "outputId": "f2a31066-2218-4ee1-a5f9-e4c0b784d135",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data_root = 'drive/My Drive/dl_noise_classification/data/'\n",
        "train_dir = 'train'\n",
        "val_dir = 'val'\n",
        "test_dir = 'test'\n",
        "class_names = ['awgn', 'bayer']\n",
        "\n",
        "train_dir = os.path.join(data_root, train_dir)\n",
        "val_dir = os.path.join(data_root, val_dir)\n",
        "test_dir = os.path.join(data_root, test_dir)\n",
        "\n",
        "train_dir"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'drive/My Drive/dl_noise_classification/data/train'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5GHuftbPKUC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "num_epochs = 1500\n",
        "batch_size = 150\n",
        "lr = 1e-4\n",
        "\n",
        "loss = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "#optimizer = torch.optim.SGD(net.parameters(), lr=lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RdNqCUxPKUG",
        "colab_type": "code",
        "outputId": "82f32cc6-3791-48dc-c402-2356cfcb520a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# МНОГО ПРОЦЕССОВ ЖРУТ ПАМЯТЬ ОЧЕНЬ МНОГО, при этом сильно не ускоряют\n",
        "train_transforms = transforms.Compose([\n",
        "    #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "val_transforms = transforms.Compose([\n",
        "    #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "\n",
        "train_dataset = datasets.ImageFolder(train_dir, train_transforms)\n",
        "val_dataset = datasets.ImageFolder(val_dir, val_transforms)\n",
        "test_dataset = datasets.ImageFolder(test_dir, train_transforms)\n",
        "\n",
        "train_size = len(train_dataset)\n",
        "val_size = len(val_dataset)\n",
        "test_size = len(test_dataset)\n",
        "\n",
        "all_train = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=train_size, shuffle=False, num_workers=batch_size)\n",
        "\n",
        "#train_dataloader = torch.utils.data.DataLoader(\n",
        "#    train_dataset, batch_size=batch_size, shuffle=True, num_workers=batch_size)\n",
        "\n",
        "#val_dataloader = torch.utils.data.DataLoader(\n",
        "#    val_dataset, batch_size=batch_size, shuffle=False, num_workers=batch_size)\n",
        "\n",
        "all_val = torch.utils.data.DataLoader(\n",
        "    val_dataset, batch_size=train_size, shuffle=False, num_workers=val_size)\n",
        "\n",
        "all_test = torch.utils.data.DataLoader(\n",
        "    test_dataset, batch_size=train_size, shuffle=False, num_workers=test_size)\n",
        "print(\"TRAIN_SIZE: {}\\n VAL_SIZE: {}\\nTEST_SIZE: {}\".format(train_size, val_size, test_size))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN_SIZE: 900\n",
            " VAL_SIZE: 300\n",
            "TEST_SIZE: 300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJj_d24c-VB7",
        "colab_type": "code",
        "outputId": "52d106d9-248a-46a8-8ebe-e086272710d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "%%time\n",
        "#print('all val data loading ...')\n",
        "#val_data = next(iter(all_val))\n",
        "print('all train data loading ...')\n",
        "train_data = next(iter(all_train))\n",
        "#print('test data loading')\n",
        "#test_data = next(iter(all_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all train data loading ...\n",
            "all train data loading ...\n",
            "CPU times: user 243 ms, sys: 3.45 s, total: 3.7 s\n",
            "Wall time: 4min 46s\n",
            "CPU times: user 243 ms, sys: 3.45 s, total: 3.7 s\n",
            "Wall time: 4min 46s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dg3NGt_cbfo",
        "colab_type": "code",
        "outputId": "87234b56-100c-426c-a74e-72e267586dce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "# net.run(train_data[0])\n",
        "net.run(X_train)\n",
        "#del train_data"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PCANet first stage...\n",
            "images shape torch.Size([11000, 1, 28, 28])\n",
            "patches_shape,  torch.Size([11000, 9, 784])\n",
            "should be patches shape,  (11000, 9, 784)\n",
            "filter_size 3\n",
            "pca fitting ...\n",
            "eigenvectors shape: (5, 9)\n",
            "I  torch.Size([11000, 5, 26, 26])\n",
            "PCANet second stage...\n",
            "images shape torch.Size([55000, 1, 26, 26])\n",
            "patches_shape,  torch.Size([55000, 64, 729])\n",
            "should be patches shape,  (55000, 64, 676)\n",
            "filter_size 8\n",
            "X_SHAPE  torch.Size([64, 40095000])\n",
            "pca fitting ...\n",
            "eigenvectors shape: (10, 64)\n",
            "CPU times: user 21.6 s, sys: 107 ms, total: 21.7 s\n",
            "Wall time: 13.6 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0BnHdqtc9WX",
        "colab_type": "code",
        "outputId": "86acf491-5d6b-4ea9-e607-721fc13ae347",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "print('test data loading')\n",
        "test_data = next(iter(all_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test data loading\n",
            "CPU times: user 650 ms, sys: 19.6 s, total: 20.2 s\n",
            "Wall time: 2min 24s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5ZrPROoc_H3",
        "colab_type": "code",
        "outputId": "0b35a314-5385-455a-84a4-dce20e3392f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "print('all val data loading ...')\n",
        "val_data = next(iter(all_val))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all val data loading ...\n",
            "CPU times: user 714 ms, sys: 19.7 s, total: 20.4 s\n",
            "Wall time: 1min 58s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHhiI3-0U3PQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from time import time\n",
        "\n",
        "val_loss = np.zeros(num_epochs)\n",
        "train_loss = np.zeros(num_epochs)\n",
        "\n",
        "val_acc = np.zeros(num_epochs)\n",
        "train_acc = np.zeros(num_epochs)\n",
        "\n",
        "\n",
        "def train_model(model, loss, optimizer, scheduler, num_epochs):\n",
        "    #print('Making filters....')\n",
        "    #t_start = time()\n",
        "    #model.run(train_data[0])\n",
        "    #print('TIME: ', time() - t_start)\n",
        "\n",
        "\n",
        "    model.W_1 = model.W_1.to(device)\n",
        "    model.W_2 = model.W_2.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        print('Epoch {}/{}:'.format(epoch, num_epochs - 1), flush=True)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                #data = train_data\n",
        "                data = (X_train, y_train)\n",
        "                model.train()  # training mode\n",
        "                history_acc = train_acc\n",
        "                history_loss = train_loss\n",
        "            else:\n",
        "                #data = test_data\n",
        "                data = (X_test, y_test)\n",
        "                model.eval()   # evaluate mode (dropout + bn)\n",
        "                history_acc = val_acc\n",
        "                history_loss = val_loss\n",
        "\n",
        "            running_loss = 0.\n",
        "            running_acc = 0.\n",
        "\n",
        "            # Iterate over data.\n",
        "            inputs, labels = data\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward and backward\n",
        "            with torch.set_grad_enabled(phase=='train'):\n",
        "                preds = model(inputs)\n",
        "                loss_value = loss(preds, labels)\n",
        "                preds_class = preds.argmax(dim=1)\n",
        "\n",
        "                # backward + optimize only if in training phase\n",
        "                if phase == 'train':\n",
        "                    loss_value.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "            # statistics\n",
        "            running_loss += loss_value.cpu().item()\n",
        "            running_acc += (preds_class.cpu() == labels.cpu().data).float().mean()\n",
        "            \n",
        "            epoch_loss = running_loss\n",
        "            epoch_acc = running_acc\n",
        "            history_acc[epoch] = epoch_acc\n",
        "            history_loss[epoch] = epoch_loss\n",
        "\n",
        "            # запоминаем модель по лоссу\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}\\n'.format(phase, epoch_loss, epoch_acc), flush=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYahGgdxPKUM",
        "colab_type": "code",
        "outputId": "138fa02c-5f79-40c5-fa48-5858cae55d9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "train_model(net, loss, optimizer, None, num_epochs=num_epochs)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "val Loss: 14.0617 Acc: 0.5321\n",
            "\n",
            "Epoch 501/1499:\n",
            "train Loss: 12.2414 Acc: 0.5588\n",
            "\n",
            "val Loss: 14.0347 Acc: 0.5333\n",
            "\n",
            "Epoch 502/1499:\n",
            "train Loss: 12.2161 Acc: 0.5592\n",
            "\n",
            "val Loss: 14.0078 Acc: 0.5333\n",
            "\n",
            "Epoch 503/1499:\n",
            "train Loss: 12.1908 Acc: 0.5598\n",
            "\n",
            "val Loss: 13.9810 Acc: 0.5339\n",
            "\n",
            "Epoch 504/1499:\n",
            "train Loss: 12.1656 Acc: 0.5600\n",
            "\n",
            "val Loss: 13.9543 Acc: 0.5358\n",
            "\n",
            "Epoch 505/1499:\n",
            "train Loss: 12.1404 Acc: 0.5604\n",
            "\n",
            "val Loss: 13.9277 Acc: 0.5370\n",
            "\n",
            "Epoch 506/1499:\n",
            "train Loss: 12.1154 Acc: 0.5605\n",
            "\n",
            "val Loss: 13.9012 Acc: 0.5370\n",
            "\n",
            "Epoch 507/1499:\n",
            "train Loss: 12.0905 Acc: 0.5613\n",
            "\n",
            "val Loss: 13.8748 Acc: 0.5370\n",
            "\n",
            "Epoch 508/1499:\n",
            "train Loss: 12.0656 Acc: 0.5617\n",
            "\n",
            "val Loss: 13.8484 Acc: 0.5376\n",
            "\n",
            "Epoch 509/1499:\n",
            "train Loss: 12.0409 Acc: 0.5619\n",
            "\n",
            "val Loss: 13.8222 Acc: 0.5376\n",
            "\n",
            "Epoch 510/1499:\n",
            "train Loss: 12.0162 Acc: 0.5624\n",
            "\n",
            "val Loss: 13.7961 Acc: 0.5376\n",
            "\n",
            "Epoch 511/1499:\n",
            "train Loss: 11.9916 Acc: 0.5628\n",
            "\n",
            "val Loss: 13.7700 Acc: 0.5376\n",
            "\n",
            "Epoch 512/1499:\n",
            "train Loss: 11.9671 Acc: 0.5631\n",
            "\n",
            "val Loss: 13.7440 Acc: 0.5376\n",
            "\n",
            "Epoch 513/1499:\n",
            "train Loss: 11.9427 Acc: 0.5638\n",
            "\n",
            "val Loss: 13.7181 Acc: 0.5382\n",
            "\n",
            "Epoch 514/1499:\n",
            "train Loss: 11.9184 Acc: 0.5644\n",
            "\n",
            "val Loss: 13.6923 Acc: 0.5382\n",
            "\n",
            "Epoch 515/1499:\n",
            "train Loss: 11.8942 Acc: 0.5645\n",
            "\n",
            "val Loss: 13.6666 Acc: 0.5382\n",
            "\n",
            "Epoch 516/1499:\n",
            "train Loss: 11.8700 Acc: 0.5652\n",
            "\n",
            "val Loss: 13.6410 Acc: 0.5388\n",
            "\n",
            "Epoch 517/1499:\n",
            "train Loss: 11.8460 Acc: 0.5658\n",
            "\n",
            "val Loss: 13.6155 Acc: 0.5388\n",
            "\n",
            "Epoch 518/1499:\n",
            "train Loss: 11.8220 Acc: 0.5663\n",
            "\n",
            "val Loss: 13.5901 Acc: 0.5388\n",
            "\n",
            "Epoch 519/1499:\n",
            "train Loss: 11.7981 Acc: 0.5666\n",
            "\n",
            "val Loss: 13.5648 Acc: 0.5400\n",
            "\n",
            "Epoch 520/1499:\n",
            "train Loss: 11.7743 Acc: 0.5669\n",
            "\n",
            "val Loss: 13.5395 Acc: 0.5424\n",
            "\n",
            "Epoch 521/1499:\n",
            "train Loss: 11.7506 Acc: 0.5675\n",
            "\n",
            "val Loss: 13.5144 Acc: 0.5436\n",
            "\n",
            "Epoch 522/1499:\n",
            "train Loss: 11.7270 Acc: 0.5678\n",
            "\n",
            "val Loss: 13.4893 Acc: 0.5442\n",
            "\n",
            "Epoch 523/1499:\n",
            "train Loss: 11.7034 Acc: 0.5688\n",
            "\n",
            "val Loss: 13.4643 Acc: 0.5442\n",
            "\n",
            "Epoch 524/1499:\n",
            "train Loss: 11.6800 Acc: 0.5693\n",
            "\n",
            "val Loss: 13.4395 Acc: 0.5442\n",
            "\n",
            "Epoch 525/1499:\n",
            "train Loss: 11.6566 Acc: 0.5695\n",
            "\n",
            "val Loss: 13.4147 Acc: 0.5455\n",
            "\n",
            "Epoch 526/1499:\n",
            "train Loss: 11.6334 Acc: 0.5699\n",
            "\n",
            "val Loss: 13.3900 Acc: 0.5455\n",
            "\n",
            "Epoch 527/1499:\n",
            "train Loss: 11.6102 Acc: 0.5701\n",
            "\n",
            "val Loss: 13.3654 Acc: 0.5461\n",
            "\n",
            "Epoch 528/1499:\n",
            "train Loss: 11.5871 Acc: 0.5706\n",
            "\n",
            "val Loss: 13.3409 Acc: 0.5461\n",
            "\n",
            "Epoch 529/1499:\n",
            "train Loss: 11.5641 Acc: 0.5711\n",
            "\n",
            "val Loss: 13.3165 Acc: 0.5461\n",
            "\n",
            "Epoch 530/1499:\n",
            "train Loss: 11.5412 Acc: 0.5719\n",
            "\n",
            "val Loss: 13.2922 Acc: 0.5467\n",
            "\n",
            "Epoch 531/1499:\n",
            "train Loss: 11.5183 Acc: 0.5721\n",
            "\n",
            "val Loss: 13.2680 Acc: 0.5467\n",
            "\n",
            "Epoch 532/1499:\n",
            "train Loss: 11.4956 Acc: 0.5726\n",
            "\n",
            "val Loss: 13.2438 Acc: 0.5467\n",
            "\n",
            "Epoch 533/1499:\n",
            "train Loss: 11.4729 Acc: 0.5728\n",
            "\n",
            "val Loss: 13.2198 Acc: 0.5491\n",
            "\n",
            "Epoch 534/1499:\n",
            "train Loss: 11.4503 Acc: 0.5735\n",
            "\n",
            "val Loss: 13.1959 Acc: 0.5491\n",
            "\n",
            "Epoch 535/1499:\n",
            "train Loss: 11.4278 Acc: 0.5735\n",
            "\n",
            "val Loss: 13.1720 Acc: 0.5485\n",
            "\n",
            "Epoch 536/1499:\n",
            "train Loss: 11.4054 Acc: 0.5742\n",
            "\n",
            "val Loss: 13.1483 Acc: 0.5485\n",
            "\n",
            "Epoch 537/1499:\n",
            "train Loss: 11.3831 Acc: 0.5746\n",
            "\n",
            "val Loss: 13.1246 Acc: 0.5485\n",
            "\n",
            "Epoch 538/1499:\n",
            "train Loss: 11.3608 Acc: 0.5751\n",
            "\n",
            "val Loss: 13.1010 Acc: 0.5503\n",
            "\n",
            "Epoch 539/1499:\n",
            "train Loss: 11.3387 Acc: 0.5758\n",
            "\n",
            "val Loss: 13.0776 Acc: 0.5503\n",
            "\n",
            "Epoch 540/1499:\n",
            "train Loss: 11.3166 Acc: 0.5763\n",
            "\n",
            "val Loss: 13.0542 Acc: 0.5503\n",
            "\n",
            "Epoch 541/1499:\n",
            "train Loss: 11.2946 Acc: 0.5766\n",
            "\n",
            "val Loss: 13.0309 Acc: 0.5503\n",
            "\n",
            "Epoch 542/1499:\n",
            "train Loss: 11.2727 Acc: 0.5773\n",
            "\n",
            "val Loss: 13.0077 Acc: 0.5503\n",
            "\n",
            "Epoch 543/1499:\n",
            "train Loss: 11.2509 Acc: 0.5775\n",
            "\n",
            "val Loss: 12.9847 Acc: 0.5503\n",
            "\n",
            "Epoch 544/1499:\n",
            "train Loss: 11.2291 Acc: 0.5781\n",
            "\n",
            "val Loss: 12.9617 Acc: 0.5503\n",
            "\n",
            "Epoch 545/1499:\n",
            "train Loss: 11.2074 Acc: 0.5787\n",
            "\n",
            "val Loss: 12.9388 Acc: 0.5503\n",
            "\n",
            "Epoch 546/1499:\n",
            "train Loss: 11.1858 Acc: 0.5793\n",
            "\n",
            "val Loss: 12.9160 Acc: 0.5527\n",
            "\n",
            "Epoch 547/1499:\n",
            "train Loss: 11.1643 Acc: 0.5799\n",
            "\n",
            "val Loss: 12.8933 Acc: 0.5527\n",
            "\n",
            "Epoch 548/1499:\n",
            "train Loss: 11.1429 Acc: 0.5806\n",
            "\n",
            "val Loss: 12.8707 Acc: 0.5527\n",
            "\n",
            "Epoch 549/1499:\n",
            "train Loss: 11.1215 Acc: 0.5808\n",
            "\n",
            "val Loss: 12.8482 Acc: 0.5527\n",
            "\n",
            "Epoch 550/1499:\n",
            "train Loss: 11.1002 Acc: 0.5810\n",
            "\n",
            "val Loss: 12.8257 Acc: 0.5533\n",
            "\n",
            "Epoch 551/1499:\n",
            "train Loss: 11.0790 Acc: 0.5813\n",
            "\n",
            "val Loss: 12.8034 Acc: 0.5539\n",
            "\n",
            "Epoch 552/1499:\n",
            "train Loss: 11.0579 Acc: 0.5824\n",
            "\n",
            "val Loss: 12.7812 Acc: 0.5539\n",
            "\n",
            "Epoch 553/1499:\n",
            "train Loss: 11.0368 Acc: 0.5825\n",
            "\n",
            "val Loss: 12.7590 Acc: 0.5545\n",
            "\n",
            "Epoch 554/1499:\n",
            "train Loss: 11.0159 Acc: 0.5827\n",
            "\n",
            "val Loss: 12.7370 Acc: 0.5552\n",
            "\n",
            "Epoch 555/1499:\n",
            "train Loss: 10.9950 Acc: 0.5835\n",
            "\n",
            "val Loss: 12.7150 Acc: 0.5558\n",
            "\n",
            "Epoch 556/1499:\n",
            "train Loss: 10.9741 Acc: 0.5837\n",
            "\n",
            "val Loss: 12.6932 Acc: 0.5558\n",
            "\n",
            "Epoch 557/1499:\n",
            "train Loss: 10.9534 Acc: 0.5842\n",
            "\n",
            "val Loss: 12.6714 Acc: 0.5558\n",
            "\n",
            "Epoch 558/1499:\n",
            "train Loss: 10.9327 Acc: 0.5842\n",
            "\n",
            "val Loss: 12.6497 Acc: 0.5564\n",
            "\n",
            "Epoch 559/1499:\n",
            "train Loss: 10.9121 Acc: 0.5845\n",
            "\n",
            "val Loss: 12.6281 Acc: 0.5576\n",
            "\n",
            "Epoch 560/1499:\n",
            "train Loss: 10.8916 Acc: 0.5848\n",
            "\n",
            "val Loss: 12.6066 Acc: 0.5582\n",
            "\n",
            "Epoch 561/1499:\n",
            "train Loss: 10.8711 Acc: 0.5854\n",
            "\n",
            "val Loss: 12.5852 Acc: 0.5588\n",
            "\n",
            "Epoch 562/1499:\n",
            "train Loss: 10.8508 Acc: 0.5855\n",
            "\n",
            "val Loss: 12.5638 Acc: 0.5588\n",
            "\n",
            "Epoch 563/1499:\n",
            "train Loss: 10.8305 Acc: 0.5861\n",
            "\n",
            "val Loss: 12.5426 Acc: 0.5594\n",
            "\n",
            "Epoch 564/1499:\n",
            "train Loss: 10.8102 Acc: 0.5865\n",
            "\n",
            "val Loss: 12.5214 Acc: 0.5594\n",
            "\n",
            "Epoch 565/1499:\n",
            "train Loss: 10.7901 Acc: 0.5871\n",
            "\n",
            "val Loss: 12.5003 Acc: 0.5594\n",
            "\n",
            "Epoch 566/1499:\n",
            "train Loss: 10.7700 Acc: 0.5874\n",
            "\n",
            "val Loss: 12.4793 Acc: 0.5594\n",
            "\n",
            "Epoch 567/1499:\n",
            "train Loss: 10.7500 Acc: 0.5875\n",
            "\n",
            "val Loss: 12.4584 Acc: 0.5594\n",
            "\n",
            "Epoch 568/1499:\n",
            "train Loss: 10.7300 Acc: 0.5880\n",
            "\n",
            "val Loss: 12.4376 Acc: 0.5594\n",
            "\n",
            "Epoch 569/1499:\n",
            "train Loss: 10.7101 Acc: 0.5883\n",
            "\n",
            "val Loss: 12.4168 Acc: 0.5600\n",
            "\n",
            "Epoch 570/1499:\n",
            "train Loss: 10.6903 Acc: 0.5888\n",
            "\n",
            "val Loss: 12.3961 Acc: 0.5606\n",
            "\n",
            "Epoch 571/1499:\n",
            "train Loss: 10.6706 Acc: 0.5890\n",
            "\n",
            "val Loss: 12.3755 Acc: 0.5624\n",
            "\n",
            "Epoch 572/1499:\n",
            "train Loss: 10.6509 Acc: 0.5894\n",
            "\n",
            "val Loss: 12.3550 Acc: 0.5624\n",
            "\n",
            "Epoch 573/1499:\n",
            "train Loss: 10.6313 Acc: 0.5896\n",
            "\n",
            "val Loss: 12.3345 Acc: 0.5618\n",
            "\n",
            "Epoch 574/1499:\n",
            "train Loss: 10.6118 Acc: 0.5901\n",
            "\n",
            "val Loss: 12.3141 Acc: 0.5618\n",
            "\n",
            "Epoch 575/1499:\n",
            "train Loss: 10.5923 Acc: 0.5906\n",
            "\n",
            "val Loss: 12.2938 Acc: 0.5624\n",
            "\n",
            "Epoch 576/1499:\n",
            "train Loss: 10.5729 Acc: 0.5909\n",
            "\n",
            "val Loss: 12.2736 Acc: 0.5630\n",
            "\n",
            "Epoch 577/1499:\n",
            "train Loss: 10.5535 Acc: 0.5915\n",
            "\n",
            "val Loss: 12.2534 Acc: 0.5630\n",
            "\n",
            "Epoch 578/1499:\n",
            "train Loss: 10.5342 Acc: 0.5918\n",
            "\n",
            "val Loss: 12.2333 Acc: 0.5636\n",
            "\n",
            "Epoch 579/1499:\n",
            "train Loss: 10.5150 Acc: 0.5924\n",
            "\n",
            "val Loss: 12.2132 Acc: 0.5648\n",
            "\n",
            "Epoch 580/1499:\n",
            "train Loss: 10.4959 Acc: 0.5925\n",
            "\n",
            "val Loss: 12.1932 Acc: 0.5648\n",
            "\n",
            "Epoch 581/1499:\n",
            "train Loss: 10.4768 Acc: 0.5929\n",
            "\n",
            "val Loss: 12.1733 Acc: 0.5648\n",
            "\n",
            "Epoch 582/1499:\n",
            "train Loss: 10.4577 Acc: 0.5935\n",
            "\n",
            "val Loss: 12.1535 Acc: 0.5655\n",
            "\n",
            "Epoch 583/1499:\n",
            "train Loss: 10.4388 Acc: 0.5938\n",
            "\n",
            "val Loss: 12.1337 Acc: 0.5661\n",
            "\n",
            "Epoch 584/1499:\n",
            "train Loss: 10.4199 Acc: 0.5945\n",
            "\n",
            "val Loss: 12.1139 Acc: 0.5667\n",
            "\n",
            "Epoch 585/1499:\n",
            "train Loss: 10.4010 Acc: 0.5949\n",
            "\n",
            "val Loss: 12.0943 Acc: 0.5679\n",
            "\n",
            "Epoch 586/1499:\n",
            "train Loss: 10.3822 Acc: 0.5953\n",
            "\n",
            "val Loss: 12.0747 Acc: 0.5685\n",
            "\n",
            "Epoch 587/1499:\n",
            "train Loss: 10.3635 Acc: 0.5958\n",
            "\n",
            "val Loss: 12.0552 Acc: 0.5679\n",
            "\n",
            "Epoch 588/1499:\n",
            "train Loss: 10.3449 Acc: 0.5965\n",
            "\n",
            "val Loss: 12.0357 Acc: 0.5679\n",
            "\n",
            "Epoch 589/1499:\n",
            "train Loss: 10.3263 Acc: 0.5964\n",
            "\n",
            "val Loss: 12.0163 Acc: 0.5685\n",
            "\n",
            "Epoch 590/1499:\n",
            "train Loss: 10.3077 Acc: 0.5969\n",
            "\n",
            "val Loss: 11.9970 Acc: 0.5685\n",
            "\n",
            "Epoch 591/1499:\n",
            "train Loss: 10.2892 Acc: 0.5973\n",
            "\n",
            "val Loss: 11.9777 Acc: 0.5691\n",
            "\n",
            "Epoch 592/1499:\n",
            "train Loss: 10.2708 Acc: 0.5976\n",
            "\n",
            "val Loss: 11.9585 Acc: 0.5691\n",
            "\n",
            "Epoch 593/1499:\n",
            "train Loss: 10.2525 Acc: 0.5977\n",
            "\n",
            "val Loss: 11.9394 Acc: 0.5709\n",
            "\n",
            "Epoch 594/1499:\n",
            "train Loss: 10.2341 Acc: 0.5985\n",
            "\n",
            "val Loss: 11.9204 Acc: 0.5715\n",
            "\n",
            "Epoch 595/1499:\n",
            "train Loss: 10.2159 Acc: 0.5988\n",
            "\n",
            "val Loss: 11.9014 Acc: 0.5715\n",
            "\n",
            "Epoch 596/1499:\n",
            "train Loss: 10.1977 Acc: 0.5991\n",
            "\n",
            "val Loss: 11.8825 Acc: 0.5715\n",
            "\n",
            "Epoch 597/1499:\n",
            "train Loss: 10.1796 Acc: 0.5998\n",
            "\n",
            "val Loss: 11.8636 Acc: 0.5721\n",
            "\n",
            "Epoch 598/1499:\n",
            "train Loss: 10.1615 Acc: 0.6002\n",
            "\n",
            "val Loss: 11.8449 Acc: 0.5721\n",
            "\n",
            "Epoch 599/1499:\n",
            "train Loss: 10.1435 Acc: 0.6005\n",
            "\n",
            "val Loss: 11.8262 Acc: 0.5721\n",
            "\n",
            "Epoch 600/1499:\n",
            "train Loss: 10.1255 Acc: 0.6005\n",
            "\n",
            "val Loss: 11.8076 Acc: 0.5721\n",
            "\n",
            "Epoch 601/1499:\n",
            "train Loss: 10.1076 Acc: 0.6007\n",
            "\n",
            "val Loss: 11.7890 Acc: 0.5739\n",
            "\n",
            "Epoch 602/1499:\n",
            "train Loss: 10.0898 Acc: 0.6013\n",
            "\n",
            "val Loss: 11.7705 Acc: 0.5752\n",
            "\n",
            "Epoch 603/1499:\n",
            "train Loss: 10.0720 Acc: 0.6020\n",
            "\n",
            "val Loss: 11.7521 Acc: 0.5752\n",
            "\n",
            "Epoch 604/1499:\n",
            "train Loss: 10.0543 Acc: 0.6020\n",
            "\n",
            "val Loss: 11.7337 Acc: 0.5752\n",
            "\n",
            "Epoch 605/1499:\n",
            "train Loss: 10.0366 Acc: 0.6023\n",
            "\n",
            "val Loss: 11.7154 Acc: 0.5752\n",
            "\n",
            "Epoch 606/1499:\n",
            "train Loss: 10.0190 Acc: 0.6028\n",
            "\n",
            "val Loss: 11.6972 Acc: 0.5752\n",
            "\n",
            "Epoch 607/1499:\n",
            "train Loss: 10.0014 Acc: 0.6030\n",
            "\n",
            "val Loss: 11.6790 Acc: 0.5758\n",
            "\n",
            "Epoch 608/1499:\n",
            "train Loss: 9.9839 Acc: 0.6035\n",
            "\n",
            "val Loss: 11.6609 Acc: 0.5758\n",
            "\n",
            "Epoch 609/1499:\n",
            "train Loss: 9.9665 Acc: 0.6041\n",
            "\n",
            "val Loss: 11.6429 Acc: 0.5764\n",
            "\n",
            "Epoch 610/1499:\n",
            "train Loss: 9.9491 Acc: 0.6045\n",
            "\n",
            "val Loss: 11.6249 Acc: 0.5764\n",
            "\n",
            "Epoch 611/1499:\n",
            "train Loss: 9.9317 Acc: 0.6050\n",
            "\n",
            "val Loss: 11.6070 Acc: 0.5764\n",
            "\n",
            "Epoch 612/1499:\n",
            "train Loss: 9.9144 Acc: 0.6056\n",
            "\n",
            "val Loss: 11.5891 Acc: 0.5758\n",
            "\n",
            "Epoch 613/1499:\n",
            "train Loss: 9.8972 Acc: 0.6057\n",
            "\n",
            "val Loss: 11.5713 Acc: 0.5764\n",
            "\n",
            "Epoch 614/1499:\n",
            "train Loss: 9.8800 Acc: 0.6061\n",
            "\n",
            "val Loss: 11.5535 Acc: 0.5770\n",
            "\n",
            "Epoch 615/1499:\n",
            "train Loss: 9.8628 Acc: 0.6064\n",
            "\n",
            "val Loss: 11.5358 Acc: 0.5770\n",
            "\n",
            "Epoch 616/1499:\n",
            "train Loss: 9.8457 Acc: 0.6062\n",
            "\n",
            "val Loss: 11.5182 Acc: 0.5776\n",
            "\n",
            "Epoch 617/1499:\n",
            "train Loss: 9.8287 Acc: 0.6064\n",
            "\n",
            "val Loss: 11.5006 Acc: 0.5782\n",
            "\n",
            "Epoch 618/1499:\n",
            "train Loss: 9.8117 Acc: 0.6068\n",
            "\n",
            "val Loss: 11.4830 Acc: 0.5782\n",
            "\n",
            "Epoch 619/1499:\n",
            "train Loss: 9.7947 Acc: 0.6071\n",
            "\n",
            "val Loss: 11.4655 Acc: 0.5782\n",
            "\n",
            "Epoch 620/1499:\n",
            "train Loss: 9.7778 Acc: 0.6075\n",
            "\n",
            "val Loss: 11.4481 Acc: 0.5782\n",
            "\n",
            "Epoch 621/1499:\n",
            "train Loss: 9.7610 Acc: 0.6077\n",
            "\n",
            "val Loss: 11.4307 Acc: 0.5782\n",
            "\n",
            "Epoch 622/1499:\n",
            "train Loss: 9.7442 Acc: 0.6083\n",
            "\n",
            "val Loss: 11.4133 Acc: 0.5782\n",
            "\n",
            "Epoch 623/1499:\n",
            "train Loss: 9.7274 Acc: 0.6086\n",
            "\n",
            "val Loss: 11.3960 Acc: 0.5794\n",
            "\n",
            "Epoch 624/1499:\n",
            "train Loss: 9.7107 Acc: 0.6087\n",
            "\n",
            "val Loss: 11.3787 Acc: 0.5794\n",
            "\n",
            "Epoch 625/1499:\n",
            "train Loss: 9.6941 Acc: 0.6089\n",
            "\n",
            "val Loss: 11.3615 Acc: 0.5800\n",
            "\n",
            "Epoch 626/1499:\n",
            "train Loss: 9.6775 Acc: 0.6092\n",
            "\n",
            "val Loss: 11.3444 Acc: 0.5812\n",
            "\n",
            "Epoch 627/1499:\n",
            "train Loss: 9.6609 Acc: 0.6094\n",
            "\n",
            "val Loss: 11.3272 Acc: 0.5818\n",
            "\n",
            "Epoch 628/1499:\n",
            "train Loss: 9.6444 Acc: 0.6097\n",
            "\n",
            "val Loss: 11.3101 Acc: 0.5824\n",
            "\n",
            "Epoch 629/1499:\n",
            "train Loss: 9.6279 Acc: 0.6102\n",
            "\n",
            "val Loss: 11.2931 Acc: 0.5824\n",
            "\n",
            "Epoch 630/1499:\n",
            "train Loss: 9.6115 Acc: 0.6105\n",
            "\n",
            "val Loss: 11.2761 Acc: 0.5830\n",
            "\n",
            "Epoch 631/1499:\n",
            "train Loss: 9.5951 Acc: 0.6110\n",
            "\n",
            "val Loss: 11.2592 Acc: 0.5830\n",
            "\n",
            "Epoch 632/1499:\n",
            "train Loss: 9.5787 Acc: 0.6113\n",
            "\n",
            "val Loss: 11.2423 Acc: 0.5830\n",
            "\n",
            "Epoch 633/1499:\n",
            "train Loss: 9.5625 Acc: 0.6115\n",
            "\n",
            "val Loss: 11.2254 Acc: 0.5836\n",
            "\n",
            "Epoch 634/1499:\n",
            "train Loss: 9.5462 Acc: 0.6117\n",
            "\n",
            "val Loss: 11.2086 Acc: 0.5848\n",
            "\n",
            "Epoch 635/1499:\n",
            "train Loss: 9.5300 Acc: 0.6119\n",
            "\n",
            "val Loss: 11.1918 Acc: 0.5842\n",
            "\n",
            "Epoch 636/1499:\n",
            "train Loss: 9.5138 Acc: 0.6122\n",
            "\n",
            "val Loss: 11.1751 Acc: 0.5848\n",
            "\n",
            "Epoch 637/1499:\n",
            "train Loss: 9.4977 Acc: 0.6124\n",
            "\n",
            "val Loss: 11.1584 Acc: 0.5848\n",
            "\n",
            "Epoch 638/1499:\n",
            "train Loss: 9.4817 Acc: 0.6125\n",
            "\n",
            "val Loss: 11.1418 Acc: 0.5848\n",
            "\n",
            "Epoch 639/1499:\n",
            "train Loss: 9.4656 Acc: 0.6125\n",
            "\n",
            "val Loss: 11.1252 Acc: 0.5848\n",
            "\n",
            "Epoch 640/1499:\n",
            "train Loss: 9.4497 Acc: 0.6131\n",
            "\n",
            "val Loss: 11.1086 Acc: 0.5848\n",
            "\n",
            "Epoch 641/1499:\n",
            "train Loss: 9.4337 Acc: 0.6134\n",
            "\n",
            "val Loss: 11.0921 Acc: 0.5848\n",
            "\n",
            "Epoch 642/1499:\n",
            "train Loss: 9.4178 Acc: 0.6135\n",
            "\n",
            "val Loss: 11.0757 Acc: 0.5855\n",
            "\n",
            "Epoch 643/1499:\n",
            "train Loss: 9.4020 Acc: 0.6140\n",
            "\n",
            "val Loss: 11.0592 Acc: 0.5855\n",
            "\n",
            "Epoch 644/1499:\n",
            "train Loss: 9.3862 Acc: 0.6143\n",
            "\n",
            "val Loss: 11.0429 Acc: 0.5855\n",
            "\n",
            "Epoch 645/1499:\n",
            "train Loss: 9.3704 Acc: 0.6145\n",
            "\n",
            "val Loss: 11.0265 Acc: 0.5848\n",
            "\n",
            "Epoch 646/1499:\n",
            "train Loss: 9.3547 Acc: 0.6151\n",
            "\n",
            "val Loss: 11.0103 Acc: 0.5848\n",
            "\n",
            "Epoch 647/1499:\n",
            "train Loss: 9.3391 Acc: 0.6152\n",
            "\n",
            "val Loss: 10.9940 Acc: 0.5861\n",
            "\n",
            "Epoch 648/1499:\n",
            "train Loss: 9.3234 Acc: 0.6154\n",
            "\n",
            "val Loss: 10.9778 Acc: 0.5855\n",
            "\n",
            "Epoch 649/1499:\n",
            "train Loss: 9.3079 Acc: 0.6158\n",
            "\n",
            "val Loss: 10.9617 Acc: 0.5855\n",
            "\n",
            "Epoch 650/1499:\n",
            "train Loss: 9.2923 Acc: 0.6162\n",
            "\n",
            "val Loss: 10.9456 Acc: 0.5848\n",
            "\n",
            "Epoch 651/1499:\n",
            "train Loss: 9.2768 Acc: 0.6165\n",
            "\n",
            "val Loss: 10.9295 Acc: 0.5848\n",
            "\n",
            "Epoch 652/1499:\n",
            "train Loss: 9.2614 Acc: 0.6166\n",
            "\n",
            "val Loss: 10.9135 Acc: 0.5848\n",
            "\n",
            "Epoch 653/1499:\n",
            "train Loss: 9.2460 Acc: 0.6169\n",
            "\n",
            "val Loss: 10.8976 Acc: 0.5848\n",
            "\n",
            "Epoch 654/1499:\n",
            "train Loss: 9.2306 Acc: 0.6171\n",
            "\n",
            "val Loss: 10.8816 Acc: 0.5861\n",
            "\n",
            "Epoch 655/1499:\n",
            "train Loss: 9.2153 Acc: 0.6175\n",
            "\n",
            "val Loss: 10.8658 Acc: 0.5873\n",
            "\n",
            "Epoch 656/1499:\n",
            "train Loss: 9.2000 Acc: 0.6176\n",
            "\n",
            "val Loss: 10.8499 Acc: 0.5873\n",
            "\n",
            "Epoch 657/1499:\n",
            "train Loss: 9.1848 Acc: 0.6177\n",
            "\n",
            "val Loss: 10.8342 Acc: 0.5879\n",
            "\n",
            "Epoch 658/1499:\n",
            "train Loss: 9.1696 Acc: 0.6177\n",
            "\n",
            "val Loss: 10.8184 Acc: 0.5885\n",
            "\n",
            "Epoch 659/1499:\n",
            "train Loss: 9.1545 Acc: 0.6182\n",
            "\n",
            "val Loss: 10.8027 Acc: 0.5897\n",
            "\n",
            "Epoch 660/1499:\n",
            "train Loss: 9.1394 Acc: 0.6184\n",
            "\n",
            "val Loss: 10.7871 Acc: 0.5909\n",
            "\n",
            "Epoch 661/1499:\n",
            "train Loss: 9.1244 Acc: 0.6185\n",
            "\n",
            "val Loss: 10.7715 Acc: 0.5909\n",
            "\n",
            "Epoch 662/1499:\n",
            "train Loss: 9.1093 Acc: 0.6192\n",
            "\n",
            "val Loss: 10.7559 Acc: 0.5909\n",
            "\n",
            "Epoch 663/1499:\n",
            "train Loss: 9.0944 Acc: 0.6197\n",
            "\n",
            "val Loss: 10.7404 Acc: 0.5909\n",
            "\n",
            "Epoch 664/1499:\n",
            "train Loss: 9.0795 Acc: 0.6202\n",
            "\n",
            "val Loss: 10.7249 Acc: 0.5915\n",
            "\n",
            "Epoch 665/1499:\n",
            "train Loss: 9.0646 Acc: 0.6207\n",
            "\n",
            "val Loss: 10.7095 Acc: 0.5915\n",
            "\n",
            "Epoch 666/1499:\n",
            "train Loss: 9.0497 Acc: 0.6211\n",
            "\n",
            "val Loss: 10.6941 Acc: 0.5909\n",
            "\n",
            "Epoch 667/1499:\n",
            "train Loss: 9.0349 Acc: 0.6212\n",
            "\n",
            "val Loss: 10.6787 Acc: 0.5909\n",
            "\n",
            "Epoch 668/1499:\n",
            "train Loss: 9.0202 Acc: 0.6216\n",
            "\n",
            "val Loss: 10.6634 Acc: 0.5909\n",
            "\n",
            "Epoch 669/1499:\n",
            "train Loss: 9.0055 Acc: 0.6217\n",
            "\n",
            "val Loss: 10.6482 Acc: 0.5909\n",
            "\n",
            "Epoch 670/1499:\n",
            "train Loss: 8.9908 Acc: 0.6219\n",
            "\n",
            "val Loss: 10.6330 Acc: 0.5915\n",
            "\n",
            "Epoch 671/1499:\n",
            "train Loss: 8.9762 Acc: 0.6225\n",
            "\n",
            "val Loss: 10.6178 Acc: 0.5915\n",
            "\n",
            "Epoch 672/1499:\n",
            "train Loss: 8.9616 Acc: 0.6228\n",
            "\n",
            "val Loss: 10.6027 Acc: 0.5909\n",
            "\n",
            "Epoch 673/1499:\n",
            "train Loss: 8.9470 Acc: 0.6236\n",
            "\n",
            "val Loss: 10.5876 Acc: 0.5909\n",
            "\n",
            "Epoch 674/1499:\n",
            "train Loss: 8.9325 Acc: 0.6240\n",
            "\n",
            "val Loss: 10.5726 Acc: 0.5909\n",
            "\n",
            "Epoch 675/1499:\n",
            "train Loss: 8.9181 Acc: 0.6243\n",
            "\n",
            "val Loss: 10.5576 Acc: 0.5915\n",
            "\n",
            "Epoch 676/1499:\n",
            "train Loss: 8.9036 Acc: 0.6245\n",
            "\n",
            "val Loss: 10.5426 Acc: 0.5915\n",
            "\n",
            "Epoch 677/1499:\n",
            "train Loss: 8.8893 Acc: 0.6250\n",
            "\n",
            "val Loss: 10.5277 Acc: 0.5915\n",
            "\n",
            "Epoch 678/1499:\n",
            "train Loss: 8.8749 Acc: 0.6255\n",
            "\n",
            "val Loss: 10.5128 Acc: 0.5921\n",
            "\n",
            "Epoch 679/1499:\n",
            "train Loss: 8.8606 Acc: 0.6256\n",
            "\n",
            "val Loss: 10.4980 Acc: 0.5921\n",
            "\n",
            "Epoch 680/1499:\n",
            "train Loss: 8.8464 Acc: 0.6256\n",
            "\n",
            "val Loss: 10.4832 Acc: 0.5927\n",
            "\n",
            "Epoch 681/1499:\n",
            "train Loss: 8.8321 Acc: 0.6260\n",
            "\n",
            "val Loss: 10.4685 Acc: 0.5927\n",
            "\n",
            "Epoch 682/1499:\n",
            "train Loss: 8.8180 Acc: 0.6262\n",
            "\n",
            "val Loss: 10.4538 Acc: 0.5927\n",
            "\n",
            "Epoch 683/1499:\n",
            "train Loss: 8.8038 Acc: 0.6263\n",
            "\n",
            "val Loss: 10.4391 Acc: 0.5927\n",
            "\n",
            "Epoch 684/1499:\n",
            "train Loss: 8.7897 Acc: 0.6265\n",
            "\n",
            "val Loss: 10.4245 Acc: 0.5927\n",
            "\n",
            "Epoch 685/1499:\n",
            "train Loss: 8.7757 Acc: 0.6266\n",
            "\n",
            "val Loss: 10.4099 Acc: 0.5933\n",
            "\n",
            "Epoch 686/1499:\n",
            "train Loss: 8.7616 Acc: 0.6273\n",
            "\n",
            "val Loss: 10.3954 Acc: 0.5939\n",
            "\n",
            "Epoch 687/1499:\n",
            "train Loss: 8.7477 Acc: 0.6275\n",
            "\n",
            "val Loss: 10.3809 Acc: 0.5939\n",
            "\n",
            "Epoch 688/1499:\n",
            "train Loss: 8.7337 Acc: 0.6277\n",
            "\n",
            "val Loss: 10.3664 Acc: 0.5939\n",
            "\n",
            "Epoch 689/1499:\n",
            "train Loss: 8.7198 Acc: 0.6280\n",
            "\n",
            "val Loss: 10.3520 Acc: 0.5945\n",
            "\n",
            "Epoch 690/1499:\n",
            "train Loss: 8.7060 Acc: 0.6284\n",
            "\n",
            "val Loss: 10.3376 Acc: 0.5952\n",
            "\n",
            "Epoch 691/1499:\n",
            "train Loss: 8.6922 Acc: 0.6288\n",
            "\n",
            "val Loss: 10.3233 Acc: 0.5952\n",
            "\n",
            "Epoch 692/1499:\n",
            "train Loss: 8.6784 Acc: 0.6290\n",
            "\n",
            "val Loss: 10.3090 Acc: 0.5958\n",
            "\n",
            "Epoch 693/1499:\n",
            "train Loss: 8.6646 Acc: 0.6294\n",
            "\n",
            "val Loss: 10.2948 Acc: 0.5958\n",
            "\n",
            "Epoch 694/1499:\n",
            "train Loss: 8.6510 Acc: 0.6300\n",
            "\n",
            "val Loss: 10.2806 Acc: 0.5958\n",
            "\n",
            "Epoch 695/1499:\n",
            "train Loss: 8.6373 Acc: 0.6305\n",
            "\n",
            "val Loss: 10.2664 Acc: 0.5958\n",
            "\n",
            "Epoch 696/1499:\n",
            "train Loss: 8.6237 Acc: 0.6308\n",
            "\n",
            "val Loss: 10.2523 Acc: 0.5958\n",
            "\n",
            "Epoch 697/1499:\n",
            "train Loss: 8.6101 Acc: 0.6312\n",
            "\n",
            "val Loss: 10.2382 Acc: 0.5958\n",
            "\n",
            "Epoch 698/1499:\n",
            "train Loss: 8.5966 Acc: 0.6317\n",
            "\n",
            "val Loss: 10.2242 Acc: 0.5964\n",
            "\n",
            "Epoch 699/1499:\n",
            "train Loss: 8.5831 Acc: 0.6317\n",
            "\n",
            "val Loss: 10.2102 Acc: 0.5964\n",
            "\n",
            "Epoch 700/1499:\n",
            "train Loss: 8.5696 Acc: 0.6321\n",
            "\n",
            "val Loss: 10.1962 Acc: 0.5964\n",
            "\n",
            "Epoch 701/1499:\n",
            "train Loss: 8.5562 Acc: 0.6324\n",
            "\n",
            "val Loss: 10.1823 Acc: 0.5970\n",
            "\n",
            "Epoch 702/1499:\n",
            "train Loss: 8.5428 Acc: 0.6326\n",
            "\n",
            "val Loss: 10.1684 Acc: 0.5970\n",
            "\n",
            "Epoch 703/1499:\n",
            "train Loss: 8.5294 Acc: 0.6331\n",
            "\n",
            "val Loss: 10.1546 Acc: 0.5976\n",
            "\n",
            "Epoch 704/1499:\n",
            "train Loss: 8.5161 Acc: 0.6332\n",
            "\n",
            "val Loss: 10.1408 Acc: 0.5976\n",
            "\n",
            "Epoch 705/1499:\n",
            "train Loss: 8.5029 Acc: 0.6335\n",
            "\n",
            "val Loss: 10.1271 Acc: 0.5976\n",
            "\n",
            "Epoch 706/1499:\n",
            "train Loss: 8.4896 Acc: 0.6335\n",
            "\n",
            "val Loss: 10.1133 Acc: 0.5982\n",
            "\n",
            "Epoch 707/1499:\n",
            "train Loss: 8.4764 Acc: 0.6334\n",
            "\n",
            "val Loss: 10.0997 Acc: 0.5982\n",
            "\n",
            "Epoch 708/1499:\n",
            "train Loss: 8.4633 Acc: 0.6335\n",
            "\n",
            "val Loss: 10.0860 Acc: 0.5982\n",
            "\n",
            "Epoch 709/1499:\n",
            "train Loss: 8.4502 Acc: 0.6345\n",
            "\n",
            "val Loss: 10.0725 Acc: 0.5988\n",
            "\n",
            "Epoch 710/1499:\n",
            "train Loss: 8.4371 Acc: 0.6347\n",
            "\n",
            "val Loss: 10.0589 Acc: 0.5988\n",
            "\n",
            "Epoch 711/1499:\n",
            "train Loss: 8.4241 Acc: 0.6351\n",
            "\n",
            "val Loss: 10.0454 Acc: 0.5994\n",
            "\n",
            "Epoch 712/1499:\n",
            "train Loss: 8.4111 Acc: 0.6353\n",
            "\n",
            "val Loss: 10.0319 Acc: 0.5994\n",
            "\n",
            "Epoch 713/1499:\n",
            "train Loss: 8.3981 Acc: 0.6354\n",
            "\n",
            "val Loss: 10.0185 Acc: 0.6000\n",
            "\n",
            "Epoch 714/1499:\n",
            "train Loss: 8.3852 Acc: 0.6354\n",
            "\n",
            "val Loss: 10.0051 Acc: 0.6000\n",
            "\n",
            "Epoch 715/1499:\n",
            "train Loss: 8.3723 Acc: 0.6355\n",
            "\n",
            "val Loss: 9.9918 Acc: 0.6006\n",
            "\n",
            "Epoch 716/1499:\n",
            "train Loss: 8.3595 Acc: 0.6357\n",
            "\n",
            "val Loss: 9.9784 Acc: 0.6006\n",
            "\n",
            "Epoch 717/1499:\n",
            "train Loss: 8.3466 Acc: 0.6357\n",
            "\n",
            "val Loss: 9.9652 Acc: 0.6006\n",
            "\n",
            "Epoch 718/1499:\n",
            "train Loss: 8.3339 Acc: 0.6360\n",
            "\n",
            "val Loss: 9.9519 Acc: 0.6006\n",
            "\n",
            "Epoch 719/1499:\n",
            "train Loss: 8.3211 Acc: 0.6365\n",
            "\n",
            "val Loss: 9.9387 Acc: 0.6006\n",
            "\n",
            "Epoch 720/1499:\n",
            "train Loss: 8.3084 Acc: 0.6365\n",
            "\n",
            "val Loss: 9.9255 Acc: 0.6012\n",
            "\n",
            "Epoch 721/1499:\n",
            "train Loss: 8.2958 Acc: 0.6366\n",
            "\n",
            "val Loss: 9.9124 Acc: 0.6012\n",
            "\n",
            "Epoch 722/1499:\n",
            "train Loss: 8.2831 Acc: 0.6370\n",
            "\n",
            "val Loss: 9.8993 Acc: 0.6012\n",
            "\n",
            "Epoch 723/1499:\n",
            "train Loss: 8.2705 Acc: 0.6371\n",
            "\n",
            "val Loss: 9.8862 Acc: 0.6006\n",
            "\n",
            "Epoch 724/1499:\n",
            "train Loss: 8.2580 Acc: 0.6371\n",
            "\n",
            "val Loss: 9.8732 Acc: 0.6006\n",
            "\n",
            "Epoch 725/1499:\n",
            "train Loss: 8.2455 Acc: 0.6375\n",
            "\n",
            "val Loss: 9.8602 Acc: 0.6012\n",
            "\n",
            "Epoch 726/1499:\n",
            "train Loss: 8.2330 Acc: 0.6381\n",
            "\n",
            "val Loss: 9.8473 Acc: 0.6012\n",
            "\n",
            "Epoch 727/1499:\n",
            "train Loss: 8.2205 Acc: 0.6384\n",
            "\n",
            "val Loss: 9.8343 Acc: 0.6012\n",
            "\n",
            "Epoch 728/1499:\n",
            "train Loss: 8.2081 Acc: 0.6386\n",
            "\n",
            "val Loss: 9.8214 Acc: 0.6012\n",
            "\n",
            "Epoch 729/1499:\n",
            "train Loss: 8.1957 Acc: 0.6386\n",
            "\n",
            "val Loss: 9.8086 Acc: 0.6012\n",
            "\n",
            "Epoch 730/1499:\n",
            "train Loss: 8.1834 Acc: 0.6389\n",
            "\n",
            "val Loss: 9.7958 Acc: 0.6006\n",
            "\n",
            "Epoch 731/1499:\n",
            "train Loss: 8.1711 Acc: 0.6391\n",
            "\n",
            "val Loss: 9.7830 Acc: 0.6006\n",
            "\n",
            "Epoch 732/1499:\n",
            "train Loss: 8.1588 Acc: 0.6393\n",
            "\n",
            "val Loss: 9.7702 Acc: 0.6012\n",
            "\n",
            "Epoch 733/1499:\n",
            "train Loss: 8.1466 Acc: 0.6395\n",
            "\n",
            "val Loss: 9.7575 Acc: 0.6018\n",
            "\n",
            "Epoch 734/1499:\n",
            "train Loss: 8.1344 Acc: 0.6397\n",
            "\n",
            "val Loss: 9.7448 Acc: 0.6024\n",
            "\n",
            "Epoch 735/1499:\n",
            "train Loss: 8.1222 Acc: 0.6401\n",
            "\n",
            "val Loss: 9.7321 Acc: 0.6024\n",
            "\n",
            "Epoch 736/1499:\n",
            "train Loss: 8.1101 Acc: 0.6405\n",
            "\n",
            "val Loss: 9.7195 Acc: 0.6024\n",
            "\n",
            "Epoch 737/1499:\n",
            "train Loss: 8.0980 Acc: 0.6407\n",
            "\n",
            "val Loss: 9.7069 Acc: 0.6024\n",
            "\n",
            "Epoch 738/1499:\n",
            "train Loss: 8.0859 Acc: 0.6408\n",
            "\n",
            "val Loss: 9.6944 Acc: 0.6024\n",
            "\n",
            "Epoch 739/1499:\n",
            "train Loss: 8.0739 Acc: 0.6415\n",
            "\n",
            "val Loss: 9.6818 Acc: 0.6024\n",
            "\n",
            "Epoch 740/1499:\n",
            "train Loss: 8.0619 Acc: 0.6416\n",
            "\n",
            "val Loss: 9.6693 Acc: 0.6024\n",
            "\n",
            "Epoch 741/1499:\n",
            "train Loss: 8.0499 Acc: 0.6416\n",
            "\n",
            "val Loss: 9.6569 Acc: 0.6024\n",
            "\n",
            "Epoch 742/1499:\n",
            "train Loss: 8.0380 Acc: 0.6420\n",
            "\n",
            "val Loss: 9.6445 Acc: 0.6036\n",
            "\n",
            "Epoch 743/1499:\n",
            "train Loss: 8.0261 Acc: 0.6423\n",
            "\n",
            "val Loss: 9.6321 Acc: 0.6042\n",
            "\n",
            "Epoch 744/1499:\n",
            "train Loss: 8.0143 Acc: 0.6424\n",
            "\n",
            "val Loss: 9.6197 Acc: 0.6048\n",
            "\n",
            "Epoch 745/1499:\n",
            "train Loss: 8.0024 Acc: 0.6425\n",
            "\n",
            "val Loss: 9.6073 Acc: 0.6048\n",
            "\n",
            "Epoch 746/1499:\n",
            "train Loss: 7.9906 Acc: 0.6427\n",
            "\n",
            "val Loss: 9.5950 Acc: 0.6048\n",
            "\n",
            "Epoch 747/1499:\n",
            "train Loss: 7.9789 Acc: 0.6432\n",
            "\n",
            "val Loss: 9.5827 Acc: 0.6061\n",
            "\n",
            "Epoch 748/1499:\n",
            "train Loss: 7.9671 Acc: 0.6435\n",
            "\n",
            "val Loss: 9.5705 Acc: 0.6067\n",
            "\n",
            "Epoch 749/1499:\n",
            "train Loss: 7.9554 Acc: 0.6440\n",
            "\n",
            "val Loss: 9.5583 Acc: 0.6067\n",
            "\n",
            "Epoch 750/1499:\n",
            "train Loss: 7.9438 Acc: 0.6443\n",
            "\n",
            "val Loss: 9.5461 Acc: 0.6073\n",
            "\n",
            "Epoch 751/1499:\n",
            "train Loss: 7.9321 Acc: 0.6447\n",
            "\n",
            "val Loss: 9.5339 Acc: 0.6073\n",
            "\n",
            "Epoch 752/1499:\n",
            "train Loss: 7.9205 Acc: 0.6449\n",
            "\n",
            "val Loss: 9.5217 Acc: 0.6073\n",
            "\n",
            "Epoch 753/1499:\n",
            "train Loss: 7.9090 Acc: 0.6450\n",
            "\n",
            "val Loss: 9.5096 Acc: 0.6073\n",
            "\n",
            "Epoch 754/1499:\n",
            "train Loss: 7.8974 Acc: 0.6452\n",
            "\n",
            "val Loss: 9.4975 Acc: 0.6079\n",
            "\n",
            "Epoch 755/1499:\n",
            "train Loss: 7.8859 Acc: 0.6454\n",
            "\n",
            "val Loss: 9.4855 Acc: 0.6085\n",
            "\n",
            "Epoch 756/1499:\n",
            "train Loss: 7.8744 Acc: 0.6455\n",
            "\n",
            "val Loss: 9.4734 Acc: 0.6085\n",
            "\n",
            "Epoch 757/1499:\n",
            "train Loss: 7.8630 Acc: 0.6458\n",
            "\n",
            "val Loss: 9.4614 Acc: 0.6091\n",
            "\n",
            "Epoch 758/1499:\n",
            "train Loss: 7.8516 Acc: 0.6462\n",
            "\n",
            "val Loss: 9.4494 Acc: 0.6091\n",
            "\n",
            "Epoch 759/1499:\n",
            "train Loss: 7.8402 Acc: 0.6464\n",
            "\n",
            "val Loss: 9.4375 Acc: 0.6097\n",
            "\n",
            "Epoch 760/1499:\n",
            "train Loss: 7.8288 Acc: 0.6467\n",
            "\n",
            "val Loss: 9.4255 Acc: 0.6097\n",
            "\n",
            "Epoch 761/1499:\n",
            "train Loss: 7.8175 Acc: 0.6471\n",
            "\n",
            "val Loss: 9.4136 Acc: 0.6103\n",
            "\n",
            "Epoch 762/1499:\n",
            "train Loss: 7.8062 Acc: 0.6474\n",
            "\n",
            "val Loss: 9.4017 Acc: 0.6109\n",
            "\n",
            "Epoch 763/1499:\n",
            "train Loss: 7.7949 Acc: 0.6474\n",
            "\n",
            "val Loss: 9.3899 Acc: 0.6115\n",
            "\n",
            "Epoch 764/1499:\n",
            "train Loss: 7.7837 Acc: 0.6476\n",
            "\n",
            "val Loss: 9.3781 Acc: 0.6115\n",
            "\n",
            "Epoch 765/1499:\n",
            "train Loss: 7.7724 Acc: 0.6481\n",
            "\n",
            "val Loss: 9.3663 Acc: 0.6115\n",
            "\n",
            "Epoch 766/1499:\n",
            "train Loss: 7.7613 Acc: 0.6484\n",
            "\n",
            "val Loss: 9.3545 Acc: 0.6121\n",
            "\n",
            "Epoch 767/1499:\n",
            "train Loss: 7.7501 Acc: 0.6486\n",
            "\n",
            "val Loss: 9.3427 Acc: 0.6127\n",
            "\n",
            "Epoch 768/1499:\n",
            "train Loss: 7.7390 Acc: 0.6488\n",
            "\n",
            "val Loss: 9.3310 Acc: 0.6121\n",
            "\n",
            "Epoch 769/1499:\n",
            "train Loss: 7.7279 Acc: 0.6490\n",
            "\n",
            "val Loss: 9.3193 Acc: 0.6133\n",
            "\n",
            "Epoch 770/1499:\n",
            "train Loss: 7.7168 Acc: 0.6495\n",
            "\n",
            "val Loss: 9.3077 Acc: 0.6133\n",
            "\n",
            "Epoch 771/1499:\n",
            "train Loss: 7.7058 Acc: 0.6499\n",
            "\n",
            "val Loss: 9.2960 Acc: 0.6133\n",
            "\n",
            "Epoch 772/1499:\n",
            "train Loss: 7.6948 Acc: 0.6500\n",
            "\n",
            "val Loss: 9.2844 Acc: 0.6133\n",
            "\n",
            "Epoch 773/1499:\n",
            "train Loss: 7.6838 Acc: 0.6503\n",
            "\n",
            "val Loss: 9.2728 Acc: 0.6139\n",
            "\n",
            "Epoch 774/1499:\n",
            "train Loss: 7.6728 Acc: 0.6507\n",
            "\n",
            "val Loss: 9.2612 Acc: 0.6145\n",
            "\n",
            "Epoch 775/1499:\n",
            "train Loss: 7.6619 Acc: 0.6507\n",
            "\n",
            "val Loss: 9.2497 Acc: 0.6152\n",
            "\n",
            "Epoch 776/1499:\n",
            "train Loss: 7.6510 Acc: 0.6507\n",
            "\n",
            "val Loss: 9.2382 Acc: 0.6158\n",
            "\n",
            "Epoch 777/1499:\n",
            "train Loss: 7.6401 Acc: 0.6511\n",
            "\n",
            "val Loss: 9.2267 Acc: 0.6158\n",
            "\n",
            "Epoch 778/1499:\n",
            "train Loss: 7.6292 Acc: 0.6512\n",
            "\n",
            "val Loss: 9.2153 Acc: 0.6158\n",
            "\n",
            "Epoch 779/1499:\n",
            "train Loss: 7.6184 Acc: 0.6515\n",
            "\n",
            "val Loss: 9.2038 Acc: 0.6164\n",
            "\n",
            "Epoch 780/1499:\n",
            "train Loss: 7.6076 Acc: 0.6519\n",
            "\n",
            "val Loss: 9.1924 Acc: 0.6164\n",
            "\n",
            "Epoch 781/1499:\n",
            "train Loss: 7.5968 Acc: 0.6523\n",
            "\n",
            "val Loss: 9.1810 Acc: 0.6170\n",
            "\n",
            "Epoch 782/1499:\n",
            "train Loss: 7.5861 Acc: 0.6525\n",
            "\n",
            "val Loss: 9.1697 Acc: 0.6164\n",
            "\n",
            "Epoch 783/1499:\n",
            "train Loss: 7.5754 Acc: 0.6527\n",
            "\n",
            "val Loss: 9.1584 Acc: 0.6170\n",
            "\n",
            "Epoch 784/1499:\n",
            "train Loss: 7.5647 Acc: 0.6533\n",
            "\n",
            "val Loss: 9.1471 Acc: 0.6170\n",
            "\n",
            "Epoch 785/1499:\n",
            "train Loss: 7.5540 Acc: 0.6537\n",
            "\n",
            "val Loss: 9.1358 Acc: 0.6182\n",
            "\n",
            "Epoch 786/1499:\n",
            "train Loss: 7.5434 Acc: 0.6539\n",
            "\n",
            "val Loss: 9.1245 Acc: 0.6188\n",
            "\n",
            "Epoch 787/1499:\n",
            "train Loss: 7.5327 Acc: 0.6539\n",
            "\n",
            "val Loss: 9.1133 Acc: 0.6206\n",
            "\n",
            "Epoch 788/1499:\n",
            "train Loss: 7.5221 Acc: 0.6545\n",
            "\n",
            "val Loss: 9.1021 Acc: 0.6206\n",
            "\n",
            "Epoch 789/1499:\n",
            "train Loss: 7.5116 Acc: 0.6546\n",
            "\n",
            "val Loss: 9.0909 Acc: 0.6206\n",
            "\n",
            "Epoch 790/1499:\n",
            "train Loss: 7.5010 Acc: 0.6546\n",
            "\n",
            "val Loss: 9.0798 Acc: 0.6212\n",
            "\n",
            "Epoch 791/1499:\n",
            "train Loss: 7.4905 Acc: 0.6550\n",
            "\n",
            "val Loss: 9.0687 Acc: 0.6218\n",
            "\n",
            "Epoch 792/1499:\n",
            "train Loss: 7.4800 Acc: 0.6551\n",
            "\n",
            "val Loss: 9.0576 Acc: 0.6224\n",
            "\n",
            "Epoch 793/1499:\n",
            "train Loss: 7.4695 Acc: 0.6552\n",
            "\n",
            "val Loss: 9.0465 Acc: 0.6230\n",
            "\n",
            "Epoch 794/1499:\n",
            "train Loss: 7.4591 Acc: 0.6553\n",
            "\n",
            "val Loss: 9.0355 Acc: 0.6236\n",
            "\n",
            "Epoch 795/1499:\n",
            "train Loss: 7.4487 Acc: 0.6555\n",
            "\n",
            "val Loss: 9.0245 Acc: 0.6236\n",
            "\n",
            "Epoch 796/1499:\n",
            "train Loss: 7.4383 Acc: 0.6561\n",
            "\n",
            "val Loss: 9.0135 Acc: 0.6236\n",
            "\n",
            "Epoch 797/1499:\n",
            "train Loss: 7.4279 Acc: 0.6565\n",
            "\n",
            "val Loss: 9.0025 Acc: 0.6248\n",
            "\n",
            "Epoch 798/1499:\n",
            "train Loss: 7.4175 Acc: 0.6563\n",
            "\n",
            "val Loss: 8.9916 Acc: 0.6255\n",
            "\n",
            "Epoch 799/1499:\n",
            "train Loss: 7.4072 Acc: 0.6562\n",
            "\n",
            "val Loss: 8.9807 Acc: 0.6255\n",
            "\n",
            "Epoch 800/1499:\n",
            "train Loss: 7.3969 Acc: 0.6560\n",
            "\n",
            "val Loss: 8.9698 Acc: 0.6255\n",
            "\n",
            "Epoch 801/1499:\n",
            "train Loss: 7.3866 Acc: 0.6564\n",
            "\n",
            "val Loss: 8.9590 Acc: 0.6255\n",
            "\n",
            "Epoch 802/1499:\n",
            "train Loss: 7.3764 Acc: 0.6570\n",
            "\n",
            "val Loss: 8.9482 Acc: 0.6255\n",
            "\n",
            "Epoch 803/1499:\n",
            "train Loss: 7.3662 Acc: 0.6575\n",
            "\n",
            "val Loss: 8.9374 Acc: 0.6255\n",
            "\n",
            "Epoch 804/1499:\n",
            "train Loss: 7.3560 Acc: 0.6578\n",
            "\n",
            "val Loss: 8.9266 Acc: 0.6255\n",
            "\n",
            "Epoch 805/1499:\n",
            "train Loss: 7.3458 Acc: 0.6582\n",
            "\n",
            "val Loss: 8.9158 Acc: 0.6267\n",
            "\n",
            "Epoch 806/1499:\n",
            "train Loss: 7.3356 Acc: 0.6582\n",
            "\n",
            "val Loss: 8.9051 Acc: 0.6273\n",
            "\n",
            "Epoch 807/1499:\n",
            "train Loss: 7.3255 Acc: 0.6585\n",
            "\n",
            "val Loss: 8.8944 Acc: 0.6279\n",
            "\n",
            "Epoch 808/1499:\n",
            "train Loss: 7.3154 Acc: 0.6586\n",
            "\n",
            "val Loss: 8.8837 Acc: 0.6279\n",
            "\n",
            "Epoch 809/1499:\n",
            "train Loss: 7.3053 Acc: 0.6587\n",
            "\n",
            "val Loss: 8.8731 Acc: 0.6279\n",
            "\n",
            "Epoch 810/1499:\n",
            "train Loss: 7.2952 Acc: 0.6591\n",
            "\n",
            "val Loss: 8.8625 Acc: 0.6279\n",
            "\n",
            "Epoch 811/1499:\n",
            "train Loss: 7.2852 Acc: 0.6593\n",
            "\n",
            "val Loss: 8.8519 Acc: 0.6279\n",
            "\n",
            "Epoch 812/1499:\n",
            "train Loss: 7.2752 Acc: 0.6594\n",
            "\n",
            "val Loss: 8.8413 Acc: 0.6291\n",
            "\n",
            "Epoch 813/1499:\n",
            "train Loss: 7.2652 Acc: 0.6595\n",
            "\n",
            "val Loss: 8.8308 Acc: 0.6291\n",
            "\n",
            "Epoch 814/1499:\n",
            "train Loss: 7.2553 Acc: 0.6601\n",
            "\n",
            "val Loss: 8.8203 Acc: 0.6297\n",
            "\n",
            "Epoch 815/1499:\n",
            "train Loss: 7.2453 Acc: 0.6604\n",
            "\n",
            "val Loss: 8.8098 Acc: 0.6291\n",
            "\n",
            "Epoch 816/1499:\n",
            "train Loss: 7.2354 Acc: 0.6603\n",
            "\n",
            "val Loss: 8.7993 Acc: 0.6291\n",
            "\n",
            "Epoch 817/1499:\n",
            "train Loss: 7.2255 Acc: 0.6605\n",
            "\n",
            "val Loss: 8.7889 Acc: 0.6297\n",
            "\n",
            "Epoch 818/1499:\n",
            "train Loss: 7.2157 Acc: 0.6606\n",
            "\n",
            "val Loss: 8.7785 Acc: 0.6297\n",
            "\n",
            "Epoch 819/1499:\n",
            "train Loss: 7.2058 Acc: 0.6610\n",
            "\n",
            "val Loss: 8.7681 Acc: 0.6303\n",
            "\n",
            "Epoch 820/1499:\n",
            "train Loss: 7.1960 Acc: 0.6612\n",
            "\n",
            "val Loss: 8.7577 Acc: 0.6303\n",
            "\n",
            "Epoch 821/1499:\n",
            "train Loss: 7.1862 Acc: 0.6616\n",
            "\n",
            "val Loss: 8.7474 Acc: 0.6303\n",
            "\n",
            "Epoch 822/1499:\n",
            "train Loss: 7.1765 Acc: 0.6615\n",
            "\n",
            "val Loss: 8.7371 Acc: 0.6303\n",
            "\n",
            "Epoch 823/1499:\n",
            "train Loss: 7.1667 Acc: 0.6621\n",
            "\n",
            "val Loss: 8.7268 Acc: 0.6303\n",
            "\n",
            "Epoch 824/1499:\n",
            "train Loss: 7.1570 Acc: 0.6625\n",
            "\n",
            "val Loss: 8.7165 Acc: 0.6303\n",
            "\n",
            "Epoch 825/1499:\n",
            "train Loss: 7.1473 Acc: 0.6625\n",
            "\n",
            "val Loss: 8.7063 Acc: 0.6315\n",
            "\n",
            "Epoch 826/1499:\n",
            "train Loss: 7.1376 Acc: 0.6625\n",
            "\n",
            "val Loss: 8.6961 Acc: 0.6321\n",
            "\n",
            "Epoch 827/1499:\n",
            "train Loss: 7.1280 Acc: 0.6626\n",
            "\n",
            "val Loss: 8.6859 Acc: 0.6321\n",
            "\n",
            "Epoch 828/1499:\n",
            "train Loss: 7.1184 Acc: 0.6627\n",
            "\n",
            "val Loss: 8.6758 Acc: 0.6327\n",
            "\n",
            "Epoch 829/1499:\n",
            "train Loss: 7.1088 Acc: 0.6631\n",
            "\n",
            "val Loss: 8.6656 Acc: 0.6327\n",
            "\n",
            "Epoch 830/1499:\n",
            "train Loss: 7.0992 Acc: 0.6634\n",
            "\n",
            "val Loss: 8.6555 Acc: 0.6339\n",
            "\n",
            "Epoch 831/1499:\n",
            "train Loss: 7.0897 Acc: 0.6637\n",
            "\n",
            "val Loss: 8.6455 Acc: 0.6339\n",
            "\n",
            "Epoch 832/1499:\n",
            "train Loss: 7.0801 Acc: 0.6642\n",
            "\n",
            "val Loss: 8.6354 Acc: 0.6333\n",
            "\n",
            "Epoch 833/1499:\n",
            "train Loss: 7.0706 Acc: 0.6646\n",
            "\n",
            "val Loss: 8.6254 Acc: 0.6333\n",
            "\n",
            "Epoch 834/1499:\n",
            "train Loss: 7.0612 Acc: 0.6649\n",
            "\n",
            "val Loss: 8.6154 Acc: 0.6333\n",
            "\n",
            "Epoch 835/1499:\n",
            "train Loss: 7.0517 Acc: 0.6650\n",
            "\n",
            "val Loss: 8.6054 Acc: 0.6339\n",
            "\n",
            "Epoch 836/1499:\n",
            "train Loss: 7.0423 Acc: 0.6655\n",
            "\n",
            "val Loss: 8.5955 Acc: 0.6339\n",
            "\n",
            "Epoch 837/1499:\n",
            "train Loss: 7.0329 Acc: 0.6656\n",
            "\n",
            "val Loss: 8.5855 Acc: 0.6339\n",
            "\n",
            "Epoch 838/1499:\n",
            "train Loss: 7.0235 Acc: 0.6659\n",
            "\n",
            "val Loss: 8.5756 Acc: 0.6339\n",
            "\n",
            "Epoch 839/1499:\n",
            "train Loss: 7.0141 Acc: 0.6662\n",
            "\n",
            "val Loss: 8.5658 Acc: 0.6339\n",
            "\n",
            "Epoch 840/1499:\n",
            "train Loss: 7.0048 Acc: 0.6665\n",
            "\n",
            "val Loss: 8.5559 Acc: 0.6339\n",
            "\n",
            "Epoch 841/1499:\n",
            "train Loss: 6.9955 Acc: 0.6668\n",
            "\n",
            "val Loss: 8.5461 Acc: 0.6345\n",
            "\n",
            "Epoch 842/1499:\n",
            "train Loss: 6.9862 Acc: 0.6673\n",
            "\n",
            "val Loss: 8.5363 Acc: 0.6345\n",
            "\n",
            "Epoch 843/1499:\n",
            "train Loss: 6.9769 Acc: 0.6675\n",
            "\n",
            "val Loss: 8.5265 Acc: 0.6345\n",
            "\n",
            "Epoch 844/1499:\n",
            "train Loss: 6.9677 Acc: 0.6675\n",
            "\n",
            "val Loss: 8.5168 Acc: 0.6345\n",
            "\n",
            "Epoch 845/1499:\n",
            "train Loss: 6.9584 Acc: 0.6678\n",
            "\n",
            "val Loss: 8.5071 Acc: 0.6352\n",
            "\n",
            "Epoch 846/1499:\n",
            "train Loss: 6.9492 Acc: 0.6679\n",
            "\n",
            "val Loss: 8.4974 Acc: 0.6352\n",
            "\n",
            "Epoch 847/1499:\n",
            "train Loss: 6.9400 Acc: 0.6683\n",
            "\n",
            "val Loss: 8.4877 Acc: 0.6358\n",
            "\n",
            "Epoch 848/1499:\n",
            "train Loss: 6.9309 Acc: 0.6686\n",
            "\n",
            "val Loss: 8.4781 Acc: 0.6352\n",
            "\n",
            "Epoch 849/1499:\n",
            "train Loss: 6.9217 Acc: 0.6685\n",
            "\n",
            "val Loss: 8.4685 Acc: 0.6352\n",
            "\n",
            "Epoch 850/1499:\n",
            "train Loss: 6.9126 Acc: 0.6686\n",
            "\n",
            "val Loss: 8.4589 Acc: 0.6358\n",
            "\n",
            "Epoch 851/1499:\n",
            "train Loss: 6.9035 Acc: 0.6689\n",
            "\n",
            "val Loss: 8.4493 Acc: 0.6358\n",
            "\n",
            "Epoch 852/1499:\n",
            "train Loss: 6.8944 Acc: 0.6692\n",
            "\n",
            "val Loss: 8.4398 Acc: 0.6358\n",
            "\n",
            "Epoch 853/1499:\n",
            "train Loss: 6.8854 Acc: 0.6694\n",
            "\n",
            "val Loss: 8.4303 Acc: 0.6358\n",
            "\n",
            "Epoch 854/1499:\n",
            "train Loss: 6.8764 Acc: 0.6697\n",
            "\n",
            "val Loss: 8.4208 Acc: 0.6364\n",
            "\n",
            "Epoch 855/1499:\n",
            "train Loss: 6.8674 Acc: 0.6696\n",
            "\n",
            "val Loss: 8.4113 Acc: 0.6364\n",
            "\n",
            "Epoch 856/1499:\n",
            "train Loss: 6.8584 Acc: 0.6697\n",
            "\n",
            "val Loss: 8.4019 Acc: 0.6364\n",
            "\n",
            "Epoch 857/1499:\n",
            "train Loss: 6.8494 Acc: 0.6699\n",
            "\n",
            "val Loss: 8.3925 Acc: 0.6364\n",
            "\n",
            "Epoch 858/1499:\n",
            "train Loss: 6.8404 Acc: 0.6704\n",
            "\n",
            "val Loss: 8.3831 Acc: 0.6370\n",
            "\n",
            "Epoch 859/1499:\n",
            "train Loss: 6.8315 Acc: 0.6705\n",
            "\n",
            "val Loss: 8.3737 Acc: 0.6376\n",
            "\n",
            "Epoch 860/1499:\n",
            "train Loss: 6.8226 Acc: 0.6705\n",
            "\n",
            "val Loss: 8.3644 Acc: 0.6376\n",
            "\n",
            "Epoch 861/1499:\n",
            "train Loss: 6.8137 Acc: 0.6709\n",
            "\n",
            "val Loss: 8.3551 Acc: 0.6376\n",
            "\n",
            "Epoch 862/1499:\n",
            "train Loss: 6.8048 Acc: 0.6710\n",
            "\n",
            "val Loss: 8.3458 Acc: 0.6370\n",
            "\n",
            "Epoch 863/1499:\n",
            "train Loss: 6.7960 Acc: 0.6711\n",
            "\n",
            "val Loss: 8.3366 Acc: 0.6376\n",
            "\n",
            "Epoch 864/1499:\n",
            "train Loss: 6.7872 Acc: 0.6712\n",
            "\n",
            "val Loss: 8.3274 Acc: 0.6376\n",
            "\n",
            "Epoch 865/1499:\n",
            "train Loss: 6.7784 Acc: 0.6713\n",
            "\n",
            "val Loss: 8.3182 Acc: 0.6376\n",
            "\n",
            "Epoch 866/1499:\n",
            "train Loss: 6.7696 Acc: 0.6715\n",
            "\n",
            "val Loss: 8.3090 Acc: 0.6376\n",
            "\n",
            "Epoch 867/1499:\n",
            "train Loss: 6.7608 Acc: 0.6718\n",
            "\n",
            "val Loss: 8.2998 Acc: 0.6376\n",
            "\n",
            "Epoch 868/1499:\n",
            "train Loss: 6.7521 Acc: 0.6719\n",
            "\n",
            "val Loss: 8.2907 Acc: 0.6376\n",
            "\n",
            "Epoch 869/1499:\n",
            "train Loss: 6.7433 Acc: 0.6721\n",
            "\n",
            "val Loss: 8.2816 Acc: 0.6376\n",
            "\n",
            "Epoch 870/1499:\n",
            "train Loss: 6.7346 Acc: 0.6722\n",
            "\n",
            "val Loss: 8.2725 Acc: 0.6388\n",
            "\n",
            "Epoch 871/1499:\n",
            "train Loss: 6.7260 Acc: 0.6722\n",
            "\n",
            "val Loss: 8.2635 Acc: 0.6388\n",
            "\n",
            "Epoch 872/1499:\n",
            "train Loss: 6.7173 Acc: 0.6724\n",
            "\n",
            "val Loss: 8.2545 Acc: 0.6388\n",
            "\n",
            "Epoch 873/1499:\n",
            "train Loss: 6.7086 Acc: 0.6724\n",
            "\n",
            "val Loss: 8.2455 Acc: 0.6388\n",
            "\n",
            "Epoch 874/1499:\n",
            "train Loss: 6.7000 Acc: 0.6726\n",
            "\n",
            "val Loss: 8.2365 Acc: 0.6394\n",
            "\n",
            "Epoch 875/1499:\n",
            "train Loss: 6.6914 Acc: 0.6728\n",
            "\n",
            "val Loss: 8.2276 Acc: 0.6400\n",
            "\n",
            "Epoch 876/1499:\n",
            "train Loss: 6.6828 Acc: 0.6728\n",
            "\n",
            "val Loss: 8.2186 Acc: 0.6400\n",
            "\n",
            "Epoch 877/1499:\n",
            "train Loss: 6.6743 Acc: 0.6728\n",
            "\n",
            "val Loss: 8.2098 Acc: 0.6406\n",
            "\n",
            "Epoch 878/1499:\n",
            "train Loss: 6.6657 Acc: 0.6728\n",
            "\n",
            "val Loss: 8.2009 Acc: 0.6406\n",
            "\n",
            "Epoch 879/1499:\n",
            "train Loss: 6.6572 Acc: 0.6730\n",
            "\n",
            "val Loss: 8.1920 Acc: 0.6412\n",
            "\n",
            "Epoch 880/1499:\n",
            "train Loss: 6.6487 Acc: 0.6732\n",
            "\n",
            "val Loss: 8.1832 Acc: 0.6412\n",
            "\n",
            "Epoch 881/1499:\n",
            "train Loss: 6.6402 Acc: 0.6734\n",
            "\n",
            "val Loss: 8.1744 Acc: 0.6412\n",
            "\n",
            "Epoch 882/1499:\n",
            "train Loss: 6.6317 Acc: 0.6735\n",
            "\n",
            "val Loss: 8.1656 Acc: 0.6412\n",
            "\n",
            "Epoch 883/1499:\n",
            "train Loss: 6.6233 Acc: 0.6740\n",
            "\n",
            "val Loss: 8.1569 Acc: 0.6412\n",
            "\n",
            "Epoch 884/1499:\n",
            "train Loss: 6.6148 Acc: 0.6743\n",
            "\n",
            "val Loss: 8.1482 Acc: 0.6412\n",
            "\n",
            "Epoch 885/1499:\n",
            "train Loss: 6.6064 Acc: 0.6744\n",
            "\n",
            "val Loss: 8.1394 Acc: 0.6418\n",
            "\n",
            "Epoch 886/1499:\n",
            "train Loss: 6.5980 Acc: 0.6745\n",
            "\n",
            "val Loss: 8.1308 Acc: 0.6424\n",
            "\n",
            "Epoch 887/1499:\n",
            "train Loss: 6.5897 Acc: 0.6747\n",
            "\n",
            "val Loss: 8.1221 Acc: 0.6424\n",
            "\n",
            "Epoch 888/1499:\n",
            "train Loss: 6.5813 Acc: 0.6750\n",
            "\n",
            "val Loss: 8.1135 Acc: 0.6436\n",
            "\n",
            "Epoch 889/1499:\n",
            "train Loss: 6.5730 Acc: 0.6750\n",
            "\n",
            "val Loss: 8.1048 Acc: 0.6442\n",
            "\n",
            "Epoch 890/1499:\n",
            "train Loss: 6.5647 Acc: 0.6753\n",
            "\n",
            "val Loss: 8.0963 Acc: 0.6442\n",
            "\n",
            "Epoch 891/1499:\n",
            "train Loss: 6.5564 Acc: 0.6755\n",
            "\n",
            "val Loss: 8.0877 Acc: 0.6442\n",
            "\n",
            "Epoch 892/1499:\n",
            "train Loss: 6.5481 Acc: 0.6759\n",
            "\n",
            "val Loss: 8.0791 Acc: 0.6455\n",
            "\n",
            "Epoch 893/1499:\n",
            "train Loss: 6.5398 Acc: 0.6761\n",
            "\n",
            "val Loss: 8.0706 Acc: 0.6455\n",
            "\n",
            "Epoch 894/1499:\n",
            "train Loss: 6.5316 Acc: 0.6764\n",
            "\n",
            "val Loss: 8.0621 Acc: 0.6455\n",
            "\n",
            "Epoch 895/1499:\n",
            "train Loss: 6.5234 Acc: 0.6766\n",
            "\n",
            "val Loss: 8.0536 Acc: 0.6448\n",
            "\n",
            "Epoch 896/1499:\n",
            "train Loss: 6.5152 Acc: 0.6770\n",
            "\n",
            "val Loss: 8.0451 Acc: 0.6448\n",
            "\n",
            "Epoch 897/1499:\n",
            "train Loss: 6.5070 Acc: 0.6768\n",
            "\n",
            "val Loss: 8.0367 Acc: 0.6448\n",
            "\n",
            "Epoch 898/1499:\n",
            "train Loss: 6.4988 Acc: 0.6772\n",
            "\n",
            "val Loss: 8.0283 Acc: 0.6442\n",
            "\n",
            "Epoch 899/1499:\n",
            "train Loss: 6.4907 Acc: 0.6775\n",
            "\n",
            "val Loss: 8.0198 Acc: 0.6442\n",
            "\n",
            "Epoch 900/1499:\n",
            "train Loss: 6.4826 Acc: 0.6779\n",
            "\n",
            "val Loss: 8.0115 Acc: 0.6436\n",
            "\n",
            "Epoch 901/1499:\n",
            "train Loss: 6.4745 Acc: 0.6785\n",
            "\n",
            "val Loss: 8.0031 Acc: 0.6442\n",
            "\n",
            "Epoch 902/1499:\n",
            "train Loss: 6.4664 Acc: 0.6787\n",
            "\n",
            "val Loss: 7.9948 Acc: 0.6442\n",
            "\n",
            "Epoch 903/1499:\n",
            "train Loss: 6.4583 Acc: 0.6790\n",
            "\n",
            "val Loss: 7.9864 Acc: 0.6442\n",
            "\n",
            "Epoch 904/1499:\n",
            "train Loss: 6.4503 Acc: 0.6792\n",
            "\n",
            "val Loss: 7.9781 Acc: 0.6442\n",
            "\n",
            "Epoch 905/1499:\n",
            "train Loss: 6.4422 Acc: 0.6792\n",
            "\n",
            "val Loss: 7.9698 Acc: 0.6436\n",
            "\n",
            "Epoch 906/1499:\n",
            "train Loss: 6.4342 Acc: 0.6795\n",
            "\n",
            "val Loss: 7.9616 Acc: 0.6442\n",
            "\n",
            "Epoch 907/1499:\n",
            "train Loss: 6.4262 Acc: 0.6799\n",
            "\n",
            "val Loss: 7.9533 Acc: 0.6442\n",
            "\n",
            "Epoch 908/1499:\n",
            "train Loss: 6.4183 Acc: 0.6801\n",
            "\n",
            "val Loss: 7.9451 Acc: 0.6455\n",
            "\n",
            "Epoch 909/1499:\n",
            "train Loss: 6.4103 Acc: 0.6802\n",
            "\n",
            "val Loss: 7.9369 Acc: 0.6455\n",
            "\n",
            "Epoch 910/1499:\n",
            "train Loss: 6.4024 Acc: 0.6806\n",
            "\n",
            "val Loss: 7.9287 Acc: 0.6455\n",
            "\n",
            "Epoch 911/1499:\n",
            "train Loss: 6.3945 Acc: 0.6809\n",
            "\n",
            "val Loss: 7.9206 Acc: 0.6455\n",
            "\n",
            "Epoch 912/1499:\n",
            "train Loss: 6.3866 Acc: 0.6812\n",
            "\n",
            "val Loss: 7.9124 Acc: 0.6473\n",
            "\n",
            "Epoch 913/1499:\n",
            "train Loss: 6.3787 Acc: 0.6814\n",
            "\n",
            "val Loss: 7.9043 Acc: 0.6485\n",
            "\n",
            "Epoch 914/1499:\n",
            "train Loss: 6.3708 Acc: 0.6817\n",
            "\n",
            "val Loss: 7.8962 Acc: 0.6491\n",
            "\n",
            "Epoch 915/1499:\n",
            "train Loss: 6.3630 Acc: 0.6821\n",
            "\n",
            "val Loss: 7.8881 Acc: 0.6491\n",
            "\n",
            "Epoch 916/1499:\n",
            "train Loss: 6.3551 Acc: 0.6825\n",
            "\n",
            "val Loss: 7.8800 Acc: 0.6497\n",
            "\n",
            "Epoch 917/1499:\n",
            "train Loss: 6.3473 Acc: 0.6829\n",
            "\n",
            "val Loss: 7.8720 Acc: 0.6503\n",
            "\n",
            "Epoch 918/1499:\n",
            "train Loss: 6.3395 Acc: 0.6834\n",
            "\n",
            "val Loss: 7.8640 Acc: 0.6503\n",
            "\n",
            "Epoch 919/1499:\n",
            "train Loss: 6.3318 Acc: 0.6835\n",
            "\n",
            "val Loss: 7.8560 Acc: 0.6503\n",
            "\n",
            "Epoch 920/1499:\n",
            "train Loss: 6.3240 Acc: 0.6836\n",
            "\n",
            "val Loss: 7.8480 Acc: 0.6503\n",
            "\n",
            "Epoch 921/1499:\n",
            "train Loss: 6.3163 Acc: 0.6837\n",
            "\n",
            "val Loss: 7.8400 Acc: 0.6503\n",
            "\n",
            "Epoch 922/1499:\n",
            "train Loss: 6.3086 Acc: 0.6837\n",
            "\n",
            "val Loss: 7.8321 Acc: 0.6509\n",
            "\n",
            "Epoch 923/1499:\n",
            "train Loss: 6.3009 Acc: 0.6840\n",
            "\n",
            "val Loss: 7.8242 Acc: 0.6515\n",
            "\n",
            "Epoch 924/1499:\n",
            "train Loss: 6.2932 Acc: 0.6843\n",
            "\n",
            "val Loss: 7.8163 Acc: 0.6515\n",
            "\n",
            "Epoch 925/1499:\n",
            "train Loss: 6.2855 Acc: 0.6844\n",
            "\n",
            "val Loss: 7.8084 Acc: 0.6515\n",
            "\n",
            "Epoch 926/1499:\n",
            "train Loss: 6.2779 Acc: 0.6844\n",
            "\n",
            "val Loss: 7.8005 Acc: 0.6515\n",
            "\n",
            "Epoch 927/1499:\n",
            "train Loss: 6.2702 Acc: 0.6846\n",
            "\n",
            "val Loss: 7.7927 Acc: 0.6515\n",
            "\n",
            "Epoch 928/1499:\n",
            "train Loss: 6.2626 Acc: 0.6848\n",
            "\n",
            "val Loss: 7.7848 Acc: 0.6515\n",
            "\n",
            "Epoch 929/1499:\n",
            "train Loss: 6.2550 Acc: 0.6848\n",
            "\n",
            "val Loss: 7.7770 Acc: 0.6521\n",
            "\n",
            "Epoch 930/1499:\n",
            "train Loss: 6.2474 Acc: 0.6849\n",
            "\n",
            "val Loss: 7.7692 Acc: 0.6521\n",
            "\n",
            "Epoch 931/1499:\n",
            "train Loss: 6.2399 Acc: 0.6852\n",
            "\n",
            "val Loss: 7.7615 Acc: 0.6521\n",
            "\n",
            "Epoch 932/1499:\n",
            "train Loss: 6.2323 Acc: 0.6853\n",
            "\n",
            "val Loss: 7.7537 Acc: 0.6521\n",
            "\n",
            "Epoch 933/1499:\n",
            "train Loss: 6.2248 Acc: 0.6854\n",
            "\n",
            "val Loss: 7.7460 Acc: 0.6515\n",
            "\n",
            "Epoch 934/1499:\n",
            "train Loss: 6.2173 Acc: 0.6855\n",
            "\n",
            "val Loss: 7.7383 Acc: 0.6515\n",
            "\n",
            "Epoch 935/1499:\n",
            "train Loss: 6.2098 Acc: 0.6856\n",
            "\n",
            "val Loss: 7.7306 Acc: 0.6515\n",
            "\n",
            "Epoch 936/1499:\n",
            "train Loss: 6.2023 Acc: 0.6858\n",
            "\n",
            "val Loss: 7.7229 Acc: 0.6515\n",
            "\n",
            "Epoch 937/1499:\n",
            "train Loss: 6.1949 Acc: 0.6859\n",
            "\n",
            "val Loss: 7.7153 Acc: 0.6527\n",
            "\n",
            "Epoch 938/1499:\n",
            "train Loss: 6.1874 Acc: 0.6863\n",
            "\n",
            "val Loss: 7.7076 Acc: 0.6527\n",
            "\n",
            "Epoch 939/1499:\n",
            "train Loss: 6.1800 Acc: 0.6867\n",
            "\n",
            "val Loss: 7.7000 Acc: 0.6527\n",
            "\n",
            "Epoch 940/1499:\n",
            "train Loss: 6.1726 Acc: 0.6867\n",
            "\n",
            "val Loss: 7.6924 Acc: 0.6533\n",
            "\n",
            "Epoch 941/1499:\n",
            "train Loss: 6.1652 Acc: 0.6869\n",
            "\n",
            "val Loss: 7.6848 Acc: 0.6533\n",
            "\n",
            "Epoch 942/1499:\n",
            "train Loss: 6.1578 Acc: 0.6871\n",
            "\n",
            "val Loss: 7.6773 Acc: 0.6533\n",
            "\n",
            "Epoch 943/1499:\n",
            "train Loss: 6.1505 Acc: 0.6872\n",
            "\n",
            "val Loss: 7.6697 Acc: 0.6533\n",
            "\n",
            "Epoch 944/1499:\n",
            "train Loss: 6.1431 Acc: 0.6875\n",
            "\n",
            "val Loss: 7.6622 Acc: 0.6539\n",
            "\n",
            "Epoch 945/1499:\n",
            "train Loss: 6.1358 Acc: 0.6875\n",
            "\n",
            "val Loss: 7.6547 Acc: 0.6533\n",
            "\n",
            "Epoch 946/1499:\n",
            "train Loss: 6.1285 Acc: 0.6878\n",
            "\n",
            "val Loss: 7.6472 Acc: 0.6533\n",
            "\n",
            "Epoch 947/1499:\n",
            "train Loss: 6.1212 Acc: 0.6882\n",
            "\n",
            "val Loss: 7.6397 Acc: 0.6533\n",
            "\n",
            "Epoch 948/1499:\n",
            "train Loss: 6.1139 Acc: 0.6883\n",
            "\n",
            "val Loss: 7.6323 Acc: 0.6527\n",
            "\n",
            "Epoch 949/1499:\n",
            "train Loss: 6.1066 Acc: 0.6889\n",
            "\n",
            "val Loss: 7.6249 Acc: 0.6527\n",
            "\n",
            "Epoch 950/1499:\n",
            "train Loss: 6.0994 Acc: 0.6891\n",
            "\n",
            "val Loss: 7.6174 Acc: 0.6527\n",
            "\n",
            "Epoch 951/1499:\n",
            "train Loss: 6.0922 Acc: 0.6893\n",
            "\n",
            "val Loss: 7.6100 Acc: 0.6539\n",
            "\n",
            "Epoch 952/1499:\n",
            "train Loss: 6.0850 Acc: 0.6893\n",
            "\n",
            "val Loss: 7.6027 Acc: 0.6539\n",
            "\n",
            "Epoch 953/1499:\n",
            "train Loss: 6.0778 Acc: 0.6895\n",
            "\n",
            "val Loss: 7.5953 Acc: 0.6545\n",
            "\n",
            "Epoch 954/1499:\n",
            "train Loss: 6.0706 Acc: 0.6897\n",
            "\n",
            "val Loss: 7.5879 Acc: 0.6552\n",
            "\n",
            "Epoch 955/1499:\n",
            "train Loss: 6.0634 Acc: 0.6899\n",
            "\n",
            "val Loss: 7.5806 Acc: 0.6552\n",
            "\n",
            "Epoch 956/1499:\n",
            "train Loss: 6.0563 Acc: 0.6900\n",
            "\n",
            "val Loss: 7.5733 Acc: 0.6552\n",
            "\n",
            "Epoch 957/1499:\n",
            "train Loss: 6.0491 Acc: 0.6904\n",
            "\n",
            "val Loss: 7.5660 Acc: 0.6552\n",
            "\n",
            "Epoch 958/1499:\n",
            "train Loss: 6.0420 Acc: 0.6908\n",
            "\n",
            "val Loss: 7.5587 Acc: 0.6552\n",
            "\n",
            "Epoch 959/1499:\n",
            "train Loss: 6.0349 Acc: 0.6910\n",
            "\n",
            "val Loss: 7.5515 Acc: 0.6552\n",
            "\n",
            "Epoch 960/1499:\n",
            "train Loss: 6.0278 Acc: 0.6910\n",
            "\n",
            "val Loss: 7.5442 Acc: 0.6552\n",
            "\n",
            "Epoch 961/1499:\n",
            "train Loss: 6.0208 Acc: 0.6913\n",
            "\n",
            "val Loss: 7.5370 Acc: 0.6552\n",
            "\n",
            "Epoch 962/1499:\n",
            "train Loss: 6.0137 Acc: 0.6912\n",
            "\n",
            "val Loss: 7.5298 Acc: 0.6552\n",
            "\n",
            "Epoch 963/1499:\n",
            "train Loss: 6.0067 Acc: 0.6914\n",
            "\n",
            "val Loss: 7.5226 Acc: 0.6552\n",
            "\n",
            "Epoch 964/1499:\n",
            "train Loss: 5.9997 Acc: 0.6915\n",
            "\n",
            "val Loss: 7.5154 Acc: 0.6558\n",
            "\n",
            "Epoch 965/1499:\n",
            "train Loss: 5.9927 Acc: 0.6918\n",
            "\n",
            "val Loss: 7.5082 Acc: 0.6564\n",
            "\n",
            "Epoch 966/1499:\n",
            "train Loss: 5.9857 Acc: 0.6921\n",
            "\n",
            "val Loss: 7.5011 Acc: 0.6564\n",
            "\n",
            "Epoch 967/1499:\n",
            "train Loss: 5.9787 Acc: 0.6925\n",
            "\n",
            "val Loss: 7.4939 Acc: 0.6564\n",
            "\n",
            "Epoch 968/1499:\n",
            "train Loss: 5.9717 Acc: 0.6928\n",
            "\n",
            "val Loss: 7.4868 Acc: 0.6564\n",
            "\n",
            "Epoch 969/1499:\n",
            "train Loss: 5.9648 Acc: 0.6930\n",
            "\n",
            "val Loss: 7.4797 Acc: 0.6570\n",
            "\n",
            "Epoch 970/1499:\n",
            "train Loss: 5.9578 Acc: 0.6931\n",
            "\n",
            "val Loss: 7.4726 Acc: 0.6576\n",
            "\n",
            "Epoch 971/1499:\n",
            "train Loss: 5.9509 Acc: 0.6935\n",
            "\n",
            "val Loss: 7.4655 Acc: 0.6576\n",
            "\n",
            "Epoch 972/1499:\n",
            "train Loss: 5.9440 Acc: 0.6936\n",
            "\n",
            "val Loss: 7.4585 Acc: 0.6576\n",
            "\n",
            "Epoch 973/1499:\n",
            "train Loss: 5.9371 Acc: 0.6938\n",
            "\n",
            "val Loss: 7.4514 Acc: 0.6588\n",
            "\n",
            "Epoch 974/1499:\n",
            "train Loss: 5.9303 Acc: 0.6940\n",
            "\n",
            "val Loss: 7.4444 Acc: 0.6588\n",
            "\n",
            "Epoch 975/1499:\n",
            "train Loss: 5.9234 Acc: 0.6941\n",
            "\n",
            "val Loss: 7.4374 Acc: 0.6588\n",
            "\n",
            "Epoch 976/1499:\n",
            "train Loss: 5.9166 Acc: 0.6941\n",
            "\n",
            "val Loss: 7.4304 Acc: 0.6588\n",
            "\n",
            "Epoch 977/1499:\n",
            "train Loss: 5.9097 Acc: 0.6944\n",
            "\n",
            "val Loss: 7.4234 Acc: 0.6594\n",
            "\n",
            "Epoch 978/1499:\n",
            "train Loss: 5.9029 Acc: 0.6945\n",
            "\n",
            "val Loss: 7.4165 Acc: 0.6600\n",
            "\n",
            "Epoch 979/1499:\n",
            "train Loss: 5.8961 Acc: 0.6950\n",
            "\n",
            "val Loss: 7.4095 Acc: 0.6606\n",
            "\n",
            "Epoch 980/1499:\n",
            "train Loss: 5.8893 Acc: 0.6954\n",
            "\n",
            "val Loss: 7.4026 Acc: 0.6612\n",
            "\n",
            "Epoch 981/1499:\n",
            "train Loss: 5.8826 Acc: 0.6955\n",
            "\n",
            "val Loss: 7.3956 Acc: 0.6612\n",
            "\n",
            "Epoch 982/1499:\n",
            "train Loss: 5.8758 Acc: 0.6955\n",
            "\n",
            "val Loss: 7.3887 Acc: 0.6612\n",
            "\n",
            "Epoch 983/1499:\n",
            "train Loss: 5.8691 Acc: 0.6958\n",
            "\n",
            "val Loss: 7.3818 Acc: 0.6618\n",
            "\n",
            "Epoch 984/1499:\n",
            "train Loss: 5.8623 Acc: 0.6957\n",
            "\n",
            "val Loss: 7.3750 Acc: 0.6618\n",
            "\n",
            "Epoch 985/1499:\n",
            "train Loss: 5.8556 Acc: 0.6961\n",
            "\n",
            "val Loss: 7.3681 Acc: 0.6618\n",
            "\n",
            "Epoch 986/1499:\n",
            "train Loss: 5.8489 Acc: 0.6962\n",
            "\n",
            "val Loss: 7.3613 Acc: 0.6618\n",
            "\n",
            "Epoch 987/1499:\n",
            "train Loss: 5.8422 Acc: 0.6965\n",
            "\n",
            "val Loss: 7.3544 Acc: 0.6618\n",
            "\n",
            "Epoch 988/1499:\n",
            "train Loss: 5.8356 Acc: 0.6965\n",
            "\n",
            "val Loss: 7.3476 Acc: 0.6624\n",
            "\n",
            "Epoch 989/1499:\n",
            "train Loss: 5.8289 Acc: 0.6966\n",
            "\n",
            "val Loss: 7.3408 Acc: 0.6624\n",
            "\n",
            "Epoch 990/1499:\n",
            "train Loss: 5.8223 Acc: 0.6969\n",
            "\n",
            "val Loss: 7.3340 Acc: 0.6624\n",
            "\n",
            "Epoch 991/1499:\n",
            "train Loss: 5.8156 Acc: 0.6971\n",
            "\n",
            "val Loss: 7.3272 Acc: 0.6624\n",
            "\n",
            "Epoch 992/1499:\n",
            "train Loss: 5.8090 Acc: 0.6973\n",
            "\n",
            "val Loss: 7.3205 Acc: 0.6624\n",
            "\n",
            "Epoch 993/1499:\n",
            "train Loss: 5.8024 Acc: 0.6972\n",
            "\n",
            "val Loss: 7.3137 Acc: 0.6624\n",
            "\n",
            "Epoch 994/1499:\n",
            "train Loss: 5.7958 Acc: 0.6974\n",
            "\n",
            "val Loss: 7.3070 Acc: 0.6624\n",
            "\n",
            "Epoch 995/1499:\n",
            "train Loss: 5.7892 Acc: 0.6975\n",
            "\n",
            "val Loss: 7.3003 Acc: 0.6624\n",
            "\n",
            "Epoch 996/1499:\n",
            "train Loss: 5.7827 Acc: 0.6977\n",
            "\n",
            "val Loss: 7.2936 Acc: 0.6624\n",
            "\n",
            "Epoch 997/1499:\n",
            "train Loss: 5.7761 Acc: 0.6978\n",
            "\n",
            "val Loss: 7.2869 Acc: 0.6624\n",
            "\n",
            "Epoch 998/1499:\n",
            "train Loss: 5.7696 Acc: 0.6980\n",
            "\n",
            "val Loss: 7.2802 Acc: 0.6624\n",
            "\n",
            "Epoch 999/1499:\n",
            "train Loss: 5.7630 Acc: 0.6982\n",
            "\n",
            "val Loss: 7.2735 Acc: 0.6624\n",
            "\n",
            "Epoch 1000/1499:\n",
            "train Loss: 5.7565 Acc: 0.6983\n",
            "\n",
            "val Loss: 7.2669 Acc: 0.6624\n",
            "\n",
            "Epoch 1001/1499:\n",
            "train Loss: 5.7500 Acc: 0.6985\n",
            "\n",
            "val Loss: 7.2603 Acc: 0.6624\n",
            "\n",
            "Epoch 1002/1499:\n",
            "train Loss: 5.7435 Acc: 0.6987\n",
            "\n",
            "val Loss: 7.2536 Acc: 0.6630\n",
            "\n",
            "Epoch 1003/1499:\n",
            "train Loss: 5.7371 Acc: 0.6992\n",
            "\n",
            "val Loss: 7.2470 Acc: 0.6630\n",
            "\n",
            "Epoch 1004/1499:\n",
            "train Loss: 5.7306 Acc: 0.6994\n",
            "\n",
            "val Loss: 7.2404 Acc: 0.6636\n",
            "\n",
            "Epoch 1005/1499:\n",
            "train Loss: 5.7241 Acc: 0.6995\n",
            "\n",
            "val Loss: 7.2339 Acc: 0.6636\n",
            "\n",
            "Epoch 1006/1499:\n",
            "train Loss: 5.7177 Acc: 0.6998\n",
            "\n",
            "val Loss: 7.2273 Acc: 0.6636\n",
            "\n",
            "Epoch 1007/1499:\n",
            "train Loss: 5.7113 Acc: 0.7002\n",
            "\n",
            "val Loss: 7.2207 Acc: 0.6642\n",
            "\n",
            "Epoch 1008/1499:\n",
            "train Loss: 5.7049 Acc: 0.7004\n",
            "\n",
            "val Loss: 7.2142 Acc: 0.6642\n",
            "\n",
            "Epoch 1009/1499:\n",
            "train Loss: 5.6985 Acc: 0.7005\n",
            "\n",
            "val Loss: 7.2077 Acc: 0.6642\n",
            "\n",
            "Epoch 1010/1499:\n",
            "train Loss: 5.6921 Acc: 0.7007\n",
            "\n",
            "val Loss: 7.2011 Acc: 0.6642\n",
            "\n",
            "Epoch 1011/1499:\n",
            "train Loss: 5.6857 Acc: 0.7009\n",
            "\n",
            "val Loss: 7.1946 Acc: 0.6642\n",
            "\n",
            "Epoch 1012/1499:\n",
            "train Loss: 5.6793 Acc: 0.7014\n",
            "\n",
            "val Loss: 7.1882 Acc: 0.6648\n",
            "\n",
            "Epoch 1013/1499:\n",
            "train Loss: 5.6730 Acc: 0.7014\n",
            "\n",
            "val Loss: 7.1817 Acc: 0.6648\n",
            "\n",
            "Epoch 1014/1499:\n",
            "train Loss: 5.6666 Acc: 0.7014\n",
            "\n",
            "val Loss: 7.1752 Acc: 0.6655\n",
            "\n",
            "Epoch 1015/1499:\n",
            "train Loss: 5.6603 Acc: 0.7014\n",
            "\n",
            "val Loss: 7.1688 Acc: 0.6655\n",
            "\n",
            "Epoch 1016/1499:\n",
            "train Loss: 5.6540 Acc: 0.7017\n",
            "\n",
            "val Loss: 7.1623 Acc: 0.6655\n",
            "\n",
            "Epoch 1017/1499:\n",
            "train Loss: 5.6477 Acc: 0.7022\n",
            "\n",
            "val Loss: 7.1559 Acc: 0.6655\n",
            "\n",
            "Epoch 1018/1499:\n",
            "train Loss: 5.6414 Acc: 0.7024\n",
            "\n",
            "val Loss: 7.1495 Acc: 0.6655\n",
            "\n",
            "Epoch 1019/1499:\n",
            "train Loss: 5.6351 Acc: 0.7025\n",
            "\n",
            "val Loss: 7.1431 Acc: 0.6655\n",
            "\n",
            "Epoch 1020/1499:\n",
            "train Loss: 5.6289 Acc: 0.7029\n",
            "\n",
            "val Loss: 7.1367 Acc: 0.6655\n",
            "\n",
            "Epoch 1021/1499:\n",
            "train Loss: 5.6226 Acc: 0.7032\n",
            "\n",
            "val Loss: 7.1303 Acc: 0.6661\n",
            "\n",
            "Epoch 1022/1499:\n",
            "train Loss: 5.6164 Acc: 0.7032\n",
            "\n",
            "val Loss: 7.1240 Acc: 0.6661\n",
            "\n",
            "Epoch 1023/1499:\n",
            "train Loss: 5.6101 Acc: 0.7034\n",
            "\n",
            "val Loss: 7.1176 Acc: 0.6667\n",
            "\n",
            "Epoch 1024/1499:\n",
            "train Loss: 5.6039 Acc: 0.7034\n",
            "\n",
            "val Loss: 7.1113 Acc: 0.6673\n",
            "\n",
            "Epoch 1025/1499:\n",
            "train Loss: 5.5977 Acc: 0.7032\n",
            "\n",
            "val Loss: 7.1049 Acc: 0.6679\n",
            "\n",
            "Epoch 1026/1499:\n",
            "train Loss: 5.5915 Acc: 0.7035\n",
            "\n",
            "val Loss: 7.0986 Acc: 0.6679\n",
            "\n",
            "Epoch 1027/1499:\n",
            "train Loss: 5.5854 Acc: 0.7038\n",
            "\n",
            "val Loss: 7.0923 Acc: 0.6691\n",
            "\n",
            "Epoch 1028/1499:\n",
            "train Loss: 5.5792 Acc: 0.7040\n",
            "\n",
            "val Loss: 7.0860 Acc: 0.6691\n",
            "\n",
            "Epoch 1029/1499:\n",
            "train Loss: 5.5730 Acc: 0.7042\n",
            "\n",
            "val Loss: 7.0797 Acc: 0.6691\n",
            "\n",
            "Epoch 1030/1499:\n",
            "train Loss: 5.5669 Acc: 0.7045\n",
            "\n",
            "val Loss: 7.0735 Acc: 0.6697\n",
            "\n",
            "Epoch 1031/1499:\n",
            "train Loss: 5.5607 Acc: 0.7045\n",
            "\n",
            "val Loss: 7.0672 Acc: 0.6697\n",
            "\n",
            "Epoch 1032/1499:\n",
            "train Loss: 5.5546 Acc: 0.7046\n",
            "\n",
            "val Loss: 7.0610 Acc: 0.6697\n",
            "\n",
            "Epoch 1033/1499:\n",
            "train Loss: 5.5485 Acc: 0.7049\n",
            "\n",
            "val Loss: 7.0547 Acc: 0.6697\n",
            "\n",
            "Epoch 1034/1499:\n",
            "train Loss: 5.5424 Acc: 0.7048\n",
            "\n",
            "val Loss: 7.0485 Acc: 0.6697\n",
            "\n",
            "Epoch 1035/1499:\n",
            "train Loss: 5.5363 Acc: 0.7050\n",
            "\n",
            "val Loss: 7.0423 Acc: 0.6697\n",
            "\n",
            "Epoch 1036/1499:\n",
            "train Loss: 5.5303 Acc: 0.7048\n",
            "\n",
            "val Loss: 7.0361 Acc: 0.6697\n",
            "\n",
            "Epoch 1037/1499:\n",
            "train Loss: 5.5242 Acc: 0.7048\n",
            "\n",
            "val Loss: 7.0299 Acc: 0.6703\n",
            "\n",
            "Epoch 1038/1499:\n",
            "train Loss: 5.5182 Acc: 0.7049\n",
            "\n",
            "val Loss: 7.0237 Acc: 0.6703\n",
            "\n",
            "Epoch 1039/1499:\n",
            "train Loss: 5.5121 Acc: 0.7050\n",
            "\n",
            "val Loss: 7.0176 Acc: 0.6709\n",
            "\n",
            "Epoch 1040/1499:\n",
            "train Loss: 5.5061 Acc: 0.7052\n",
            "\n",
            "val Loss: 7.0114 Acc: 0.6715\n",
            "\n",
            "Epoch 1041/1499:\n",
            "train Loss: 5.5001 Acc: 0.7054\n",
            "\n",
            "val Loss: 7.0053 Acc: 0.6715\n",
            "\n",
            "Epoch 1042/1499:\n",
            "train Loss: 5.4941 Acc: 0.7054\n",
            "\n",
            "val Loss: 6.9991 Acc: 0.6715\n",
            "\n",
            "Epoch 1043/1499:\n",
            "train Loss: 5.4881 Acc: 0.7055\n",
            "\n",
            "val Loss: 6.9930 Acc: 0.6715\n",
            "\n",
            "Epoch 1044/1499:\n",
            "train Loss: 5.4821 Acc: 0.7057\n",
            "\n",
            "val Loss: 6.9869 Acc: 0.6715\n",
            "\n",
            "Epoch 1045/1499:\n",
            "train Loss: 5.4761 Acc: 0.7060\n",
            "\n",
            "val Loss: 6.9808 Acc: 0.6715\n",
            "\n",
            "Epoch 1046/1499:\n",
            "train Loss: 5.4702 Acc: 0.7064\n",
            "\n",
            "val Loss: 6.9747 Acc: 0.6715\n",
            "\n",
            "Epoch 1047/1499:\n",
            "train Loss: 5.4642 Acc: 0.7066\n",
            "\n",
            "val Loss: 6.9686 Acc: 0.6715\n",
            "\n",
            "Epoch 1048/1499:\n",
            "train Loss: 5.4583 Acc: 0.7066\n",
            "\n",
            "val Loss: 6.9625 Acc: 0.6721\n",
            "\n",
            "Epoch 1049/1499:\n",
            "train Loss: 5.4524 Acc: 0.7067\n",
            "\n",
            "val Loss: 6.9565 Acc: 0.6721\n",
            "\n",
            "Epoch 1050/1499:\n",
            "train Loss: 5.4465 Acc: 0.7069\n",
            "\n",
            "val Loss: 6.9504 Acc: 0.6721\n",
            "\n",
            "Epoch 1051/1499:\n",
            "train Loss: 5.4406 Acc: 0.7075\n",
            "\n",
            "val Loss: 6.9444 Acc: 0.6721\n",
            "\n",
            "Epoch 1052/1499:\n",
            "train Loss: 5.4347 Acc: 0.7076\n",
            "\n",
            "val Loss: 6.9384 Acc: 0.6721\n",
            "\n",
            "Epoch 1053/1499:\n",
            "train Loss: 5.4288 Acc: 0.7078\n",
            "\n",
            "val Loss: 6.9324 Acc: 0.6721\n",
            "\n",
            "Epoch 1054/1499:\n",
            "train Loss: 5.4230 Acc: 0.7081\n",
            "\n",
            "val Loss: 6.9264 Acc: 0.6721\n",
            "\n",
            "Epoch 1055/1499:\n",
            "train Loss: 5.4171 Acc: 0.7085\n",
            "\n",
            "val Loss: 6.9204 Acc: 0.6721\n",
            "\n",
            "Epoch 1056/1499:\n",
            "train Loss: 5.4113 Acc: 0.7087\n",
            "\n",
            "val Loss: 6.9144 Acc: 0.6721\n",
            "\n",
            "Epoch 1057/1499:\n",
            "train Loss: 5.4054 Acc: 0.7089\n",
            "\n",
            "val Loss: 6.9084 Acc: 0.6721\n",
            "\n",
            "Epoch 1058/1499:\n",
            "train Loss: 5.3996 Acc: 0.7092\n",
            "\n",
            "val Loss: 6.9025 Acc: 0.6721\n",
            "\n",
            "Epoch 1059/1499:\n",
            "train Loss: 5.3938 Acc: 0.7092\n",
            "\n",
            "val Loss: 6.8965 Acc: 0.6721\n",
            "\n",
            "Epoch 1060/1499:\n",
            "train Loss: 5.3880 Acc: 0.7093\n",
            "\n",
            "val Loss: 6.8906 Acc: 0.6721\n",
            "\n",
            "Epoch 1061/1499:\n",
            "train Loss: 5.3822 Acc: 0.7096\n",
            "\n",
            "val Loss: 6.8847 Acc: 0.6733\n",
            "\n",
            "Epoch 1062/1499:\n",
            "train Loss: 5.3765 Acc: 0.7097\n",
            "\n",
            "val Loss: 6.8788 Acc: 0.6733\n",
            "\n",
            "Epoch 1063/1499:\n",
            "train Loss: 5.3707 Acc: 0.7101\n",
            "\n",
            "val Loss: 6.8729 Acc: 0.6739\n",
            "\n",
            "Epoch 1064/1499:\n",
            "train Loss: 5.3649 Acc: 0.7105\n",
            "\n",
            "val Loss: 6.8670 Acc: 0.6739\n",
            "\n",
            "Epoch 1065/1499:\n",
            "train Loss: 5.3592 Acc: 0.7107\n",
            "\n",
            "val Loss: 6.8611 Acc: 0.6739\n",
            "\n",
            "Epoch 1066/1499:\n",
            "train Loss: 5.3535 Acc: 0.7108\n",
            "\n",
            "val Loss: 6.8552 Acc: 0.6739\n",
            "\n",
            "Epoch 1067/1499:\n",
            "train Loss: 5.3478 Acc: 0.7107\n",
            "\n",
            "val Loss: 6.8494 Acc: 0.6745\n",
            "\n",
            "Epoch 1068/1499:\n",
            "train Loss: 5.3421 Acc: 0.7106\n",
            "\n",
            "val Loss: 6.8435 Acc: 0.6745\n",
            "\n",
            "Epoch 1069/1499:\n",
            "train Loss: 5.3364 Acc: 0.7106\n",
            "\n",
            "val Loss: 6.8377 Acc: 0.6739\n",
            "\n",
            "Epoch 1070/1499:\n",
            "train Loss: 5.3307 Acc: 0.7106\n",
            "\n",
            "val Loss: 6.8318 Acc: 0.6745\n",
            "\n",
            "Epoch 1071/1499:\n",
            "train Loss: 5.3250 Acc: 0.7107\n",
            "\n",
            "val Loss: 6.8260 Acc: 0.6752\n",
            "\n",
            "Epoch 1072/1499:\n",
            "train Loss: 5.3193 Acc: 0.7109\n",
            "\n",
            "val Loss: 6.8202 Acc: 0.6745\n",
            "\n",
            "Epoch 1073/1499:\n",
            "train Loss: 5.3137 Acc: 0.7110\n",
            "\n",
            "val Loss: 6.8144 Acc: 0.6745\n",
            "\n",
            "Epoch 1074/1499:\n",
            "train Loss: 5.3080 Acc: 0.7111\n",
            "\n",
            "val Loss: 6.8086 Acc: 0.6745\n",
            "\n",
            "Epoch 1075/1499:\n",
            "train Loss: 5.3024 Acc: 0.7111\n",
            "\n",
            "val Loss: 6.8028 Acc: 0.6745\n",
            "\n",
            "Epoch 1076/1499:\n",
            "train Loss: 5.2968 Acc: 0.7112\n",
            "\n",
            "val Loss: 6.7971 Acc: 0.6758\n",
            "\n",
            "Epoch 1077/1499:\n",
            "train Loss: 5.2912 Acc: 0.7114\n",
            "\n",
            "val Loss: 6.7913 Acc: 0.6758\n",
            "\n",
            "Epoch 1078/1499:\n",
            "train Loss: 5.2856 Acc: 0.7115\n",
            "\n",
            "val Loss: 6.7855 Acc: 0.6752\n",
            "\n",
            "Epoch 1079/1499:\n",
            "train Loss: 5.2800 Acc: 0.7115\n",
            "\n",
            "val Loss: 6.7798 Acc: 0.6752\n",
            "\n",
            "Epoch 1080/1499:\n",
            "train Loss: 5.2744 Acc: 0.7116\n",
            "\n",
            "val Loss: 6.7741 Acc: 0.6758\n",
            "\n",
            "Epoch 1081/1499:\n",
            "train Loss: 5.2688 Acc: 0.7117\n",
            "\n",
            "val Loss: 6.7683 Acc: 0.6764\n",
            "\n",
            "Epoch 1082/1499:\n",
            "train Loss: 5.2633 Acc: 0.7117\n",
            "\n",
            "val Loss: 6.7626 Acc: 0.6770\n",
            "\n",
            "Epoch 1083/1499:\n",
            "train Loss: 5.2577 Acc: 0.7120\n",
            "\n",
            "val Loss: 6.7569 Acc: 0.6776\n",
            "\n",
            "Epoch 1084/1499:\n",
            "train Loss: 5.2522 Acc: 0.7120\n",
            "\n",
            "val Loss: 6.7512 Acc: 0.6782\n",
            "\n",
            "Epoch 1085/1499:\n",
            "train Loss: 5.2467 Acc: 0.7121\n",
            "\n",
            "val Loss: 6.7455 Acc: 0.6788\n",
            "\n",
            "Epoch 1086/1499:\n",
            "train Loss: 5.2411 Acc: 0.7124\n",
            "\n",
            "val Loss: 6.7399 Acc: 0.6782\n",
            "\n",
            "Epoch 1087/1499:\n",
            "train Loss: 5.2356 Acc: 0.7125\n",
            "\n",
            "val Loss: 6.7342 Acc: 0.6782\n",
            "\n",
            "Epoch 1088/1499:\n",
            "train Loss: 5.2301 Acc: 0.7126\n",
            "\n",
            "val Loss: 6.7285 Acc: 0.6782\n",
            "\n",
            "Epoch 1089/1499:\n",
            "train Loss: 5.2247 Acc: 0.7126\n",
            "\n",
            "val Loss: 6.7229 Acc: 0.6788\n",
            "\n",
            "Epoch 1090/1499:\n",
            "train Loss: 5.2192 Acc: 0.7126\n",
            "\n",
            "val Loss: 6.7173 Acc: 0.6788\n",
            "\n",
            "Epoch 1091/1499:\n",
            "train Loss: 5.2137 Acc: 0.7126\n",
            "\n",
            "val Loss: 6.7116 Acc: 0.6788\n",
            "\n",
            "Epoch 1092/1499:\n",
            "train Loss: 5.2082 Acc: 0.7128\n",
            "\n",
            "val Loss: 6.7060 Acc: 0.6788\n",
            "\n",
            "Epoch 1093/1499:\n",
            "train Loss: 5.2028 Acc: 0.7128\n",
            "\n",
            "val Loss: 6.7004 Acc: 0.6788\n",
            "\n",
            "Epoch 1094/1499:\n",
            "train Loss: 5.1974 Acc: 0.7129\n",
            "\n",
            "val Loss: 6.6948 Acc: 0.6794\n",
            "\n",
            "Epoch 1095/1499:\n",
            "train Loss: 5.1919 Acc: 0.7132\n",
            "\n",
            "val Loss: 6.6892 Acc: 0.6794\n",
            "\n",
            "Epoch 1096/1499:\n",
            "train Loss: 5.1865 Acc: 0.7134\n",
            "\n",
            "val Loss: 6.6836 Acc: 0.6794\n",
            "\n",
            "Epoch 1097/1499:\n",
            "train Loss: 5.1811 Acc: 0.7136\n",
            "\n",
            "val Loss: 6.6780 Acc: 0.6794\n",
            "\n",
            "Epoch 1098/1499:\n",
            "train Loss: 5.1757 Acc: 0.7140\n",
            "\n",
            "val Loss: 6.6725 Acc: 0.6794\n",
            "\n",
            "Epoch 1099/1499:\n",
            "train Loss: 5.1703 Acc: 0.7140\n",
            "\n",
            "val Loss: 6.6669 Acc: 0.6800\n",
            "\n",
            "Epoch 1100/1499:\n",
            "train Loss: 5.1649 Acc: 0.7142\n",
            "\n",
            "val Loss: 6.6613 Acc: 0.6800\n",
            "\n",
            "Epoch 1101/1499:\n",
            "train Loss: 5.1596 Acc: 0.7145\n",
            "\n",
            "val Loss: 6.6558 Acc: 0.6800\n",
            "\n",
            "Epoch 1102/1499:\n",
            "train Loss: 5.1542 Acc: 0.7145\n",
            "\n",
            "val Loss: 6.6503 Acc: 0.6806\n",
            "\n",
            "Epoch 1103/1499:\n",
            "train Loss: 5.1489 Acc: 0.7146\n",
            "\n",
            "val Loss: 6.6447 Acc: 0.6806\n",
            "\n",
            "Epoch 1104/1499:\n",
            "train Loss: 5.1435 Acc: 0.7149\n",
            "\n",
            "val Loss: 6.6392 Acc: 0.6806\n",
            "\n",
            "Epoch 1105/1499:\n",
            "train Loss: 5.1382 Acc: 0.7149\n",
            "\n",
            "val Loss: 6.6337 Acc: 0.6806\n",
            "\n",
            "Epoch 1106/1499:\n",
            "train Loss: 5.1329 Acc: 0.7151\n",
            "\n",
            "val Loss: 6.6282 Acc: 0.6806\n",
            "\n",
            "Epoch 1107/1499:\n",
            "train Loss: 5.1276 Acc: 0.7153\n",
            "\n",
            "val Loss: 6.6227 Acc: 0.6812\n",
            "\n",
            "Epoch 1108/1499:\n",
            "train Loss: 5.1223 Acc: 0.7154\n",
            "\n",
            "val Loss: 6.6172 Acc: 0.6812\n",
            "\n",
            "Epoch 1109/1499:\n",
            "train Loss: 5.1170 Acc: 0.7156\n",
            "\n",
            "val Loss: 6.6117 Acc: 0.6818\n",
            "\n",
            "Epoch 1110/1499:\n",
            "train Loss: 5.1117 Acc: 0.7156\n",
            "\n",
            "val Loss: 6.6063 Acc: 0.6818\n",
            "\n",
            "Epoch 1111/1499:\n",
            "train Loss: 5.1064 Acc: 0.7156\n",
            "\n",
            "val Loss: 6.6008 Acc: 0.6830\n",
            "\n",
            "Epoch 1112/1499:\n",
            "train Loss: 5.1011 Acc: 0.7159\n",
            "\n",
            "val Loss: 6.5953 Acc: 0.6830\n",
            "\n",
            "Epoch 1113/1499:\n",
            "train Loss: 5.0959 Acc: 0.7162\n",
            "\n",
            "val Loss: 6.5899 Acc: 0.6836\n",
            "\n",
            "Epoch 1114/1499:\n",
            "train Loss: 5.0906 Acc: 0.7162\n",
            "\n",
            "val Loss: 6.5844 Acc: 0.6836\n",
            "\n",
            "Epoch 1115/1499:\n",
            "train Loss: 5.0854 Acc: 0.7162\n",
            "\n",
            "val Loss: 6.5790 Acc: 0.6836\n",
            "\n",
            "Epoch 1116/1499:\n",
            "train Loss: 5.0802 Acc: 0.7162\n",
            "\n",
            "val Loss: 6.5736 Acc: 0.6836\n",
            "\n",
            "Epoch 1117/1499:\n",
            "train Loss: 5.0750 Acc: 0.7163\n",
            "\n",
            "val Loss: 6.5682 Acc: 0.6836\n",
            "\n",
            "Epoch 1118/1499:\n",
            "train Loss: 5.0698 Acc: 0.7164\n",
            "\n",
            "val Loss: 6.5628 Acc: 0.6842\n",
            "\n",
            "Epoch 1119/1499:\n",
            "train Loss: 5.0646 Acc: 0.7165\n",
            "\n",
            "val Loss: 6.5574 Acc: 0.6842\n",
            "\n",
            "Epoch 1120/1499:\n",
            "train Loss: 5.0594 Acc: 0.7167\n",
            "\n",
            "val Loss: 6.5520 Acc: 0.6842\n",
            "\n",
            "Epoch 1121/1499:\n",
            "train Loss: 5.0542 Acc: 0.7166\n",
            "\n",
            "val Loss: 6.5466 Acc: 0.6842\n",
            "\n",
            "Epoch 1122/1499:\n",
            "train Loss: 5.0490 Acc: 0.7170\n",
            "\n",
            "val Loss: 6.5412 Acc: 0.6842\n",
            "\n",
            "Epoch 1123/1499:\n",
            "train Loss: 5.0439 Acc: 0.7171\n",
            "\n",
            "val Loss: 6.5358 Acc: 0.6842\n",
            "\n",
            "Epoch 1124/1499:\n",
            "train Loss: 5.0387 Acc: 0.7172\n",
            "\n",
            "val Loss: 6.5305 Acc: 0.6848\n",
            "\n",
            "Epoch 1125/1499:\n",
            "train Loss: 5.0336 Acc: 0.7172\n",
            "\n",
            "val Loss: 6.5251 Acc: 0.6848\n",
            "\n",
            "Epoch 1126/1499:\n",
            "train Loss: 5.0284 Acc: 0.7172\n",
            "\n",
            "val Loss: 6.5198 Acc: 0.6848\n",
            "\n",
            "Epoch 1127/1499:\n",
            "train Loss: 5.0233 Acc: 0.7175\n",
            "\n",
            "val Loss: 6.5144 Acc: 0.6848\n",
            "\n",
            "Epoch 1128/1499:\n",
            "train Loss: 5.0182 Acc: 0.7175\n",
            "\n",
            "val Loss: 6.5091 Acc: 0.6848\n",
            "\n",
            "Epoch 1129/1499:\n",
            "train Loss: 5.0131 Acc: 0.7178\n",
            "\n",
            "val Loss: 6.5038 Acc: 0.6855\n",
            "\n",
            "Epoch 1130/1499:\n",
            "train Loss: 5.0080 Acc: 0.7181\n",
            "\n",
            "val Loss: 6.4984 Acc: 0.6855\n",
            "\n",
            "Epoch 1131/1499:\n",
            "train Loss: 5.0029 Acc: 0.7181\n",
            "\n",
            "val Loss: 6.4931 Acc: 0.6855\n",
            "\n",
            "Epoch 1132/1499:\n",
            "train Loss: 4.9978 Acc: 0.7182\n",
            "\n",
            "val Loss: 6.4878 Acc: 0.6855\n",
            "\n",
            "Epoch 1133/1499:\n",
            "train Loss: 4.9927 Acc: 0.7184\n",
            "\n",
            "val Loss: 6.4825 Acc: 0.6855\n",
            "\n",
            "Epoch 1134/1499:\n",
            "train Loss: 4.9877 Acc: 0.7185\n",
            "\n",
            "val Loss: 6.4772 Acc: 0.6855\n",
            "\n",
            "Epoch 1135/1499:\n",
            "train Loss: 4.9826 Acc: 0.7185\n",
            "\n",
            "val Loss: 6.4720 Acc: 0.6855\n",
            "\n",
            "Epoch 1136/1499:\n",
            "train Loss: 4.9776 Acc: 0.7185\n",
            "\n",
            "val Loss: 6.4667 Acc: 0.6855\n",
            "\n",
            "Epoch 1137/1499:\n",
            "train Loss: 4.9725 Acc: 0.7185\n",
            "\n",
            "val Loss: 6.4614 Acc: 0.6855\n",
            "\n",
            "Epoch 1138/1499:\n",
            "train Loss: 4.9675 Acc: 0.7185\n",
            "\n",
            "val Loss: 6.4561 Acc: 0.6855\n",
            "\n",
            "Epoch 1139/1499:\n",
            "train Loss: 4.9625 Acc: 0.7186\n",
            "\n",
            "val Loss: 6.4509 Acc: 0.6855\n",
            "\n",
            "Epoch 1140/1499:\n",
            "train Loss: 4.9575 Acc: 0.7185\n",
            "\n",
            "val Loss: 6.4456 Acc: 0.6855\n",
            "\n",
            "Epoch 1141/1499:\n",
            "train Loss: 4.9525 Acc: 0.7187\n",
            "\n",
            "val Loss: 6.4404 Acc: 0.6855\n",
            "\n",
            "Epoch 1142/1499:\n",
            "train Loss: 4.9475 Acc: 0.7191\n",
            "\n",
            "val Loss: 6.4352 Acc: 0.6855\n",
            "\n",
            "Epoch 1143/1499:\n",
            "train Loss: 4.9425 Acc: 0.7191\n",
            "\n",
            "val Loss: 6.4299 Acc: 0.6861\n",
            "\n",
            "Epoch 1144/1499:\n",
            "train Loss: 4.9375 Acc: 0.7193\n",
            "\n",
            "val Loss: 6.4247 Acc: 0.6861\n",
            "\n",
            "Epoch 1145/1499:\n",
            "train Loss: 4.9325 Acc: 0.7193\n",
            "\n",
            "val Loss: 6.4195 Acc: 0.6861\n",
            "\n",
            "Epoch 1146/1499:\n",
            "train Loss: 4.9276 Acc: 0.7193\n",
            "\n",
            "val Loss: 6.4143 Acc: 0.6861\n",
            "\n",
            "Epoch 1147/1499:\n",
            "train Loss: 4.9226 Acc: 0.7195\n",
            "\n",
            "val Loss: 6.4091 Acc: 0.6861\n",
            "\n",
            "Epoch 1148/1499:\n",
            "train Loss: 4.9177 Acc: 0.7197\n",
            "\n",
            "val Loss: 6.4039 Acc: 0.6867\n",
            "\n",
            "Epoch 1149/1499:\n",
            "train Loss: 4.9128 Acc: 0.7198\n",
            "\n",
            "val Loss: 6.3987 Acc: 0.6867\n",
            "\n",
            "Epoch 1150/1499:\n",
            "train Loss: 4.9078 Acc: 0.7199\n",
            "\n",
            "val Loss: 6.3935 Acc: 0.6867\n",
            "\n",
            "Epoch 1151/1499:\n",
            "train Loss: 4.9029 Acc: 0.7198\n",
            "\n",
            "val Loss: 6.3884 Acc: 0.6867\n",
            "\n",
            "Epoch 1152/1499:\n",
            "train Loss: 4.8980 Acc: 0.7199\n",
            "\n",
            "val Loss: 6.3832 Acc: 0.6873\n",
            "\n",
            "Epoch 1153/1499:\n",
            "train Loss: 4.8931 Acc: 0.7199\n",
            "\n",
            "val Loss: 6.3781 Acc: 0.6873\n",
            "\n",
            "Epoch 1154/1499:\n",
            "train Loss: 4.8882 Acc: 0.7200\n",
            "\n",
            "val Loss: 6.3729 Acc: 0.6873\n",
            "\n",
            "Epoch 1155/1499:\n",
            "train Loss: 4.8833 Acc: 0.7201\n",
            "\n",
            "val Loss: 6.3678 Acc: 0.6879\n",
            "\n",
            "Epoch 1156/1499:\n",
            "train Loss: 4.8785 Acc: 0.7204\n",
            "\n",
            "val Loss: 6.3626 Acc: 0.6879\n",
            "\n",
            "Epoch 1157/1499:\n",
            "train Loss: 4.8736 Acc: 0.7204\n",
            "\n",
            "val Loss: 6.3575 Acc: 0.6879\n",
            "\n",
            "Epoch 1158/1499:\n",
            "train Loss: 4.8687 Acc: 0.7205\n",
            "\n",
            "val Loss: 6.3524 Acc: 0.6879\n",
            "\n",
            "Epoch 1159/1499:\n",
            "train Loss: 4.8639 Acc: 0.7206\n",
            "\n",
            "val Loss: 6.3473 Acc: 0.6879\n",
            "\n",
            "Epoch 1160/1499:\n",
            "train Loss: 4.8591 Acc: 0.7207\n",
            "\n",
            "val Loss: 6.3421 Acc: 0.6879\n",
            "\n",
            "Epoch 1161/1499:\n",
            "train Loss: 4.8542 Acc: 0.7207\n",
            "\n",
            "val Loss: 6.3370 Acc: 0.6879\n",
            "\n",
            "Epoch 1162/1499:\n",
            "train Loss: 4.8494 Acc: 0.7209\n",
            "\n",
            "val Loss: 6.3319 Acc: 0.6879\n",
            "\n",
            "Epoch 1163/1499:\n",
            "train Loss: 4.8446 Acc: 0.7213\n",
            "\n",
            "val Loss: 6.3269 Acc: 0.6879\n",
            "\n",
            "Epoch 1164/1499:\n",
            "train Loss: 4.8398 Acc: 0.7214\n",
            "\n",
            "val Loss: 6.3218 Acc: 0.6879\n",
            "\n",
            "Epoch 1165/1499:\n",
            "train Loss: 4.8350 Acc: 0.7215\n",
            "\n",
            "val Loss: 6.3167 Acc: 0.6879\n",
            "\n",
            "Epoch 1166/1499:\n",
            "train Loss: 4.8302 Acc: 0.7217\n",
            "\n",
            "val Loss: 6.3116 Acc: 0.6879\n",
            "\n",
            "Epoch 1167/1499:\n",
            "train Loss: 4.8254 Acc: 0.7216\n",
            "\n",
            "val Loss: 6.3066 Acc: 0.6879\n",
            "\n",
            "Epoch 1168/1499:\n",
            "train Loss: 4.8206 Acc: 0.7217\n",
            "\n",
            "val Loss: 6.3015 Acc: 0.6879\n",
            "\n",
            "Epoch 1169/1499:\n",
            "train Loss: 4.8159 Acc: 0.7219\n",
            "\n",
            "val Loss: 6.2965 Acc: 0.6879\n",
            "\n",
            "Epoch 1170/1499:\n",
            "train Loss: 4.8111 Acc: 0.7220\n",
            "\n",
            "val Loss: 6.2914 Acc: 0.6879\n",
            "\n",
            "Epoch 1171/1499:\n",
            "train Loss: 4.8063 Acc: 0.7223\n",
            "\n",
            "val Loss: 6.2864 Acc: 0.6879\n",
            "\n",
            "Epoch 1172/1499:\n",
            "train Loss: 4.8016 Acc: 0.7225\n",
            "\n",
            "val Loss: 6.2813 Acc: 0.6879\n",
            "\n",
            "Epoch 1173/1499:\n",
            "train Loss: 4.7969 Acc: 0.7226\n",
            "\n",
            "val Loss: 6.2763 Acc: 0.6879\n",
            "\n",
            "Epoch 1174/1499:\n",
            "train Loss: 4.7921 Acc: 0.7227\n",
            "\n",
            "val Loss: 6.2713 Acc: 0.6879\n",
            "\n",
            "Epoch 1175/1499:\n",
            "train Loss: 4.7874 Acc: 0.7229\n",
            "\n",
            "val Loss: 6.2663 Acc: 0.6885\n",
            "\n",
            "Epoch 1176/1499:\n",
            "train Loss: 4.7827 Acc: 0.7231\n",
            "\n",
            "val Loss: 6.2613 Acc: 0.6885\n",
            "\n",
            "Epoch 1177/1499:\n",
            "train Loss: 4.7780 Acc: 0.7235\n",
            "\n",
            "val Loss: 6.2563 Acc: 0.6891\n",
            "\n",
            "Epoch 1178/1499:\n",
            "train Loss: 4.7733 Acc: 0.7235\n",
            "\n",
            "val Loss: 6.2513 Acc: 0.6891\n",
            "\n",
            "Epoch 1179/1499:\n",
            "train Loss: 4.7686 Acc: 0.7238\n",
            "\n",
            "val Loss: 6.2463 Acc: 0.6885\n",
            "\n",
            "Epoch 1180/1499:\n",
            "train Loss: 4.7639 Acc: 0.7238\n",
            "\n",
            "val Loss: 6.2413 Acc: 0.6885\n",
            "\n",
            "Epoch 1181/1499:\n",
            "train Loss: 4.7593 Acc: 0.7238\n",
            "\n",
            "val Loss: 6.2364 Acc: 0.6885\n",
            "\n",
            "Epoch 1182/1499:\n",
            "train Loss: 4.7546 Acc: 0.7239\n",
            "\n",
            "val Loss: 6.2314 Acc: 0.6885\n",
            "\n",
            "Epoch 1183/1499:\n",
            "train Loss: 4.7499 Acc: 0.7241\n",
            "\n",
            "val Loss: 6.2264 Acc: 0.6885\n",
            "\n",
            "Epoch 1184/1499:\n",
            "train Loss: 4.7453 Acc: 0.7243\n",
            "\n",
            "val Loss: 6.2215 Acc: 0.6885\n",
            "\n",
            "Epoch 1185/1499:\n",
            "train Loss: 4.7406 Acc: 0.7244\n",
            "\n",
            "val Loss: 6.2165 Acc: 0.6885\n",
            "\n",
            "Epoch 1186/1499:\n",
            "train Loss: 4.7360 Acc: 0.7244\n",
            "\n",
            "val Loss: 6.2116 Acc: 0.6879\n",
            "\n",
            "Epoch 1187/1499:\n",
            "train Loss: 4.7314 Acc: 0.7246\n",
            "\n",
            "val Loss: 6.2067 Acc: 0.6879\n",
            "\n",
            "Epoch 1188/1499:\n",
            "train Loss: 4.7268 Acc: 0.7249\n",
            "\n",
            "val Loss: 6.2017 Acc: 0.6879\n",
            "\n",
            "Epoch 1189/1499:\n",
            "train Loss: 4.7221 Acc: 0.7251\n",
            "\n",
            "val Loss: 6.1968 Acc: 0.6879\n",
            "\n",
            "Epoch 1190/1499:\n",
            "train Loss: 4.7175 Acc: 0.7254\n",
            "\n",
            "val Loss: 6.1919 Acc: 0.6873\n",
            "\n",
            "Epoch 1191/1499:\n",
            "train Loss: 4.7129 Acc: 0.7254\n",
            "\n",
            "val Loss: 6.1870 Acc: 0.6873\n",
            "\n",
            "Epoch 1192/1499:\n",
            "train Loss: 4.7083 Acc: 0.7252\n",
            "\n",
            "val Loss: 6.1821 Acc: 0.6873\n",
            "\n",
            "Epoch 1193/1499:\n",
            "train Loss: 4.7038 Acc: 0.7252\n",
            "\n",
            "val Loss: 6.1772 Acc: 0.6873\n",
            "\n",
            "Epoch 1194/1499:\n",
            "train Loss: 4.6992 Acc: 0.7253\n",
            "\n",
            "val Loss: 6.1723 Acc: 0.6873\n",
            "\n",
            "Epoch 1195/1499:\n",
            "train Loss: 4.6946 Acc: 0.7255\n",
            "\n",
            "val Loss: 6.1674 Acc: 0.6873\n",
            "\n",
            "Epoch 1196/1499:\n",
            "train Loss: 4.6901 Acc: 0.7258\n",
            "\n",
            "val Loss: 6.1626 Acc: 0.6873\n",
            "\n",
            "Epoch 1197/1499:\n",
            "train Loss: 4.6855 Acc: 0.7257\n",
            "\n",
            "val Loss: 6.1577 Acc: 0.6873\n",
            "\n",
            "Epoch 1198/1499:\n",
            "train Loss: 4.6809 Acc: 0.7260\n",
            "\n",
            "val Loss: 6.1528 Acc: 0.6873\n",
            "\n",
            "Epoch 1199/1499:\n",
            "train Loss: 4.6764 Acc: 0.7263\n",
            "\n",
            "val Loss: 6.1480 Acc: 0.6873\n",
            "\n",
            "Epoch 1200/1499:\n",
            "train Loss: 4.6719 Acc: 0.7264\n",
            "\n",
            "val Loss: 6.1431 Acc: 0.6873\n",
            "\n",
            "Epoch 1201/1499:\n",
            "train Loss: 4.6673 Acc: 0.7266\n",
            "\n",
            "val Loss: 6.1383 Acc: 0.6873\n",
            "\n",
            "Epoch 1202/1499:\n",
            "train Loss: 4.6628 Acc: 0.7265\n",
            "\n",
            "val Loss: 6.1334 Acc: 0.6873\n",
            "\n",
            "Epoch 1203/1499:\n",
            "train Loss: 4.6583 Acc: 0.7265\n",
            "\n",
            "val Loss: 6.1286 Acc: 0.6873\n",
            "\n",
            "Epoch 1204/1499:\n",
            "train Loss: 4.6538 Acc: 0.7265\n",
            "\n",
            "val Loss: 6.1238 Acc: 0.6873\n",
            "\n",
            "Epoch 1205/1499:\n",
            "train Loss: 4.6493 Acc: 0.7267\n",
            "\n",
            "val Loss: 6.1189 Acc: 0.6873\n",
            "\n",
            "Epoch 1206/1499:\n",
            "train Loss: 4.6448 Acc: 0.7267\n",
            "\n",
            "val Loss: 6.1141 Acc: 0.6873\n",
            "\n",
            "Epoch 1207/1499:\n",
            "train Loss: 4.6403 Acc: 0.7268\n",
            "\n",
            "val Loss: 6.1093 Acc: 0.6879\n",
            "\n",
            "Epoch 1208/1499:\n",
            "train Loss: 4.6358 Acc: 0.7269\n",
            "\n",
            "val Loss: 6.1045 Acc: 0.6885\n",
            "\n",
            "Epoch 1209/1499:\n",
            "train Loss: 4.6314 Acc: 0.7270\n",
            "\n",
            "val Loss: 6.0997 Acc: 0.6885\n",
            "\n",
            "Epoch 1210/1499:\n",
            "train Loss: 4.6269 Acc: 0.7271\n",
            "\n",
            "val Loss: 6.0949 Acc: 0.6885\n",
            "\n",
            "Epoch 1211/1499:\n",
            "train Loss: 4.6224 Acc: 0.7273\n",
            "\n",
            "val Loss: 6.0901 Acc: 0.6885\n",
            "\n",
            "Epoch 1212/1499:\n",
            "train Loss: 4.6180 Acc: 0.7274\n",
            "\n",
            "val Loss: 6.0854 Acc: 0.6891\n",
            "\n",
            "Epoch 1213/1499:\n",
            "train Loss: 4.6135 Acc: 0.7274\n",
            "\n",
            "val Loss: 6.0806 Acc: 0.6891\n",
            "\n",
            "Epoch 1214/1499:\n",
            "train Loss: 4.6091 Acc: 0.7274\n",
            "\n",
            "val Loss: 6.0758 Acc: 0.6891\n",
            "\n",
            "Epoch 1215/1499:\n",
            "train Loss: 4.6047 Acc: 0.7274\n",
            "\n",
            "val Loss: 6.0711 Acc: 0.6885\n",
            "\n",
            "Epoch 1216/1499:\n",
            "train Loss: 4.6002 Acc: 0.7274\n",
            "\n",
            "val Loss: 6.0663 Acc: 0.6885\n",
            "\n",
            "Epoch 1217/1499:\n",
            "train Loss: 4.5958 Acc: 0.7275\n",
            "\n",
            "val Loss: 6.0616 Acc: 0.6885\n",
            "\n",
            "Epoch 1218/1499:\n",
            "train Loss: 4.5914 Acc: 0.7275\n",
            "\n",
            "val Loss: 6.0569 Acc: 0.6885\n",
            "\n",
            "Epoch 1219/1499:\n",
            "train Loss: 4.5870 Acc: 0.7273\n",
            "\n",
            "val Loss: 6.0521 Acc: 0.6885\n",
            "\n",
            "Epoch 1220/1499:\n",
            "train Loss: 4.5826 Acc: 0.7273\n",
            "\n",
            "val Loss: 6.0474 Acc: 0.6891\n",
            "\n",
            "Epoch 1221/1499:\n",
            "train Loss: 4.5782 Acc: 0.7275\n",
            "\n",
            "val Loss: 6.0427 Acc: 0.6891\n",
            "\n",
            "Epoch 1222/1499:\n",
            "train Loss: 4.5738 Acc: 0.7276\n",
            "\n",
            "val Loss: 6.0380 Acc: 0.6891\n",
            "\n",
            "Epoch 1223/1499:\n",
            "train Loss: 4.5694 Acc: 0.7280\n",
            "\n",
            "val Loss: 6.0333 Acc: 0.6891\n",
            "\n",
            "Epoch 1224/1499:\n",
            "train Loss: 4.5650 Acc: 0.7282\n",
            "\n",
            "val Loss: 6.0286 Acc: 0.6891\n",
            "\n",
            "Epoch 1225/1499:\n",
            "train Loss: 4.5607 Acc: 0.7284\n",
            "\n",
            "val Loss: 6.0239 Acc: 0.6891\n",
            "\n",
            "Epoch 1226/1499:\n",
            "train Loss: 4.5563 Acc: 0.7284\n",
            "\n",
            "val Loss: 6.0192 Acc: 0.6897\n",
            "\n",
            "Epoch 1227/1499:\n",
            "train Loss: 4.5520 Acc: 0.7285\n",
            "\n",
            "val Loss: 6.0145 Acc: 0.6897\n",
            "\n",
            "Epoch 1228/1499:\n",
            "train Loss: 4.5476 Acc: 0.7287\n",
            "\n",
            "val Loss: 6.0098 Acc: 0.6903\n",
            "\n",
            "Epoch 1229/1499:\n",
            "train Loss: 4.5433 Acc: 0.7287\n",
            "\n",
            "val Loss: 6.0052 Acc: 0.6909\n",
            "\n",
            "Epoch 1230/1499:\n",
            "train Loss: 4.5389 Acc: 0.7289\n",
            "\n",
            "val Loss: 6.0005 Acc: 0.6909\n",
            "\n",
            "Epoch 1231/1499:\n",
            "train Loss: 4.5346 Acc: 0.7289\n",
            "\n",
            "val Loss: 5.9959 Acc: 0.6909\n",
            "\n",
            "Epoch 1232/1499:\n",
            "train Loss: 4.5303 Acc: 0.7289\n",
            "\n",
            "val Loss: 5.9912 Acc: 0.6909\n",
            "\n",
            "Epoch 1233/1499:\n",
            "train Loss: 4.5259 Acc: 0.7289\n",
            "\n",
            "val Loss: 5.9866 Acc: 0.6909\n",
            "\n",
            "Epoch 1234/1499:\n",
            "train Loss: 4.5216 Acc: 0.7294\n",
            "\n",
            "val Loss: 5.9820 Acc: 0.6909\n",
            "\n",
            "Epoch 1235/1499:\n",
            "train Loss: 4.5173 Acc: 0.7295\n",
            "\n",
            "val Loss: 5.9773 Acc: 0.6909\n",
            "\n",
            "Epoch 1236/1499:\n",
            "train Loss: 4.5130 Acc: 0.7295\n",
            "\n",
            "val Loss: 5.9727 Acc: 0.6909\n",
            "\n",
            "Epoch 1237/1499:\n",
            "train Loss: 4.5087 Acc: 0.7295\n",
            "\n",
            "val Loss: 5.9681 Acc: 0.6909\n",
            "\n",
            "Epoch 1238/1499:\n",
            "train Loss: 4.5044 Acc: 0.7295\n",
            "\n",
            "val Loss: 5.9635 Acc: 0.6909\n",
            "\n",
            "Epoch 1239/1499:\n",
            "train Loss: 4.5002 Acc: 0.7296\n",
            "\n",
            "val Loss: 5.9589 Acc: 0.6909\n",
            "\n",
            "Epoch 1240/1499:\n",
            "train Loss: 4.4959 Acc: 0.7296\n",
            "\n",
            "val Loss: 5.9543 Acc: 0.6909\n",
            "\n",
            "Epoch 1241/1499:\n",
            "train Loss: 4.4916 Acc: 0.7295\n",
            "\n",
            "val Loss: 5.9497 Acc: 0.6915\n",
            "\n",
            "Epoch 1242/1499:\n",
            "train Loss: 4.4873 Acc: 0.7296\n",
            "\n",
            "val Loss: 5.9451 Acc: 0.6915\n",
            "\n",
            "Epoch 1243/1499:\n",
            "train Loss: 4.4831 Acc: 0.7297\n",
            "\n",
            "val Loss: 5.9405 Acc: 0.6915\n",
            "\n",
            "Epoch 1244/1499:\n",
            "train Loss: 4.4788 Acc: 0.7299\n",
            "\n",
            "val Loss: 5.9360 Acc: 0.6915\n",
            "\n",
            "Epoch 1245/1499:\n",
            "train Loss: 4.4746 Acc: 0.7301\n",
            "\n",
            "val Loss: 5.9314 Acc: 0.6915\n",
            "\n",
            "Epoch 1246/1499:\n",
            "train Loss: 4.4703 Acc: 0.7302\n",
            "\n",
            "val Loss: 5.9268 Acc: 0.6915\n",
            "\n",
            "Epoch 1247/1499:\n",
            "train Loss: 4.4661 Acc: 0.7301\n",
            "\n",
            "val Loss: 5.9223 Acc: 0.6909\n",
            "\n",
            "Epoch 1248/1499:\n",
            "train Loss: 4.4619 Acc: 0.7302\n",
            "\n",
            "val Loss: 5.9177 Acc: 0.6909\n",
            "\n",
            "Epoch 1249/1499:\n",
            "train Loss: 4.4577 Acc: 0.7304\n",
            "\n",
            "val Loss: 5.9132 Acc: 0.6909\n",
            "\n",
            "Epoch 1250/1499:\n",
            "train Loss: 4.4534 Acc: 0.7305\n",
            "\n",
            "val Loss: 5.9087 Acc: 0.6915\n",
            "\n",
            "Epoch 1251/1499:\n",
            "train Loss: 4.4492 Acc: 0.7305\n",
            "\n",
            "val Loss: 5.9041 Acc: 0.6915\n",
            "\n",
            "Epoch 1252/1499:\n",
            "train Loss: 4.4450 Acc: 0.7306\n",
            "\n",
            "val Loss: 5.8996 Acc: 0.6915\n",
            "\n",
            "Epoch 1253/1499:\n",
            "train Loss: 4.4408 Acc: 0.7307\n",
            "\n",
            "val Loss: 5.8951 Acc: 0.6915\n",
            "\n",
            "Epoch 1254/1499:\n",
            "train Loss: 4.4366 Acc: 0.7309\n",
            "\n",
            "val Loss: 5.8906 Acc: 0.6915\n",
            "\n",
            "Epoch 1255/1499:\n",
            "train Loss: 4.4324 Acc: 0.7310\n",
            "\n",
            "val Loss: 5.8861 Acc: 0.6909\n",
            "\n",
            "Epoch 1256/1499:\n",
            "train Loss: 4.4283 Acc: 0.7311\n",
            "\n",
            "val Loss: 5.8816 Acc: 0.6909\n",
            "\n",
            "Epoch 1257/1499:\n",
            "train Loss: 4.4241 Acc: 0.7312\n",
            "\n",
            "val Loss: 5.8771 Acc: 0.6909\n",
            "\n",
            "Epoch 1258/1499:\n",
            "train Loss: 4.4199 Acc: 0.7314\n",
            "\n",
            "val Loss: 5.8726 Acc: 0.6909\n",
            "\n",
            "Epoch 1259/1499:\n",
            "train Loss: 4.4157 Acc: 0.7316\n",
            "\n",
            "val Loss: 5.8682 Acc: 0.6915\n",
            "\n",
            "Epoch 1260/1499:\n",
            "train Loss: 4.4116 Acc: 0.7316\n",
            "\n",
            "val Loss: 5.8637 Acc: 0.6915\n",
            "\n",
            "Epoch 1261/1499:\n",
            "train Loss: 4.4074 Acc: 0.7317\n",
            "\n",
            "val Loss: 5.8592 Acc: 0.6915\n",
            "\n",
            "Epoch 1262/1499:\n",
            "train Loss: 4.4033 Acc: 0.7317\n",
            "\n",
            "val Loss: 5.8548 Acc: 0.6915\n",
            "\n",
            "Epoch 1263/1499:\n",
            "train Loss: 4.3991 Acc: 0.7316\n",
            "\n",
            "val Loss: 5.8503 Acc: 0.6921\n",
            "\n",
            "Epoch 1264/1499:\n",
            "train Loss: 4.3950 Acc: 0.7319\n",
            "\n",
            "val Loss: 5.8459 Acc: 0.6921\n",
            "\n",
            "Epoch 1265/1499:\n",
            "train Loss: 4.3909 Acc: 0.7319\n",
            "\n",
            "val Loss: 5.8414 Acc: 0.6921\n",
            "\n",
            "Epoch 1266/1499:\n",
            "train Loss: 4.3868 Acc: 0.7320\n",
            "\n",
            "val Loss: 5.8370 Acc: 0.6915\n",
            "\n",
            "Epoch 1267/1499:\n",
            "train Loss: 4.3826 Acc: 0.7320\n",
            "\n",
            "val Loss: 5.8326 Acc: 0.6915\n",
            "\n",
            "Epoch 1268/1499:\n",
            "train Loss: 4.3785 Acc: 0.7321\n",
            "\n",
            "val Loss: 5.8281 Acc: 0.6915\n",
            "\n",
            "Epoch 1269/1499:\n",
            "train Loss: 4.3744 Acc: 0.7321\n",
            "\n",
            "val Loss: 5.8237 Acc: 0.6915\n",
            "\n",
            "Epoch 1270/1499:\n",
            "train Loss: 4.3703 Acc: 0.7322\n",
            "\n",
            "val Loss: 5.8193 Acc: 0.6915\n",
            "\n",
            "Epoch 1271/1499:\n",
            "train Loss: 4.3662 Acc: 0.7325\n",
            "\n",
            "val Loss: 5.8149 Acc: 0.6915\n",
            "\n",
            "Epoch 1272/1499:\n",
            "train Loss: 4.3621 Acc: 0.7325\n",
            "\n",
            "val Loss: 5.8105 Acc: 0.6915\n",
            "\n",
            "Epoch 1273/1499:\n",
            "train Loss: 4.3580 Acc: 0.7325\n",
            "\n",
            "val Loss: 5.8061 Acc: 0.6915\n",
            "\n",
            "Epoch 1274/1499:\n",
            "train Loss: 4.3540 Acc: 0.7325\n",
            "\n",
            "val Loss: 5.8017 Acc: 0.6921\n",
            "\n",
            "Epoch 1275/1499:\n",
            "train Loss: 4.3499 Acc: 0.7324\n",
            "\n",
            "val Loss: 5.7974 Acc: 0.6921\n",
            "\n",
            "Epoch 1276/1499:\n",
            "train Loss: 4.3458 Acc: 0.7325\n",
            "\n",
            "val Loss: 5.7930 Acc: 0.6921\n",
            "\n",
            "Epoch 1277/1499:\n",
            "train Loss: 4.3417 Acc: 0.7325\n",
            "\n",
            "val Loss: 5.7886 Acc: 0.6921\n",
            "\n",
            "Epoch 1278/1499:\n",
            "train Loss: 4.3377 Acc: 0.7325\n",
            "\n",
            "val Loss: 5.7843 Acc: 0.6921\n",
            "\n",
            "Epoch 1279/1499:\n",
            "train Loss: 4.3336 Acc: 0.7325\n",
            "\n",
            "val Loss: 5.7799 Acc: 0.6921\n",
            "\n",
            "Epoch 1280/1499:\n",
            "train Loss: 4.3296 Acc: 0.7328\n",
            "\n",
            "val Loss: 5.7755 Acc: 0.6921\n",
            "\n",
            "Epoch 1281/1499:\n",
            "train Loss: 4.3255 Acc: 0.7328\n",
            "\n",
            "val Loss: 5.7712 Acc: 0.6921\n",
            "\n",
            "Epoch 1282/1499:\n",
            "train Loss: 4.3215 Acc: 0.7330\n",
            "\n",
            "val Loss: 5.7669 Acc: 0.6921\n",
            "\n",
            "Epoch 1283/1499:\n",
            "train Loss: 4.3175 Acc: 0.7331\n",
            "\n",
            "val Loss: 5.7625 Acc: 0.6927\n",
            "\n",
            "Epoch 1284/1499:\n",
            "train Loss: 4.3134 Acc: 0.7331\n",
            "\n",
            "val Loss: 5.7582 Acc: 0.6927\n",
            "\n",
            "Epoch 1285/1499:\n",
            "train Loss: 4.3094 Acc: 0.7332\n",
            "\n",
            "val Loss: 5.7539 Acc: 0.6927\n",
            "\n",
            "Epoch 1286/1499:\n",
            "train Loss: 4.3054 Acc: 0.7334\n",
            "\n",
            "val Loss: 5.7496 Acc: 0.6927\n",
            "\n",
            "Epoch 1287/1499:\n",
            "train Loss: 4.3014 Acc: 0.7336\n",
            "\n",
            "val Loss: 5.7453 Acc: 0.6927\n",
            "\n",
            "Epoch 1288/1499:\n",
            "train Loss: 4.2974 Acc: 0.7336\n",
            "\n",
            "val Loss: 5.7410 Acc: 0.6927\n",
            "\n",
            "Epoch 1289/1499:\n",
            "train Loss: 4.2934 Acc: 0.7337\n",
            "\n",
            "val Loss: 5.7367 Acc: 0.6927\n",
            "\n",
            "Epoch 1290/1499:\n",
            "train Loss: 4.2894 Acc: 0.7339\n",
            "\n",
            "val Loss: 5.7324 Acc: 0.6921\n",
            "\n",
            "Epoch 1291/1499:\n",
            "train Loss: 4.2854 Acc: 0.7340\n",
            "\n",
            "val Loss: 5.7281 Acc: 0.6921\n",
            "\n",
            "Epoch 1292/1499:\n",
            "train Loss: 4.2814 Acc: 0.7341\n",
            "\n",
            "val Loss: 5.7238 Acc: 0.6921\n",
            "\n",
            "Epoch 1293/1499:\n",
            "train Loss: 4.2774 Acc: 0.7341\n",
            "\n",
            "val Loss: 5.7195 Acc: 0.6927\n",
            "\n",
            "Epoch 1294/1499:\n",
            "train Loss: 4.2735 Acc: 0.7342\n",
            "\n",
            "val Loss: 5.7153 Acc: 0.6933\n",
            "\n",
            "Epoch 1295/1499:\n",
            "train Loss: 4.2695 Acc: 0.7343\n",
            "\n",
            "val Loss: 5.7110 Acc: 0.6933\n",
            "\n",
            "Epoch 1296/1499:\n",
            "train Loss: 4.2655 Acc: 0.7343\n",
            "\n",
            "val Loss: 5.7067 Acc: 0.6933\n",
            "\n",
            "Epoch 1297/1499:\n",
            "train Loss: 4.2616 Acc: 0.7344\n",
            "\n",
            "val Loss: 5.7025 Acc: 0.6933\n",
            "\n",
            "Epoch 1298/1499:\n",
            "train Loss: 4.2576 Acc: 0.7345\n",
            "\n",
            "val Loss: 5.6982 Acc: 0.6933\n",
            "\n",
            "Epoch 1299/1499:\n",
            "train Loss: 4.2537 Acc: 0.7345\n",
            "\n",
            "val Loss: 5.6940 Acc: 0.6933\n",
            "\n",
            "Epoch 1300/1499:\n",
            "train Loss: 4.2497 Acc: 0.7346\n",
            "\n",
            "val Loss: 5.6898 Acc: 0.6927\n",
            "\n",
            "Epoch 1301/1499:\n",
            "train Loss: 4.2458 Acc: 0.7345\n",
            "\n",
            "val Loss: 5.6855 Acc: 0.6927\n",
            "\n",
            "Epoch 1302/1499:\n",
            "train Loss: 4.2419 Acc: 0.7345\n",
            "\n",
            "val Loss: 5.6813 Acc: 0.6927\n",
            "\n",
            "Epoch 1303/1499:\n",
            "train Loss: 4.2380 Acc: 0.7347\n",
            "\n",
            "val Loss: 5.6771 Acc: 0.6927\n",
            "\n",
            "Epoch 1304/1499:\n",
            "train Loss: 4.2340 Acc: 0.7350\n",
            "\n",
            "val Loss: 5.6729 Acc: 0.6927\n",
            "\n",
            "Epoch 1305/1499:\n",
            "train Loss: 4.2301 Acc: 0.7349\n",
            "\n",
            "val Loss: 5.6687 Acc: 0.6939\n",
            "\n",
            "Epoch 1306/1499:\n",
            "train Loss: 4.2262 Acc: 0.7349\n",
            "\n",
            "val Loss: 5.6645 Acc: 0.6939\n",
            "\n",
            "Epoch 1307/1499:\n",
            "train Loss: 4.2223 Acc: 0.7349\n",
            "\n",
            "val Loss: 5.6603 Acc: 0.6939\n",
            "\n",
            "Epoch 1308/1499:\n",
            "train Loss: 4.2184 Acc: 0.7349\n",
            "\n",
            "val Loss: 5.6561 Acc: 0.6939\n",
            "\n",
            "Epoch 1309/1499:\n",
            "train Loss: 4.2145 Acc: 0.7352\n",
            "\n",
            "val Loss: 5.6519 Acc: 0.6939\n",
            "\n",
            "Epoch 1310/1499:\n",
            "train Loss: 4.2106 Acc: 0.7354\n",
            "\n",
            "val Loss: 5.6477 Acc: 0.6939\n",
            "\n",
            "Epoch 1311/1499:\n",
            "train Loss: 4.2067 Acc: 0.7355\n",
            "\n",
            "val Loss: 5.6436 Acc: 0.6939\n",
            "\n",
            "Epoch 1312/1499:\n",
            "train Loss: 4.2029 Acc: 0.7356\n",
            "\n",
            "val Loss: 5.6394 Acc: 0.6945\n",
            "\n",
            "Epoch 1313/1499:\n",
            "train Loss: 4.1990 Acc: 0.7355\n",
            "\n",
            "val Loss: 5.6352 Acc: 0.6945\n",
            "\n",
            "Epoch 1314/1499:\n",
            "train Loss: 4.1951 Acc: 0.7355\n",
            "\n",
            "val Loss: 5.6311 Acc: 0.6945\n",
            "\n",
            "Epoch 1315/1499:\n",
            "train Loss: 4.1912 Acc: 0.7358\n",
            "\n",
            "val Loss: 5.6269 Acc: 0.6945\n",
            "\n",
            "Epoch 1316/1499:\n",
            "train Loss: 4.1874 Acc: 0.7358\n",
            "\n",
            "val Loss: 5.6228 Acc: 0.6945\n",
            "\n",
            "Epoch 1317/1499:\n",
            "train Loss: 4.1835 Acc: 0.7363\n",
            "\n",
            "val Loss: 5.6186 Acc: 0.6958\n",
            "\n",
            "Epoch 1318/1499:\n",
            "train Loss: 4.1797 Acc: 0.7363\n",
            "\n",
            "val Loss: 5.6145 Acc: 0.6958\n",
            "\n",
            "Epoch 1319/1499:\n",
            "train Loss: 4.1758 Acc: 0.7365\n",
            "\n",
            "val Loss: 5.6104 Acc: 0.6958\n",
            "\n",
            "Epoch 1320/1499:\n",
            "train Loss: 4.1720 Acc: 0.7365\n",
            "\n",
            "val Loss: 5.6062 Acc: 0.6958\n",
            "\n",
            "Epoch 1321/1499:\n",
            "train Loss: 4.1682 Acc: 0.7367\n",
            "\n",
            "val Loss: 5.6021 Acc: 0.6952\n",
            "\n",
            "Epoch 1322/1499:\n",
            "train Loss: 4.1643 Acc: 0.7371\n",
            "\n",
            "val Loss: 5.5980 Acc: 0.6958\n",
            "\n",
            "Epoch 1323/1499:\n",
            "train Loss: 4.1605 Acc: 0.7372\n",
            "\n",
            "val Loss: 5.5939 Acc: 0.6964\n",
            "\n",
            "Epoch 1324/1499:\n",
            "train Loss: 4.1567 Acc: 0.7371\n",
            "\n",
            "val Loss: 5.5898 Acc: 0.6958\n",
            "\n",
            "Epoch 1325/1499:\n",
            "train Loss: 4.1529 Acc: 0.7374\n",
            "\n",
            "val Loss: 5.5857 Acc: 0.6958\n",
            "\n",
            "Epoch 1326/1499:\n",
            "train Loss: 4.1491 Acc: 0.7375\n",
            "\n",
            "val Loss: 5.5816 Acc: 0.6964\n",
            "\n",
            "Epoch 1327/1499:\n",
            "train Loss: 4.1453 Acc: 0.7375\n",
            "\n",
            "val Loss: 5.5775 Acc: 0.6964\n",
            "\n",
            "Epoch 1328/1499:\n",
            "train Loss: 4.1415 Acc: 0.7377\n",
            "\n",
            "val Loss: 5.5734 Acc: 0.6964\n",
            "\n",
            "Epoch 1329/1499:\n",
            "train Loss: 4.1377 Acc: 0.7378\n",
            "\n",
            "val Loss: 5.5693 Acc: 0.6964\n",
            "\n",
            "Epoch 1330/1499:\n",
            "train Loss: 4.1339 Acc: 0.7377\n",
            "\n",
            "val Loss: 5.5652 Acc: 0.6964\n",
            "\n",
            "Epoch 1331/1499:\n",
            "train Loss: 4.1301 Acc: 0.7378\n",
            "\n",
            "val Loss: 5.5612 Acc: 0.6964\n",
            "\n",
            "Epoch 1332/1499:\n",
            "train Loss: 4.1263 Acc: 0.7380\n",
            "\n",
            "val Loss: 5.5571 Acc: 0.6964\n",
            "\n",
            "Epoch 1333/1499:\n",
            "train Loss: 4.1226 Acc: 0.7382\n",
            "\n",
            "val Loss: 5.5530 Acc: 0.6970\n",
            "\n",
            "Epoch 1334/1499:\n",
            "train Loss: 4.1188 Acc: 0.7384\n",
            "\n",
            "val Loss: 5.5490 Acc: 0.6970\n",
            "\n",
            "Epoch 1335/1499:\n",
            "train Loss: 4.1150 Acc: 0.7383\n",
            "\n",
            "val Loss: 5.5449 Acc: 0.6970\n",
            "\n",
            "Epoch 1336/1499:\n",
            "train Loss: 4.1113 Acc: 0.7385\n",
            "\n",
            "val Loss: 5.5408 Acc: 0.6970\n",
            "\n",
            "Epoch 1337/1499:\n",
            "train Loss: 4.1075 Acc: 0.7385\n",
            "\n",
            "val Loss: 5.5368 Acc: 0.6976\n",
            "\n",
            "Epoch 1338/1499:\n",
            "train Loss: 4.1038 Acc: 0.7387\n",
            "\n",
            "val Loss: 5.5327 Acc: 0.6982\n",
            "\n",
            "Epoch 1339/1499:\n",
            "train Loss: 4.1000 Acc: 0.7387\n",
            "\n",
            "val Loss: 5.5287 Acc: 0.6982\n",
            "\n",
            "Epoch 1340/1499:\n",
            "train Loss: 4.0963 Acc: 0.7387\n",
            "\n",
            "val Loss: 5.5247 Acc: 0.6982\n",
            "\n",
            "Epoch 1341/1499:\n",
            "train Loss: 4.0926 Acc: 0.7389\n",
            "\n",
            "val Loss: 5.5206 Acc: 0.6982\n",
            "\n",
            "Epoch 1342/1499:\n",
            "train Loss: 4.0888 Acc: 0.7390\n",
            "\n",
            "val Loss: 5.5166 Acc: 0.6982\n",
            "\n",
            "Epoch 1343/1499:\n",
            "train Loss: 4.0851 Acc: 0.7392\n",
            "\n",
            "val Loss: 5.5126 Acc: 0.6988\n",
            "\n",
            "Epoch 1344/1499:\n",
            "train Loss: 4.0814 Acc: 0.7393\n",
            "\n",
            "val Loss: 5.5086 Acc: 0.6988\n",
            "\n",
            "Epoch 1345/1499:\n",
            "train Loss: 4.0777 Acc: 0.7394\n",
            "\n",
            "val Loss: 5.5046 Acc: 0.6988\n",
            "\n",
            "Epoch 1346/1499:\n",
            "train Loss: 4.0740 Acc: 0.7395\n",
            "\n",
            "val Loss: 5.5005 Acc: 0.6988\n",
            "\n",
            "Epoch 1347/1499:\n",
            "train Loss: 4.0703 Acc: 0.7399\n",
            "\n",
            "val Loss: 5.4965 Acc: 0.6988\n",
            "\n",
            "Epoch 1348/1499:\n",
            "train Loss: 4.0666 Acc: 0.7400\n",
            "\n",
            "val Loss: 5.4925 Acc: 0.6988\n",
            "\n",
            "Epoch 1349/1499:\n",
            "train Loss: 4.0629 Acc: 0.7403\n",
            "\n",
            "val Loss: 5.4885 Acc: 0.6994\n",
            "\n",
            "Epoch 1350/1499:\n",
            "train Loss: 4.0592 Acc: 0.7405\n",
            "\n",
            "val Loss: 5.4845 Acc: 0.7000\n",
            "\n",
            "Epoch 1351/1499:\n",
            "train Loss: 4.0555 Acc: 0.7405\n",
            "\n",
            "val Loss: 5.4805 Acc: 0.7000\n",
            "\n",
            "Epoch 1352/1499:\n",
            "train Loss: 4.0518 Acc: 0.7408\n",
            "\n",
            "val Loss: 5.4765 Acc: 0.7000\n",
            "\n",
            "Epoch 1353/1499:\n",
            "train Loss: 4.0482 Acc: 0.7412\n",
            "\n",
            "val Loss: 5.4726 Acc: 0.7000\n",
            "\n",
            "Epoch 1354/1499:\n",
            "train Loss: 4.0445 Acc: 0.7414\n",
            "\n",
            "val Loss: 5.4686 Acc: 0.7000\n",
            "\n",
            "Epoch 1355/1499:\n",
            "train Loss: 4.0408 Acc: 0.7415\n",
            "\n",
            "val Loss: 5.4646 Acc: 0.7000\n",
            "\n",
            "Epoch 1356/1499:\n",
            "train Loss: 4.0372 Acc: 0.7416\n",
            "\n",
            "val Loss: 5.4606 Acc: 0.7000\n",
            "\n",
            "Epoch 1357/1499:\n",
            "train Loss: 4.0335 Acc: 0.7417\n",
            "\n",
            "val Loss: 5.4566 Acc: 0.7012\n",
            "\n",
            "Epoch 1358/1499:\n",
            "train Loss: 4.0299 Acc: 0.7417\n",
            "\n",
            "val Loss: 5.4527 Acc: 0.7012\n",
            "\n",
            "Epoch 1359/1499:\n",
            "train Loss: 4.0262 Acc: 0.7417\n",
            "\n",
            "val Loss: 5.4487 Acc: 0.7006\n",
            "\n",
            "Epoch 1360/1499:\n",
            "train Loss: 4.0226 Acc: 0.7421\n",
            "\n",
            "val Loss: 5.4448 Acc: 0.7006\n",
            "\n",
            "Epoch 1361/1499:\n",
            "train Loss: 4.0190 Acc: 0.7421\n",
            "\n",
            "val Loss: 5.4408 Acc: 0.7006\n",
            "\n",
            "Epoch 1362/1499:\n",
            "train Loss: 4.0153 Acc: 0.7422\n",
            "\n",
            "val Loss: 5.4368 Acc: 0.7012\n",
            "\n",
            "Epoch 1363/1499:\n",
            "train Loss: 4.0117 Acc: 0.7424\n",
            "\n",
            "val Loss: 5.4329 Acc: 0.7006\n",
            "\n",
            "Epoch 1364/1499:\n",
            "train Loss: 4.0081 Acc: 0.7424\n",
            "\n",
            "val Loss: 5.4289 Acc: 0.7006\n",
            "\n",
            "Epoch 1365/1499:\n",
            "train Loss: 4.0045 Acc: 0.7425\n",
            "\n",
            "val Loss: 5.4250 Acc: 0.7006\n",
            "\n",
            "Epoch 1366/1499:\n",
            "train Loss: 4.0008 Acc: 0.7425\n",
            "\n",
            "val Loss: 5.4211 Acc: 0.7006\n",
            "\n",
            "Epoch 1367/1499:\n",
            "train Loss: 3.9972 Acc: 0.7427\n",
            "\n",
            "val Loss: 5.4171 Acc: 0.7006\n",
            "\n",
            "Epoch 1368/1499:\n",
            "train Loss: 3.9936 Acc: 0.7428\n",
            "\n",
            "val Loss: 5.4132 Acc: 0.7006\n",
            "\n",
            "Epoch 1369/1499:\n",
            "train Loss: 3.9900 Acc: 0.7432\n",
            "\n",
            "val Loss: 5.4093 Acc: 0.7006\n",
            "\n",
            "Epoch 1370/1499:\n",
            "train Loss: 3.9864 Acc: 0.7434\n",
            "\n",
            "val Loss: 5.4053 Acc: 0.7006\n",
            "\n",
            "Epoch 1371/1499:\n",
            "train Loss: 3.9828 Acc: 0.7435\n",
            "\n",
            "val Loss: 5.4014 Acc: 0.7006\n",
            "\n",
            "Epoch 1372/1499:\n",
            "train Loss: 3.9793 Acc: 0.7435\n",
            "\n",
            "val Loss: 5.3975 Acc: 0.7006\n",
            "\n",
            "Epoch 1373/1499:\n",
            "train Loss: 3.9757 Acc: 0.7435\n",
            "\n",
            "val Loss: 5.3936 Acc: 0.7012\n",
            "\n",
            "Epoch 1374/1499:\n",
            "train Loss: 3.9721 Acc: 0.7437\n",
            "\n",
            "val Loss: 5.3896 Acc: 0.7006\n",
            "\n",
            "Epoch 1375/1499:\n",
            "train Loss: 3.9685 Acc: 0.7439\n",
            "\n",
            "val Loss: 5.3857 Acc: 0.7006\n",
            "\n",
            "Epoch 1376/1499:\n",
            "train Loss: 3.9650 Acc: 0.7441\n",
            "\n",
            "val Loss: 5.3818 Acc: 0.7006\n",
            "\n",
            "Epoch 1377/1499:\n",
            "train Loss: 3.9614 Acc: 0.7445\n",
            "\n",
            "val Loss: 5.3779 Acc: 0.7006\n",
            "\n",
            "Epoch 1378/1499:\n",
            "train Loss: 3.9579 Acc: 0.7445\n",
            "\n",
            "val Loss: 5.3740 Acc: 0.7006\n",
            "\n",
            "Epoch 1379/1499:\n",
            "train Loss: 3.9543 Acc: 0.7445\n",
            "\n",
            "val Loss: 5.3701 Acc: 0.7006\n",
            "\n",
            "Epoch 1380/1499:\n",
            "train Loss: 3.9507 Acc: 0.7447\n",
            "\n",
            "val Loss: 5.3662 Acc: 0.7006\n",
            "\n",
            "Epoch 1381/1499:\n",
            "train Loss: 3.9472 Acc: 0.7449\n",
            "\n",
            "val Loss: 5.3623 Acc: 0.7006\n",
            "\n",
            "Epoch 1382/1499:\n",
            "train Loss: 3.9437 Acc: 0.7452\n",
            "\n",
            "val Loss: 5.3585 Acc: 0.7006\n",
            "\n",
            "Epoch 1383/1499:\n",
            "train Loss: 3.9401 Acc: 0.7454\n",
            "\n",
            "val Loss: 5.3546 Acc: 0.7006\n",
            "\n",
            "Epoch 1384/1499:\n",
            "train Loss: 3.9366 Acc: 0.7453\n",
            "\n",
            "val Loss: 5.3507 Acc: 0.7006\n",
            "\n",
            "Epoch 1385/1499:\n",
            "train Loss: 3.9331 Acc: 0.7454\n",
            "\n",
            "val Loss: 5.3468 Acc: 0.7006\n",
            "\n",
            "Epoch 1386/1499:\n",
            "train Loss: 3.9295 Acc: 0.7455\n",
            "\n",
            "val Loss: 5.3429 Acc: 0.7006\n",
            "\n",
            "Epoch 1387/1499:\n",
            "train Loss: 3.9260 Acc: 0.7455\n",
            "\n",
            "val Loss: 5.3391 Acc: 0.7006\n",
            "\n",
            "Epoch 1388/1499:\n",
            "train Loss: 3.9225 Acc: 0.7457\n",
            "\n",
            "val Loss: 5.3352 Acc: 0.7006\n",
            "\n",
            "Epoch 1389/1499:\n",
            "train Loss: 3.9190 Acc: 0.7458\n",
            "\n",
            "val Loss: 5.3313 Acc: 0.7006\n",
            "\n",
            "Epoch 1390/1499:\n",
            "train Loss: 3.9155 Acc: 0.7457\n",
            "\n",
            "val Loss: 5.3275 Acc: 0.7012\n",
            "\n",
            "Epoch 1391/1499:\n",
            "train Loss: 3.9120 Acc: 0.7456\n",
            "\n",
            "val Loss: 5.3236 Acc: 0.7012\n",
            "\n",
            "Epoch 1392/1499:\n",
            "train Loss: 3.9085 Acc: 0.7455\n",
            "\n",
            "val Loss: 5.3198 Acc: 0.7012\n",
            "\n",
            "Epoch 1393/1499:\n",
            "train Loss: 3.9050 Acc: 0.7456\n",
            "\n",
            "val Loss: 5.3159 Acc: 0.7012\n",
            "\n",
            "Epoch 1394/1499:\n",
            "train Loss: 3.9015 Acc: 0.7458\n",
            "\n",
            "val Loss: 5.3121 Acc: 0.7012\n",
            "\n",
            "Epoch 1395/1499:\n",
            "train Loss: 3.8980 Acc: 0.7458\n",
            "\n",
            "val Loss: 5.3082 Acc: 0.7012\n",
            "\n",
            "Epoch 1396/1499:\n",
            "train Loss: 3.8946 Acc: 0.7458\n",
            "\n",
            "val Loss: 5.3044 Acc: 0.7012\n",
            "\n",
            "Epoch 1397/1499:\n",
            "train Loss: 3.8911 Acc: 0.7460\n",
            "\n",
            "val Loss: 5.3006 Acc: 0.7012\n",
            "\n",
            "Epoch 1398/1499:\n",
            "train Loss: 3.8876 Acc: 0.7461\n",
            "\n",
            "val Loss: 5.2967 Acc: 0.7018\n",
            "\n",
            "Epoch 1399/1499:\n",
            "train Loss: 3.8842 Acc: 0.7462\n",
            "\n",
            "val Loss: 5.2929 Acc: 0.7018\n",
            "\n",
            "Epoch 1400/1499:\n",
            "train Loss: 3.8807 Acc: 0.7463\n",
            "\n",
            "val Loss: 5.2891 Acc: 0.7024\n",
            "\n",
            "Epoch 1401/1499:\n",
            "train Loss: 3.8772 Acc: 0.7463\n",
            "\n",
            "val Loss: 5.2853 Acc: 0.7030\n",
            "\n",
            "Epoch 1402/1499:\n",
            "train Loss: 3.8738 Acc: 0.7465\n",
            "\n",
            "val Loss: 5.2814 Acc: 0.7030\n",
            "\n",
            "Epoch 1403/1499:\n",
            "train Loss: 3.8703 Acc: 0.7465\n",
            "\n",
            "val Loss: 5.2776 Acc: 0.7030\n",
            "\n",
            "Epoch 1404/1499:\n",
            "train Loss: 3.8669 Acc: 0.7467\n",
            "\n",
            "val Loss: 5.2738 Acc: 0.7030\n",
            "\n",
            "Epoch 1405/1499:\n",
            "train Loss: 3.8635 Acc: 0.7468\n",
            "\n",
            "val Loss: 5.2700 Acc: 0.7030\n",
            "\n",
            "Epoch 1406/1499:\n",
            "train Loss: 3.8600 Acc: 0.7469\n",
            "\n",
            "val Loss: 5.2662 Acc: 0.7030\n",
            "\n",
            "Epoch 1407/1499:\n",
            "train Loss: 3.8566 Acc: 0.7469\n",
            "\n",
            "val Loss: 5.2624 Acc: 0.7030\n",
            "\n",
            "Epoch 1408/1499:\n",
            "train Loss: 3.8532 Acc: 0.7468\n",
            "\n",
            "val Loss: 5.2586 Acc: 0.7030\n",
            "\n",
            "Epoch 1409/1499:\n",
            "train Loss: 3.8497 Acc: 0.7468\n",
            "\n",
            "val Loss: 5.2548 Acc: 0.7024\n",
            "\n",
            "Epoch 1410/1499:\n",
            "train Loss: 3.8463 Acc: 0.7473\n",
            "\n",
            "val Loss: 5.2510 Acc: 0.7030\n",
            "\n",
            "Epoch 1411/1499:\n",
            "train Loss: 3.8429 Acc: 0.7475\n",
            "\n",
            "val Loss: 5.2472 Acc: 0.7030\n",
            "\n",
            "Epoch 1412/1499:\n",
            "train Loss: 3.8395 Acc: 0.7475\n",
            "\n",
            "val Loss: 5.2435 Acc: 0.7036\n",
            "\n",
            "Epoch 1413/1499:\n",
            "train Loss: 3.8361 Acc: 0.7476\n",
            "\n",
            "val Loss: 5.2397 Acc: 0.7036\n",
            "\n",
            "Epoch 1414/1499:\n",
            "train Loss: 3.8327 Acc: 0.7475\n",
            "\n",
            "val Loss: 5.2359 Acc: 0.7036\n",
            "\n",
            "Epoch 1415/1499:\n",
            "train Loss: 3.8293 Acc: 0.7474\n",
            "\n",
            "val Loss: 5.2321 Acc: 0.7036\n",
            "\n",
            "Epoch 1416/1499:\n",
            "train Loss: 3.8259 Acc: 0.7473\n",
            "\n",
            "val Loss: 5.2284 Acc: 0.7036\n",
            "\n",
            "Epoch 1417/1499:\n",
            "train Loss: 3.8225 Acc: 0.7475\n",
            "\n",
            "val Loss: 5.2246 Acc: 0.7036\n",
            "\n",
            "Epoch 1418/1499:\n",
            "train Loss: 3.8191 Acc: 0.7476\n",
            "\n",
            "val Loss: 5.2209 Acc: 0.7036\n",
            "\n",
            "Epoch 1419/1499:\n",
            "train Loss: 3.8158 Acc: 0.7476\n",
            "\n",
            "val Loss: 5.2171 Acc: 0.7036\n",
            "\n",
            "Epoch 1420/1499:\n",
            "train Loss: 3.8124 Acc: 0.7478\n",
            "\n",
            "val Loss: 5.2133 Acc: 0.7042\n",
            "\n",
            "Epoch 1421/1499:\n",
            "train Loss: 3.8090 Acc: 0.7479\n",
            "\n",
            "val Loss: 5.2096 Acc: 0.7042\n",
            "\n",
            "Epoch 1422/1499:\n",
            "train Loss: 3.8056 Acc: 0.7479\n",
            "\n",
            "val Loss: 5.2058 Acc: 0.7042\n",
            "\n",
            "Epoch 1423/1499:\n",
            "train Loss: 3.8023 Acc: 0.7485\n",
            "\n",
            "val Loss: 5.2021 Acc: 0.7042\n",
            "\n",
            "Epoch 1424/1499:\n",
            "train Loss: 3.7989 Acc: 0.7485\n",
            "\n",
            "val Loss: 5.1984 Acc: 0.7042\n",
            "\n",
            "Epoch 1425/1499:\n",
            "train Loss: 3.7956 Acc: 0.7485\n",
            "\n",
            "val Loss: 5.1946 Acc: 0.7042\n",
            "\n",
            "Epoch 1426/1499:\n",
            "train Loss: 3.7922 Acc: 0.7484\n",
            "\n",
            "val Loss: 5.1909 Acc: 0.7042\n",
            "\n",
            "Epoch 1427/1499:\n",
            "train Loss: 3.7889 Acc: 0.7485\n",
            "\n",
            "val Loss: 5.1872 Acc: 0.7042\n",
            "\n",
            "Epoch 1428/1499:\n",
            "train Loss: 3.7855 Acc: 0.7485\n",
            "\n",
            "val Loss: 5.1835 Acc: 0.7042\n",
            "\n",
            "Epoch 1429/1499:\n",
            "train Loss: 3.7822 Acc: 0.7487\n",
            "\n",
            "val Loss: 5.1797 Acc: 0.7042\n",
            "\n",
            "Epoch 1430/1499:\n",
            "train Loss: 3.7789 Acc: 0.7491\n",
            "\n",
            "val Loss: 5.1760 Acc: 0.7042\n",
            "\n",
            "Epoch 1431/1499:\n",
            "train Loss: 3.7755 Acc: 0.7493\n",
            "\n",
            "val Loss: 5.1723 Acc: 0.7042\n",
            "\n",
            "Epoch 1432/1499:\n",
            "train Loss: 3.7722 Acc: 0.7495\n",
            "\n",
            "val Loss: 5.1686 Acc: 0.7036\n",
            "\n",
            "Epoch 1433/1499:\n",
            "train Loss: 3.7689 Acc: 0.7497\n",
            "\n",
            "val Loss: 5.1649 Acc: 0.7042\n",
            "\n",
            "Epoch 1434/1499:\n",
            "train Loss: 3.7656 Acc: 0.7499\n",
            "\n",
            "val Loss: 5.1612 Acc: 0.7048\n",
            "\n",
            "Epoch 1435/1499:\n",
            "train Loss: 3.7623 Acc: 0.7499\n",
            "\n",
            "val Loss: 5.1575 Acc: 0.7048\n",
            "\n",
            "Epoch 1436/1499:\n",
            "train Loss: 3.7590 Acc: 0.7499\n",
            "\n",
            "val Loss: 5.1538 Acc: 0.7048\n",
            "\n",
            "Epoch 1437/1499:\n",
            "train Loss: 3.7557 Acc: 0.7500\n",
            "\n",
            "val Loss: 5.1501 Acc: 0.7048\n",
            "\n",
            "Epoch 1438/1499:\n",
            "train Loss: 3.7524 Acc: 0.7504\n",
            "\n",
            "val Loss: 5.1464 Acc: 0.7048\n",
            "\n",
            "Epoch 1439/1499:\n",
            "train Loss: 3.7491 Acc: 0.7505\n",
            "\n",
            "val Loss: 5.1427 Acc: 0.7055\n",
            "\n",
            "Epoch 1440/1499:\n",
            "train Loss: 3.7458 Acc: 0.7505\n",
            "\n",
            "val Loss: 5.1391 Acc: 0.7061\n",
            "\n",
            "Epoch 1441/1499:\n",
            "train Loss: 3.7425 Acc: 0.7506\n",
            "\n",
            "val Loss: 5.1354 Acc: 0.7061\n",
            "\n",
            "Epoch 1442/1499:\n",
            "train Loss: 3.7392 Acc: 0.7506\n",
            "\n",
            "val Loss: 5.1317 Acc: 0.7061\n",
            "\n",
            "Epoch 1443/1499:\n",
            "train Loss: 3.7359 Acc: 0.7508\n",
            "\n",
            "val Loss: 5.1281 Acc: 0.7067\n",
            "\n",
            "Epoch 1444/1499:\n",
            "train Loss: 3.7327 Acc: 0.7508\n",
            "\n",
            "val Loss: 5.1244 Acc: 0.7067\n",
            "\n",
            "Epoch 1445/1499:\n",
            "train Loss: 3.7294 Acc: 0.7508\n",
            "\n",
            "val Loss: 5.1207 Acc: 0.7067\n",
            "\n",
            "Epoch 1446/1499:\n",
            "train Loss: 3.7261 Acc: 0.7507\n",
            "\n",
            "val Loss: 5.1171 Acc: 0.7067\n",
            "\n",
            "Epoch 1447/1499:\n",
            "train Loss: 3.7229 Acc: 0.7508\n",
            "\n",
            "val Loss: 5.1134 Acc: 0.7067\n",
            "\n",
            "Epoch 1448/1499:\n",
            "train Loss: 3.7196 Acc: 0.7507\n",
            "\n",
            "val Loss: 5.1098 Acc: 0.7073\n",
            "\n",
            "Epoch 1449/1499:\n",
            "train Loss: 3.7164 Acc: 0.7506\n",
            "\n",
            "val Loss: 5.1061 Acc: 0.7073\n",
            "\n",
            "Epoch 1450/1499:\n",
            "train Loss: 3.7131 Acc: 0.7507\n",
            "\n",
            "val Loss: 5.1025 Acc: 0.7073\n",
            "\n",
            "Epoch 1451/1499:\n",
            "train Loss: 3.7099 Acc: 0.7507\n",
            "\n",
            "val Loss: 5.0989 Acc: 0.7073\n",
            "\n",
            "Epoch 1452/1499:\n",
            "train Loss: 3.7066 Acc: 0.7511\n",
            "\n",
            "val Loss: 5.0952 Acc: 0.7073\n",
            "\n",
            "Epoch 1453/1499:\n",
            "train Loss: 3.7034 Acc: 0.7512\n",
            "\n",
            "val Loss: 5.0916 Acc: 0.7073\n",
            "\n",
            "Epoch 1454/1499:\n",
            "train Loss: 3.7002 Acc: 0.7512\n",
            "\n",
            "val Loss: 5.0880 Acc: 0.7073\n",
            "\n",
            "Epoch 1455/1499:\n",
            "train Loss: 3.6969 Acc: 0.7512\n",
            "\n",
            "val Loss: 5.0844 Acc: 0.7073\n",
            "\n",
            "Epoch 1456/1499:\n",
            "train Loss: 3.6937 Acc: 0.7511\n",
            "\n",
            "val Loss: 5.0807 Acc: 0.7073\n",
            "\n",
            "Epoch 1457/1499:\n",
            "train Loss: 3.6905 Acc: 0.7512\n",
            "\n",
            "val Loss: 5.0771 Acc: 0.7073\n",
            "\n",
            "Epoch 1458/1499:\n",
            "train Loss: 3.6873 Acc: 0.7513\n",
            "\n",
            "val Loss: 5.0735 Acc: 0.7073\n",
            "\n",
            "Epoch 1459/1499:\n",
            "train Loss: 3.6841 Acc: 0.7511\n",
            "\n",
            "val Loss: 5.0699 Acc: 0.7073\n",
            "\n",
            "Epoch 1460/1499:\n",
            "train Loss: 3.6809 Acc: 0.7512\n",
            "\n",
            "val Loss: 5.0663 Acc: 0.7085\n",
            "\n",
            "Epoch 1461/1499:\n",
            "train Loss: 3.6777 Acc: 0.7514\n",
            "\n",
            "val Loss: 5.0627 Acc: 0.7085\n",
            "\n",
            "Epoch 1462/1499:\n",
            "train Loss: 3.6745 Acc: 0.7515\n",
            "\n",
            "val Loss: 5.0591 Acc: 0.7085\n",
            "\n",
            "Epoch 1463/1499:\n",
            "train Loss: 3.6713 Acc: 0.7515\n",
            "\n",
            "val Loss: 5.0555 Acc: 0.7085\n",
            "\n",
            "Epoch 1464/1499:\n",
            "train Loss: 3.6681 Acc: 0.7517\n",
            "\n",
            "val Loss: 5.0520 Acc: 0.7085\n",
            "\n",
            "Epoch 1465/1499:\n",
            "train Loss: 3.6649 Acc: 0.7518\n",
            "\n",
            "val Loss: 5.0484 Acc: 0.7085\n",
            "\n",
            "Epoch 1466/1499:\n",
            "train Loss: 3.6617 Acc: 0.7519\n",
            "\n",
            "val Loss: 5.0448 Acc: 0.7085\n",
            "\n",
            "Epoch 1467/1499:\n",
            "train Loss: 3.6585 Acc: 0.7521\n",
            "\n",
            "val Loss: 5.0412 Acc: 0.7085\n",
            "\n",
            "Epoch 1468/1499:\n",
            "train Loss: 3.6554 Acc: 0.7523\n",
            "\n",
            "val Loss: 5.0376 Acc: 0.7079\n",
            "\n",
            "Epoch 1469/1499:\n",
            "train Loss: 3.6522 Acc: 0.7525\n",
            "\n",
            "val Loss: 5.0341 Acc: 0.7079\n",
            "\n",
            "Epoch 1470/1499:\n",
            "train Loss: 3.6490 Acc: 0.7526\n",
            "\n",
            "val Loss: 5.0305 Acc: 0.7079\n",
            "\n",
            "Epoch 1471/1499:\n",
            "train Loss: 3.6459 Acc: 0.7526\n",
            "\n",
            "val Loss: 5.0270 Acc: 0.7085\n",
            "\n",
            "Epoch 1472/1499:\n",
            "train Loss: 3.6427 Acc: 0.7527\n",
            "\n",
            "val Loss: 5.0234 Acc: 0.7085\n",
            "\n",
            "Epoch 1473/1499:\n",
            "train Loss: 3.6396 Acc: 0.7527\n",
            "\n",
            "val Loss: 5.0199 Acc: 0.7085\n",
            "\n",
            "Epoch 1474/1499:\n",
            "train Loss: 3.6364 Acc: 0.7526\n",
            "\n",
            "val Loss: 5.0163 Acc: 0.7085\n",
            "\n",
            "Epoch 1475/1499:\n",
            "train Loss: 3.6333 Acc: 0.7527\n",
            "\n",
            "val Loss: 5.0128 Acc: 0.7085\n",
            "\n",
            "Epoch 1476/1499:\n",
            "train Loss: 3.6301 Acc: 0.7527\n",
            "\n",
            "val Loss: 5.0092 Acc: 0.7085\n",
            "\n",
            "Epoch 1477/1499:\n",
            "train Loss: 3.6270 Acc: 0.7528\n",
            "\n",
            "val Loss: 5.0057 Acc: 0.7085\n",
            "\n",
            "Epoch 1478/1499:\n",
            "train Loss: 3.6238 Acc: 0.7528\n",
            "\n",
            "val Loss: 5.0022 Acc: 0.7085\n",
            "\n",
            "Epoch 1479/1499:\n",
            "train Loss: 3.6207 Acc: 0.7532\n",
            "\n",
            "val Loss: 4.9986 Acc: 0.7085\n",
            "\n",
            "Epoch 1480/1499:\n",
            "train Loss: 3.6176 Acc: 0.7535\n",
            "\n",
            "val Loss: 4.9951 Acc: 0.7085\n",
            "\n",
            "Epoch 1481/1499:\n",
            "train Loss: 3.6145 Acc: 0.7535\n",
            "\n",
            "val Loss: 4.9916 Acc: 0.7085\n",
            "\n",
            "Epoch 1482/1499:\n",
            "train Loss: 3.6113 Acc: 0.7535\n",
            "\n",
            "val Loss: 4.9881 Acc: 0.7085\n",
            "\n",
            "Epoch 1483/1499:\n",
            "train Loss: 3.6082 Acc: 0.7535\n",
            "\n",
            "val Loss: 4.9846 Acc: 0.7085\n",
            "\n",
            "Epoch 1484/1499:\n",
            "train Loss: 3.6051 Acc: 0.7536\n",
            "\n",
            "val Loss: 4.9811 Acc: 0.7085\n",
            "\n",
            "Epoch 1485/1499:\n",
            "train Loss: 3.6020 Acc: 0.7535\n",
            "\n",
            "val Loss: 4.9776 Acc: 0.7085\n",
            "\n",
            "Epoch 1486/1499:\n",
            "train Loss: 3.5989 Acc: 0.7535\n",
            "\n",
            "val Loss: 4.9741 Acc: 0.7085\n",
            "\n",
            "Epoch 1487/1499:\n",
            "train Loss: 3.5958 Acc: 0.7535\n",
            "\n",
            "val Loss: 4.9706 Acc: 0.7085\n",
            "\n",
            "Epoch 1488/1499:\n",
            "train Loss: 3.5927 Acc: 0.7540\n",
            "\n",
            "val Loss: 4.9671 Acc: 0.7085\n",
            "\n",
            "Epoch 1489/1499:\n",
            "train Loss: 3.5896 Acc: 0.7542\n",
            "\n",
            "val Loss: 4.9636 Acc: 0.7085\n",
            "\n",
            "Epoch 1490/1499:\n",
            "train Loss: 3.5865 Acc: 0.7542\n",
            "\n",
            "val Loss: 4.9601 Acc: 0.7085\n",
            "\n",
            "Epoch 1491/1499:\n",
            "train Loss: 3.5834 Acc: 0.7544\n",
            "\n",
            "val Loss: 4.9566 Acc: 0.7079\n",
            "\n",
            "Epoch 1492/1499:\n",
            "train Loss: 3.5804 Acc: 0.7545\n",
            "\n",
            "val Loss: 4.9531 Acc: 0.7079\n",
            "\n",
            "Epoch 1493/1499:\n",
            "train Loss: 3.5773 Acc: 0.7546\n",
            "\n",
            "val Loss: 4.9497 Acc: 0.7079\n",
            "\n",
            "Epoch 1494/1499:\n",
            "train Loss: 3.5742 Acc: 0.7546\n",
            "\n",
            "val Loss: 4.9462 Acc: 0.7079\n",
            "\n",
            "Epoch 1495/1499:\n",
            "train Loss: 3.5711 Acc: 0.7547\n",
            "\n",
            "val Loss: 4.9427 Acc: 0.7079\n",
            "\n",
            "Epoch 1496/1499:\n",
            "train Loss: 3.5681 Acc: 0.7547\n",
            "\n",
            "val Loss: 4.9393 Acc: 0.7079\n",
            "\n",
            "Epoch 1497/1499:\n",
            "train Loss: 3.5650 Acc: 0.7548\n",
            "\n",
            "val Loss: 4.9358 Acc: 0.7085\n",
            "\n",
            "Epoch 1498/1499:\n",
            "train Loss: 3.5619 Acc: 0.7549\n",
            "\n",
            "val Loss: 4.9324 Acc: 0.7085\n",
            "\n",
            "Epoch 1499/1499:\n",
            "train Loss: 3.5589 Acc: 0.7552\n",
            "\n",
            "val Loss: 4.9289 Acc: 0.7085\n",
            "\n",
            "CPU times: user 21.9 s, sys: 4.26 s, total: 26.1 s\n",
            "Wall time: 23.8 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OK38TywePKUU",
        "colab_type": "code",
        "outputId": "466c7f0c-8a1b-489d-c1ad-606039b862b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "# Loss\n",
        "plt.plot(range(num_epochs), train_loss, val_loss)\n",
        "plt.legend(['train', 'val'])\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('cross entropy loss')\n",
        "plt.title('loss')"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'loss')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oE2UJmSmPKUX",
        "colab_type": "code",
        "outputId": "d0bb1e8d-243b-4c69-e6be-e06c810dc93f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "# accuracy\n",
        "plt.plot(range(num_epochs), train_acc, val_acc)\n",
        "plt.legend(['train', 'val'])\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.title('accuracy')"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'accuracy')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEqB1t96GI-O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f47q_lMXGJEv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}