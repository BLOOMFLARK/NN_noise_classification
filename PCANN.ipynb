{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "PCANN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8az9TUBPNn3",
        "colab_type": "code",
        "outputId": "10341a1c-8408-4d18-95b7-1c507f3c25a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZOtN3-sW09u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# написать функцию fit в класс PCANet, который бы обучал модель"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZOKuZLzPKT1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn.functional as F\n",
        "from sklearn.decomposition import PCA, IncrementalPCA\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torchvision import transforms, models, datasets\n",
        "\n",
        "import random\n",
        "\n",
        "\n",
        "class PCANet(torch.nn.Module):\n",
        "    def __init__(self, num_filters: list, filters_sizes: list, batch_size=256):\n",
        "        super(PCANet, self).__init__()\n",
        "        self.params = {\n",
        "            'num_filters': num_filters,\n",
        "            'filters_sizes': filters_sizes,\n",
        "        }\n",
        "        self.W_1 = None\n",
        "        self.pool1 = torch.nn.MaxPool2d(kernel_size=2, stride=1)\n",
        "        self.act1 = torch.nn.ReLU()\n",
        "        self.W_2 = None\n",
        "        # self.pool2 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc = torch.nn.Linear(28090, 2, bias=True)\n",
        "        #self.fc = torch.nn.Linear(30250, 2, bias=True)\n",
        "        # self.fc2 = torch.nn.Linear(100, 2, bias=True)\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.conv2d(x, self.W_1)\n",
        "        x = self.act1(x)\n",
        "        x = self.pool1(x)\n",
        "        N1, C1, H1, W1 = x.shape\n",
        "\n",
        "        x = F.conv2d(x, self.W_2)\n",
        "        x = self.act1(x)\n",
        "        x = self.pool1(x)\n",
        "        N, C, H, W = x.shape\n",
        "\n",
        "        x_flat = x.view(N, C * H * W)\n",
        "\n",
        "        x_flat = self.fc(x_flat)\n",
        "        # x_flat = self.act1(x_flat)\n",
        "        # x_flat = self.fc2(x_flat)\n",
        "        return x_flat\n",
        "            \n",
        "    @staticmethod        \n",
        "    def _extract_image_patches(imgs: torch.Tensor, filter_size, stride=1, remove_mean=True):\n",
        "        # imgs.shape = (N, C, H, W) -> (N, 1, H, W) \n",
        "        # так должно быть, но сюда могут прийти не grayscale изображения первого шага, а со второго\n",
        "        # на котором применено L1 фильтров -> L1 каналов\n",
        "        N, n_channels, H, W = imgs.shape\n",
        "        \n",
        "        if n_channels > 1:\n",
        "            # изображение вида (N, C, H, W) - N C-канальных изображений\n",
        "            # приводим к виду (N*C, 1, H, W) - N*C одно-канальных изображений\n",
        "            imgs = imgs.view(-1, 1, H, W)\n",
        "        print('images shape', imgs.shape)\n",
        "            \n",
        "        k = filter_size\n",
        "        patches = torch.nn.functional.unfold(imgs, k, padding=k//2) # (N, k^2, H*W)\n",
        "        print('patches_shape, ', patches.shape)\n",
        "        print('should be patches shape, ', (imgs.shape[0], k**2, H*W))\n",
        "        \n",
        "        if remove_mean:\n",
        "            patches -= patches.mean(dim=1, keepdim=True) # последнее измерение - количество патчей\n",
        "        \n",
        "        print('filter_size', k)\n",
        "        X = patches.view(k**2, -1) # (k^2, N*H*W)\n",
        "\n",
        "        return X\n",
        "    \n",
        "    def _convolve(self, imgs: torch.Tensor, filter_bank: torch.Tensor) -> torch.Tensor:\n",
        "        weight = filter_bank\n",
        "        output = F.conv2d(imgs, weight) #, padding=padding)\n",
        "        return output\n",
        "    \n",
        "    def _first_stage(self, imgs: torch.Tensor, train: bool) -> torch.Tensor:\n",
        "        # (N, C, H, W) image\n",
        "        # (train_size, 1, H, W) - grayscale\n",
        "        assert imgs.dim() == 4 and imgs.nelement() > 0\n",
        "\n",
        "        print('PCANet first stage...')\n",
        "\n",
        "        if train:\n",
        "            # достаем все патчи из всех N изображений\n",
        "            filter_size1 = self.params['filters_sizes'][0]\n",
        "            X = self._extract_image_patches(\n",
        "                imgs, filter_size1)\n",
        "            \n",
        "            n_filters = self.params['num_filters'][0]\n",
        "            \n",
        "            eigenvectors = self.get_pca_eigenvectors(X, n_components=n_filters, batch_size=self.batch_size)\n",
        "            self.W_1 = torch.FloatTensor(eigenvectors).view(n_filters, 1, filter_size1, filter_size1)\n",
        "         \n",
        "        I = self._convolve(imgs, self.W_1)  # (N, 1, H, W) * (L1, k1, k1) -> (N, L1, H', W')\n",
        "        return I\n",
        "    \n",
        "    @staticmethod\n",
        "    def conv_output_size(w, filter_size, padding=0, stride=1):\n",
        "        return int((w - filter_size + 2 * padding) / stride + 1)\n",
        "    \n",
        "    @staticmethod\n",
        "    def get_pca_eigenvectors(X, n_components, batch_size=100):\n",
        "        ipca = IncrementalPCA(n_components=n_components, batch_size=batch_size)\n",
        "        print('pca fitting ...')\n",
        "        ipca.fit(X @ X.t())\n",
        "        eigenvectors = ipca.components_\n",
        "        print('eigenvectors shape:', eigenvectors.shape)\n",
        "        return eigenvectors\n",
        "        \n",
        "    def _second_stage(self, I: torch.Tensor, train):\n",
        "        print('PCANet second stage...')\n",
        "        # I: (N, L1, H, W)\n",
        "        if train:\n",
        "            N, L1, H, W = I.shape\n",
        "            filter_size2 = self.params['filters_sizes'][1]\n",
        "            n_filters2 = self.params['num_filters'][1]\n",
        "            n_filters1 = self.params['num_filters'][0]\n",
        "            \n",
        "            H_new = self.conv_output_size(I.shape[2], filter_size2)\n",
        "            W_new = self.conv_output_size(I.shape[3], filter_size2)\n",
        "            \n",
        "            X = self._extract_image_patches(I, filter_size2)\n",
        "            print('X_SHAPE ', X.shape)\n",
        "            eigenvectors = self.get_pca_eigenvectors(X, n_components=n_filters2, batch_size=self.batch_size)\n",
        "            W_2 = torch.FloatTensor(eigenvectors).view(n_filters2, 1, filter_size2, filter_size2) # (L2, 1, k2, k2)\n",
        "            self.W_2 = W_2.repeat(1, n_filters1, 1, 1) # (L2, L1, k2, k2) - повторяет L1 раз для конкретного l из L2\n",
        "        return self._convolve(I, self.W_2)\n",
        "    \n",
        "    def run(self, images):\n",
        "        # Создаем фильтры\n",
        "        # images: (N, 1, H, W)\n",
        "        I = self._first_stage(images, train=True)\n",
        "        print(\"I \", I.shape)\n",
        "        II = self._second_stage(I, train=True)\n",
        "        N, C, H, W = II.shape\n",
        "        # self.fc = torch.nn.Linear(H * W, 2, bias=True)\n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ag7yTvPNPKT7",
        "colab_type": "code",
        "outputId": "5c5588ed-c2fd-4726-8a14-546fcf022df5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "net = PCANet([5, 10], [3, 8])\n",
        "net.params\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "net = net.to(device)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9cpvOhiPbwl",
        "colab_type": "code",
        "outputId": "f2a31066-2218-4ee1-a5f9-e4c0b784d135",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data_root = 'drive/My Drive/dl_noise_classification/data/'\n",
        "train_dir = 'train'\n",
        "val_dir = 'val'\n",
        "test_dir = 'test'\n",
        "class_names = ['awgn', 'bayer']\n",
        "\n",
        "train_dir = os.path.join(data_root, train_dir)\n",
        "val_dir = os.path.join(data_root, val_dir)\n",
        "test_dir = os.path.join(data_root, test_dir)\n",
        "\n",
        "train_dir"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'drive/My Drive/dl_noise_classification/data/train'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5GHuftbPKUC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "num_epochs = 500\n",
        "batch_size = 150\n",
        "lr = 1e-4\n",
        "\n",
        "loss = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "#optimizer = torch.optim.SGD(net.parameters(), lr=lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RdNqCUxPKUG",
        "colab_type": "code",
        "outputId": "82f32cc6-3791-48dc-c402-2356cfcb520a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# МНОГО ПРОЦЕССОВ ЖРУТ ПАМЯТЬ ОЧЕНЬ МНОГО, при этом сильно не ускоряют\n",
        "train_transforms = transforms.Compose([\n",
        "    #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "val_transforms = transforms.Compose([\n",
        "    #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "\n",
        "train_dataset = datasets.ImageFolder(train_dir, train_transforms)\n",
        "val_dataset = datasets.ImageFolder(val_dir, val_transforms)\n",
        "test_dataset = datasets.ImageFolder(test_dir, train_transforms)\n",
        "\n",
        "train_size = len(train_dataset)\n",
        "val_size = len(val_dataset)\n",
        "test_size = len(test_dataset)\n",
        "\n",
        "all_train = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=train_size, shuffle=False, num_workers=batch_size)\n",
        "\n",
        "#train_dataloader = torch.utils.data.DataLoader(\n",
        "#    train_dataset, batch_size=batch_size, shuffle=True, num_workers=batch_size)\n",
        "\n",
        "#val_dataloader = torch.utils.data.DataLoader(\n",
        "#    val_dataset, batch_size=batch_size, shuffle=False, num_workers=batch_size)\n",
        "\n",
        "all_val = torch.utils.data.DataLoader(\n",
        "    val_dataset, batch_size=train_size, shuffle=False, num_workers=val_size)\n",
        "\n",
        "all_test = torch.utils.data.DataLoader(\n",
        "    test_dataset, batch_size=train_size, shuffle=False, num_workers=test_size)\n",
        "print(\"TRAIN_SIZE: {}\\n VAL_SIZE: {}\\nTEST_SIZE: {}\".format(train_size, val_size, test_size))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN_SIZE: 900\n",
            " VAL_SIZE: 300\n",
            "TEST_SIZE: 300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJj_d24c-VB7",
        "colab_type": "code",
        "outputId": "52d106d9-248a-46a8-8ebe-e086272710d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "%%time\n",
        "#print('all val data loading ...')\n",
        "#val_data = next(iter(all_val))\n",
        "print('all train data loading ...')\n",
        "train_data = next(iter(all_train))\n",
        "#print('test data loading')\n",
        "#test_data = next(iter(all_test))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all train data loading ...\n",
            "all train data loading ...\n",
            "CPU times: user 243 ms, sys: 3.45 s, total: 3.7 s\n",
            "Wall time: 4min 46s\n",
            "CPU times: user 243 ms, sys: 3.45 s, total: 3.7 s\n",
            "Wall time: 4min 46s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dg3NGt_cbfo",
        "colab_type": "code",
        "outputId": "0aeead30-44b0-43e2-af43-9c2063e35808",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "net.run(train_data[0])\n",
        "#del train_data"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PCANet first stage...\n",
            "images shape torch.Size([900, 1, 64, 64])\n",
            "patches_shape,  torch.Size([900, 9, 4096])\n",
            "should be patches shape,  (900, 9, 4096)\n",
            "filter_size 3\n",
            "pca fitting ...\n",
            "eigenvectors shape: (5, 9)\n",
            "I  torch.Size([900, 5, 62, 62])\n",
            "PCANet second stage...\n",
            "images shape torch.Size([4500, 1, 62, 62])\n",
            "patches_shape,  torch.Size([4500, 64, 3969])\n",
            "should be patches shape,  (4500, 64, 3844)\n",
            "filter_size 8\n",
            "X_SHAPE  torch.Size([64, 17860500])\n",
            "pca fitting ...\n",
            "eigenvectors shape: (10, 64)\n",
            "CPU times: user 8.76 s, sys: 845 ms, total: 9.6 s\n",
            "Wall time: 5.74 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0BnHdqtc9WX",
        "colab_type": "code",
        "outputId": "86acf491-5d6b-4ea9-e607-721fc13ae347",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "print('test data loading')\n",
        "test_data = next(iter(all_test))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test data loading\n",
            "CPU times: user 650 ms, sys: 19.6 s, total: 20.2 s\n",
            "Wall time: 2min 24s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5ZrPROoc_H3",
        "colab_type": "code",
        "outputId": "0b35a314-5385-455a-84a4-dce20e3392f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "print('all val data loading ...')\n",
        "val_data = next(iter(all_val))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all val data loading ...\n",
            "CPU times: user 714 ms, sys: 19.7 s, total: 20.4 s\n",
            "Wall time: 1min 58s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHhiI3-0U3PQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from time import time\n",
        "\n",
        "val_loss = np.zeros(num_epochs)\n",
        "train_loss = np.zeros(num_epochs)\n",
        "\n",
        "val_acc = np.zeros(num_epochs)\n",
        "train_acc = np.zeros(num_epochs)\n",
        "\n",
        "\n",
        "def train_model(model, loss, optimizer, scheduler, num_epochs):\n",
        "    #print('Making filters....')\n",
        "    #t_start = time()\n",
        "    #model.run(train_data[0])\n",
        "    #print('TIME: ', time() - t_start)\n",
        "\n",
        "\n",
        "    model.W_1 = model.W_1.to(device)\n",
        "    model.W_2 = model.W_2.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        print('Epoch {}/{}:'.format(epoch, num_epochs - 1), flush=True)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                data = train_data\n",
        "                model.train()  # training mode\n",
        "                history_acc = train_acc\n",
        "                history_loss = train_loss\n",
        "            else:\n",
        "                data = test_data\n",
        "                model.eval()   # evaluate mode (dropout + bn)\n",
        "                history_acc = val_acc\n",
        "                history_loss = val_loss\n",
        "\n",
        "            running_loss = 0.\n",
        "            running_acc = 0.\n",
        "\n",
        "            # Iterate over data.\n",
        "            inputs, labels = data\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward and backward\n",
        "            with torch.set_grad_enabled(phase=='train'):\n",
        "                preds = model(inputs)\n",
        "                loss_value = loss(preds, labels)\n",
        "                preds_class = preds.argmax(dim=1)\n",
        "\n",
        "                # backward + optimize only if in training phase\n",
        "                if phase == 'train':\n",
        "                    loss_value.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "            # statistics\n",
        "            running_loss += loss_value.cpu().item()\n",
        "            running_acc += (preds_class.cpu() == labels.cpu().data).float().mean()\n",
        "            \n",
        "            epoch_loss = running_loss\n",
        "            epoch_acc = running_acc\n",
        "            history_acc[epoch] = epoch_acc\n",
        "            history_loss[epoch] = epoch_loss\n",
        "\n",
        "            # запоминаем модель по лоссу\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}\\n'.format(phase, epoch_loss, epoch_acc), flush=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYahGgdxPKUM",
        "colab_type": "code",
        "outputId": "3da7a139-17ba-4e43-e6a9-a3a7de87f8f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "train_model(net, loss, optimizer, None, num_epochs=num_epochs)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/499:\n",
            "train Loss: 0.7030 Acc: 0.5000\n",
            "\n",
            "val Loss: 0.7320 Acc: 0.5000\n",
            "\n",
            "Epoch 1/499:\n",
            "train Loss: 0.7296 Acc: 0.5000\n",
            "\n",
            "val Loss: 0.7013 Acc: 0.5000\n",
            "\n",
            "Epoch 2/499:\n",
            "train Loss: 0.7014 Acc: 0.5022\n",
            "\n",
            "val Loss: 0.7002 Acc: 0.5000\n",
            "\n",
            "Epoch 3/499:\n",
            "train Loss: 0.6981 Acc: 0.4978\n",
            "\n",
            "val Loss: 0.7147 Acc: 0.5000\n",
            "\n",
            "Epoch 4/499:\n",
            "train Loss: 0.7082 Acc: 0.5000\n",
            "\n",
            "val Loss: 0.7099 Acc: 0.5000\n",
            "\n",
            "Epoch 5/499:\n",
            "train Loss: 0.7028 Acc: 0.5000\n",
            "\n",
            "val Loss: 0.6985 Acc: 0.5000\n",
            "\n",
            "Epoch 6/499:\n",
            "train Loss: 0.6931 Acc: 0.5144\n",
            "\n",
            "val Loss: 0.6971 Acc: 0.5367\n",
            "\n",
            "Epoch 7/499:\n",
            "train Loss: 0.6927 Acc: 0.5289\n",
            "\n",
            "val Loss: 0.7028 Acc: 0.5000\n",
            "\n",
            "Epoch 8/499:\n",
            "train Loss: 0.6981 Acc: 0.5000\n",
            "\n",
            "val Loss: 0.7036 Acc: 0.5000\n",
            "\n",
            "Epoch 9/499:\n",
            "train Loss: 0.6984 Acc: 0.5000\n",
            "\n",
            "val Loss: 0.6991 Acc: 0.5100\n",
            "\n",
            "Epoch 10/499:\n",
            "train Loss: 0.6933 Acc: 0.5011\n",
            "\n",
            "val Loss: 0.6966 Acc: 0.5000\n",
            "\n",
            "Epoch 11/499:\n",
            "train Loss: 0.6898 Acc: 0.5544\n",
            "\n",
            "val Loss: 0.6992 Acc: 0.5000\n",
            "\n",
            "Epoch 12/499:\n",
            "train Loss: 0.6911 Acc: 0.5156\n",
            "\n",
            "val Loss: 0.7024 Acc: 0.5000\n",
            "\n",
            "Epoch 13/499:\n",
            "train Loss: 0.6934 Acc: 0.4989\n",
            "\n",
            "val Loss: 0.7015 Acc: 0.5000\n",
            "\n",
            "Epoch 14/499:\n",
            "train Loss: 0.6923 Acc: 0.4989\n",
            "\n",
            "val Loss: 0.6982 Acc: 0.5000\n",
            "\n",
            "Epoch 15/499:\n",
            "train Loss: 0.6889 Acc: 0.5167\n",
            "\n",
            "val Loss: 0.6969 Acc: 0.5000\n",
            "\n",
            "Epoch 16/499:\n",
            "train Loss: 0.6871 Acc: 0.5589\n",
            "\n",
            "val Loss: 0.6991 Acc: 0.4500\n",
            "\n",
            "Epoch 17/499:\n",
            "train Loss: 0.6879 Acc: 0.5178\n",
            "\n",
            "val Loss: 0.7014 Acc: 0.5000\n",
            "\n",
            "Epoch 18/499:\n",
            "train Loss: 0.6889 Acc: 0.5433\n",
            "\n",
            "val Loss: 0.7010 Acc: 0.4767\n",
            "\n",
            "Epoch 19/499:\n",
            "train Loss: 0.6879 Acc: 0.5289\n",
            "\n",
            "val Loss: 0.6988 Acc: 0.5000\n",
            "\n",
            "Epoch 20/499:\n",
            "train Loss: 0.6857 Acc: 0.5000\n",
            "\n",
            "val Loss: 0.6975 Acc: 0.5000\n",
            "\n",
            "Epoch 21/499:\n",
            "train Loss: 0.6846 Acc: 0.6033\n",
            "\n",
            "val Loss: 0.6980 Acc: 0.5000\n",
            "\n",
            "Epoch 22/499:\n",
            "train Loss: 0.6851 Acc: 0.5722\n",
            "\n",
            "val Loss: 0.6988 Acc: 0.5400\n",
            "\n",
            "Epoch 23/499:\n",
            "train Loss: 0.6853 Acc: 0.5344\n",
            "\n",
            "val Loss: 0.6985 Acc: 0.5033\n",
            "\n",
            "Epoch 24/499:\n",
            "train Loss: 0.6843 Acc: 0.5689\n",
            "\n",
            "val Loss: 0.6981 Acc: 0.4967\n",
            "\n",
            "Epoch 25/499:\n",
            "train Loss: 0.6827 Acc: 0.5922\n",
            "\n",
            "val Loss: 0.6989 Acc: 0.5000\n",
            "\n",
            "Epoch 26/499:\n",
            "train Loss: 0.6820 Acc: 0.6167\n",
            "\n",
            "val Loss: 0.7005 Acc: 0.5000\n",
            "\n",
            "Epoch 27/499:\n",
            "train Loss: 0.6821 Acc: 0.5167\n",
            "\n",
            "val Loss: 0.7013 Acc: 0.5000\n",
            "\n",
            "Epoch 28/499:\n",
            "train Loss: 0.6819 Acc: 0.5144\n",
            "\n",
            "val Loss: 0.7007 Acc: 0.4700\n",
            "\n",
            "Epoch 29/499:\n",
            "train Loss: 0.6808 Acc: 0.5344\n",
            "\n",
            "val Loss: 0.6997 Acc: 0.5000\n",
            "\n",
            "Epoch 30/499:\n",
            "train Loss: 0.6797 Acc: 0.6278\n",
            "\n",
            "val Loss: 0.6994 Acc: 0.5000\n",
            "\n",
            "Epoch 31/499:\n",
            "train Loss: 0.6793 Acc: 0.6733\n",
            "\n",
            "val Loss: 0.6997 Acc: 0.4767\n",
            "\n",
            "Epoch 32/499:\n",
            "train Loss: 0.6791 Acc: 0.6122\n",
            "\n",
            "val Loss: 0.7000 Acc: 0.4867\n",
            "\n",
            "Epoch 33/499:\n",
            "train Loss: 0.6786 Acc: 0.6267\n",
            "\n",
            "val Loss: 0.7003 Acc: 0.5000\n",
            "\n",
            "Epoch 34/499:\n",
            "train Loss: 0.6776 Acc: 0.6533\n",
            "\n",
            "val Loss: 0.7012 Acc: 0.5000\n",
            "\n",
            "Epoch 35/499:\n",
            "train Loss: 0.6768 Acc: 0.6533\n",
            "\n",
            "val Loss: 0.7026 Acc: 0.5000\n",
            "\n",
            "Epoch 36/499:\n",
            "train Loss: 0.6764 Acc: 0.5722\n",
            "\n",
            "val Loss: 0.7037 Acc: 0.5000\n",
            "\n",
            "Epoch 37/499:\n",
            "train Loss: 0.6760 Acc: 0.6011\n",
            "\n",
            "val Loss: 0.7038 Acc: 0.5000\n",
            "\n",
            "Epoch 38/499:\n",
            "train Loss: 0.6752 Acc: 0.5833\n",
            "\n",
            "val Loss: 0.7034 Acc: 0.5000\n",
            "\n",
            "Epoch 39/499:\n",
            "train Loss: 0.6743 Acc: 0.6433\n",
            "\n",
            "val Loss: 0.7031 Acc: 0.5000\n",
            "\n",
            "Epoch 40/499:\n",
            "train Loss: 0.6738 Acc: 0.7278\n",
            "\n",
            "val Loss: 0.7033 Acc: 0.5167\n",
            "\n",
            "Epoch 41/499:\n",
            "train Loss: 0.6733 Acc: 0.7167\n",
            "\n",
            "val Loss: 0.7038 Acc: 0.5267\n",
            "\n",
            "Epoch 42/499:\n",
            "train Loss: 0.6727 Acc: 0.7244\n",
            "\n",
            "val Loss: 0.7047 Acc: 0.5000\n",
            "\n",
            "Epoch 43/499:\n",
            "train Loss: 0.6719 Acc: 0.7211\n",
            "\n",
            "val Loss: 0.7060 Acc: 0.5000\n",
            "\n",
            "Epoch 44/499:\n",
            "train Loss: 0.6712 Acc: 0.6567\n",
            "\n",
            "val Loss: 0.7074 Acc: 0.5000\n",
            "\n",
            "Epoch 45/499:\n",
            "train Loss: 0.6706 Acc: 0.6544\n",
            "\n",
            "val Loss: 0.7083 Acc: 0.5000\n",
            "\n",
            "Epoch 46/499:\n",
            "train Loss: 0.6701 Acc: 0.6756\n",
            "\n",
            "val Loss: 0.7084 Acc: 0.5000\n",
            "\n",
            "Epoch 47/499:\n",
            "train Loss: 0.6693 Acc: 0.6578\n",
            "\n",
            "val Loss: 0.7082 Acc: 0.5000\n",
            "\n",
            "Epoch 48/499:\n",
            "train Loss: 0.6686 Acc: 0.7200\n",
            "\n",
            "val Loss: 0.7081 Acc: 0.5000\n",
            "\n",
            "Epoch 49/499:\n",
            "train Loss: 0.6680 Acc: 0.7544\n",
            "\n",
            "val Loss: 0.7085 Acc: 0.5000\n",
            "\n",
            "Epoch 50/499:\n",
            "train Loss: 0.6674 Acc: 0.7578\n",
            "\n",
            "val Loss: 0.7093 Acc: 0.5000\n",
            "\n",
            "Epoch 51/499:\n",
            "train Loss: 0.6666 Acc: 0.7700\n",
            "\n",
            "val Loss: 0.7105 Acc: 0.5000\n",
            "\n",
            "Epoch 52/499:\n",
            "train Loss: 0.6659 Acc: 0.7522\n",
            "\n",
            "val Loss: 0.7119 Acc: 0.5000\n",
            "\n",
            "Epoch 53/499:\n",
            "train Loss: 0.6653 Acc: 0.7300\n",
            "\n",
            "val Loss: 0.7131 Acc: 0.5000\n",
            "\n",
            "Epoch 54/499:\n",
            "train Loss: 0.6646 Acc: 0.7022\n",
            "\n",
            "val Loss: 0.7138 Acc: 0.5000\n",
            "\n",
            "Epoch 55/499:\n",
            "train Loss: 0.6639 Acc: 0.7300\n",
            "\n",
            "val Loss: 0.7140 Acc: 0.5000\n",
            "\n",
            "Epoch 56/499:\n",
            "train Loss: 0.6632 Acc: 0.7544\n",
            "\n",
            "val Loss: 0.7143 Acc: 0.5000\n",
            "\n",
            "Epoch 57/499:\n",
            "train Loss: 0.6625 Acc: 0.7989\n",
            "\n",
            "val Loss: 0.7148 Acc: 0.5000\n",
            "\n",
            "Epoch 58/499:\n",
            "train Loss: 0.6618 Acc: 0.8044\n",
            "\n",
            "val Loss: 0.7158 Acc: 0.5000\n",
            "\n",
            "Epoch 59/499:\n",
            "train Loss: 0.6611 Acc: 0.8078\n",
            "\n",
            "val Loss: 0.7173 Acc: 0.5000\n",
            "\n",
            "Epoch 60/499:\n",
            "train Loss: 0.6604 Acc: 0.7956\n",
            "\n",
            "val Loss: 0.7189 Acc: 0.5000\n",
            "\n",
            "Epoch 61/499:\n",
            "train Loss: 0.6597 Acc: 0.7811\n",
            "\n",
            "val Loss: 0.7203 Acc: 0.5000\n",
            "\n",
            "Epoch 62/499:\n",
            "train Loss: 0.6590 Acc: 0.7489\n",
            "\n",
            "val Loss: 0.7213 Acc: 0.5000\n",
            "\n",
            "Epoch 63/499:\n",
            "train Loss: 0.6583 Acc: 0.7622\n",
            "\n",
            "val Loss: 0.7219 Acc: 0.5000\n",
            "\n",
            "Epoch 64/499:\n",
            "train Loss: 0.6576 Acc: 0.8044\n",
            "\n",
            "val Loss: 0.7224 Acc: 0.5000\n",
            "\n",
            "Epoch 65/499:\n",
            "train Loss: 0.6569 Acc: 0.8044\n",
            "\n",
            "val Loss: 0.7232 Acc: 0.5000\n",
            "\n",
            "Epoch 66/499:\n",
            "train Loss: 0.6562 Acc: 0.8067\n",
            "\n",
            "val Loss: 0.7244 Acc: 0.5000\n",
            "\n",
            "Epoch 67/499:\n",
            "train Loss: 0.6554 Acc: 0.8089\n",
            "\n",
            "val Loss: 0.7259 Acc: 0.5000\n",
            "\n",
            "Epoch 68/499:\n",
            "train Loss: 0.6547 Acc: 0.8156\n",
            "\n",
            "val Loss: 0.7276 Acc: 0.5000\n",
            "\n",
            "Epoch 69/499:\n",
            "train Loss: 0.6540 Acc: 0.8122\n",
            "\n",
            "val Loss: 0.7290 Acc: 0.5000\n",
            "\n",
            "Epoch 70/499:\n",
            "train Loss: 0.6533 Acc: 0.8033\n",
            "\n",
            "val Loss: 0.7300 Acc: 0.5000\n",
            "\n",
            "Epoch 71/499:\n",
            "train Loss: 0.6525 Acc: 0.8078\n",
            "\n",
            "val Loss: 0.7308 Acc: 0.5000\n",
            "\n",
            "Epoch 72/499:\n",
            "train Loss: 0.6518 Acc: 0.8267\n",
            "\n",
            "val Loss: 0.7316 Acc: 0.5000\n",
            "\n",
            "Epoch 73/499:\n",
            "train Loss: 0.6511 Acc: 0.8467\n",
            "\n",
            "val Loss: 0.7327 Acc: 0.5000\n",
            "\n",
            "Epoch 74/499:\n",
            "train Loss: 0.6503 Acc: 0.8522\n",
            "\n",
            "val Loss: 0.7342 Acc: 0.5000\n",
            "\n",
            "Epoch 75/499:\n",
            "train Loss: 0.6496 Acc: 0.8500\n",
            "\n",
            "val Loss: 0.7359 Acc: 0.5000\n",
            "\n",
            "Epoch 76/499:\n",
            "train Loss: 0.6489 Acc: 0.8411\n",
            "\n",
            "val Loss: 0.7376 Acc: 0.5000\n",
            "\n",
            "Epoch 77/499:\n",
            "train Loss: 0.6481 Acc: 0.8333\n",
            "\n",
            "val Loss: 0.7391 Acc: 0.5000\n",
            "\n",
            "Epoch 78/499:\n",
            "train Loss: 0.6474 Acc: 0.8344\n",
            "\n",
            "val Loss: 0.7403 Acc: 0.5000\n",
            "\n",
            "Epoch 79/499:\n",
            "train Loss: 0.6466 Acc: 0.8411\n",
            "\n",
            "val Loss: 0.7414 Acc: 0.5000\n",
            "\n",
            "Epoch 80/499:\n",
            "train Loss: 0.6459 Acc: 0.8522\n",
            "\n",
            "val Loss: 0.7426 Acc: 0.5000\n",
            "\n",
            "Epoch 81/499:\n",
            "train Loss: 0.6451 Acc: 0.8544\n",
            "\n",
            "val Loss: 0.7441 Acc: 0.5000\n",
            "\n",
            "Epoch 82/499:\n",
            "train Loss: 0.6444 Acc: 0.8556\n",
            "\n",
            "val Loss: 0.7458 Acc: 0.5000\n",
            "\n",
            "Epoch 83/499:\n",
            "train Loss: 0.6436 Acc: 0.8578\n",
            "\n",
            "val Loss: 0.7477 Acc: 0.5000\n",
            "\n",
            "Epoch 84/499:\n",
            "train Loss: 0.6429 Acc: 0.8522\n",
            "\n",
            "val Loss: 0.7494 Acc: 0.5000\n",
            "\n",
            "Epoch 85/499:\n",
            "train Loss: 0.6421 Acc: 0.8533\n",
            "\n",
            "val Loss: 0.7509 Acc: 0.5000\n",
            "\n",
            "Epoch 86/499:\n",
            "train Loss: 0.6414 Acc: 0.8578\n",
            "\n",
            "val Loss: 0.7522 Acc: 0.5000\n",
            "\n",
            "Epoch 87/499:\n",
            "train Loss: 0.6406 Acc: 0.8667\n",
            "\n",
            "val Loss: 0.7536 Acc: 0.5000\n",
            "\n",
            "Epoch 88/499:\n",
            "train Loss: 0.6399 Acc: 0.8767\n",
            "\n",
            "val Loss: 0.7551 Acc: 0.5000\n",
            "\n",
            "Epoch 89/499:\n",
            "train Loss: 0.6391 Acc: 0.8822\n",
            "\n",
            "val Loss: 0.7569 Acc: 0.5000\n",
            "\n",
            "Epoch 90/499:\n",
            "train Loss: 0.6383 Acc: 0.8833\n",
            "\n",
            "val Loss: 0.7588 Acc: 0.5000\n",
            "\n",
            "Epoch 91/499:\n",
            "train Loss: 0.6376 Acc: 0.8800\n",
            "\n",
            "val Loss: 0.7607 Acc: 0.5000\n",
            "\n",
            "Epoch 92/499:\n",
            "train Loss: 0.6368 Acc: 0.8756\n",
            "\n",
            "val Loss: 0.7624 Acc: 0.5000\n",
            "\n",
            "Epoch 93/499:\n",
            "train Loss: 0.6360 Acc: 0.8778\n",
            "\n",
            "val Loss: 0.7640 Acc: 0.5000\n",
            "\n",
            "Epoch 94/499:\n",
            "train Loss: 0.6353 Acc: 0.8856\n",
            "\n",
            "val Loss: 0.7656 Acc: 0.5000\n",
            "\n",
            "Epoch 95/499:\n",
            "train Loss: 0.6345 Acc: 0.8900\n",
            "\n",
            "val Loss: 0.7673 Acc: 0.5000\n",
            "\n",
            "Epoch 96/499:\n",
            "train Loss: 0.6337 Acc: 0.8922\n",
            "\n",
            "val Loss: 0.7692 Acc: 0.5000\n",
            "\n",
            "Epoch 97/499:\n",
            "train Loss: 0.6330 Acc: 0.8922\n",
            "\n",
            "val Loss: 0.7712 Acc: 0.5000\n",
            "\n",
            "Epoch 98/499:\n",
            "train Loss: 0.6322 Acc: 0.8922\n",
            "\n",
            "val Loss: 0.7733 Acc: 0.5000\n",
            "\n",
            "Epoch 99/499:\n",
            "train Loss: 0.6314 Acc: 0.8944\n",
            "\n",
            "val Loss: 0.7751 Acc: 0.5000\n",
            "\n",
            "Epoch 100/499:\n",
            "train Loss: 0.6307 Acc: 0.8967\n",
            "\n",
            "val Loss: 0.7769 Acc: 0.5000\n",
            "\n",
            "Epoch 101/499:\n",
            "train Loss: 0.6299 Acc: 0.9000\n",
            "\n",
            "val Loss: 0.7787 Acc: 0.5000\n",
            "\n",
            "Epoch 102/499:\n",
            "train Loss: 0.6291 Acc: 0.9056\n",
            "\n",
            "val Loss: 0.7806 Acc: 0.5000\n",
            "\n",
            "Epoch 103/499:\n",
            "train Loss: 0.6284 Acc: 0.9067\n",
            "\n",
            "val Loss: 0.7826 Acc: 0.5000\n",
            "\n",
            "Epoch 104/499:\n",
            "train Loss: 0.6276 Acc: 0.9078\n",
            "\n",
            "val Loss: 0.7847 Acc: 0.5000\n",
            "\n",
            "Epoch 105/499:\n",
            "train Loss: 0.6268 Acc: 0.9089\n",
            "\n",
            "val Loss: 0.7868 Acc: 0.5000\n",
            "\n",
            "Epoch 106/499:\n",
            "train Loss: 0.6260 Acc: 0.9100\n",
            "\n",
            "val Loss: 0.7889 Acc: 0.5000\n",
            "\n",
            "Epoch 107/499:\n",
            "train Loss: 0.6253 Acc: 0.9111\n",
            "\n",
            "val Loss: 0.7909 Acc: 0.5000\n",
            "\n",
            "Epoch 108/499:\n",
            "train Loss: 0.6245 Acc: 0.9111\n",
            "\n",
            "val Loss: 0.7928 Acc: 0.5000\n",
            "\n",
            "Epoch 109/499:\n",
            "train Loss: 0.6237 Acc: 0.9122\n",
            "\n",
            "val Loss: 0.7949 Acc: 0.5000\n",
            "\n",
            "Epoch 110/499:\n",
            "train Loss: 0.6229 Acc: 0.9167\n",
            "\n",
            "val Loss: 0.7971 Acc: 0.5000\n",
            "\n",
            "Epoch 111/499:\n",
            "train Loss: 0.6222 Acc: 0.9156\n",
            "\n",
            "val Loss: 0.7993 Acc: 0.5000\n",
            "\n",
            "Epoch 112/499:\n",
            "train Loss: 0.6214 Acc: 0.9133\n",
            "\n",
            "val Loss: 0.8016 Acc: 0.5000\n",
            "\n",
            "Epoch 113/499:\n",
            "train Loss: 0.6206 Acc: 0.9144\n",
            "\n",
            "val Loss: 0.8038 Acc: 0.5000\n",
            "\n",
            "Epoch 114/499:\n",
            "train Loss: 0.6198 Acc: 0.9144\n",
            "\n",
            "val Loss: 0.8060 Acc: 0.5000\n",
            "\n",
            "Epoch 115/499:\n",
            "train Loss: 0.6191 Acc: 0.9167\n",
            "\n",
            "val Loss: 0.8081 Acc: 0.5000\n",
            "\n",
            "Epoch 116/499:\n",
            "train Loss: 0.6183 Acc: 0.9167\n",
            "\n",
            "val Loss: 0.8104 Acc: 0.5000\n",
            "\n",
            "Epoch 117/499:\n",
            "train Loss: 0.6175 Acc: 0.9167\n",
            "\n",
            "val Loss: 0.8127 Acc: 0.5000\n",
            "\n",
            "Epoch 118/499:\n",
            "train Loss: 0.6167 Acc: 0.9167\n",
            "\n",
            "val Loss: 0.8151 Acc: 0.5000\n",
            "\n",
            "Epoch 119/499:\n",
            "train Loss: 0.6159 Acc: 0.9178\n",
            "\n",
            "val Loss: 0.8174 Acc: 0.5000\n",
            "\n",
            "Epoch 120/499:\n",
            "train Loss: 0.6152 Acc: 0.9200\n",
            "\n",
            "val Loss: 0.8197 Acc: 0.5000\n",
            "\n",
            "Epoch 121/499:\n",
            "train Loss: 0.6144 Acc: 0.9211\n",
            "\n",
            "val Loss: 0.8220 Acc: 0.5000\n",
            "\n",
            "Epoch 122/499:\n",
            "train Loss: 0.6136 Acc: 0.9189\n",
            "\n",
            "val Loss: 0.8243 Acc: 0.5000\n",
            "\n",
            "Epoch 123/499:\n",
            "train Loss: 0.6128 Acc: 0.9211\n",
            "\n",
            "val Loss: 0.8267 Acc: 0.5000\n",
            "\n",
            "Epoch 124/499:\n",
            "train Loss: 0.6121 Acc: 0.9222\n",
            "\n",
            "val Loss: 0.8291 Acc: 0.5000\n",
            "\n",
            "Epoch 125/499:\n",
            "train Loss: 0.6113 Acc: 0.9233\n",
            "\n",
            "val Loss: 0.8316 Acc: 0.5000\n",
            "\n",
            "Epoch 126/499:\n",
            "train Loss: 0.6105 Acc: 0.9222\n",
            "\n",
            "val Loss: 0.8341 Acc: 0.5000\n",
            "\n",
            "Epoch 127/499:\n",
            "train Loss: 0.6097 Acc: 0.9233\n",
            "\n",
            "val Loss: 0.8365 Acc: 0.5000\n",
            "\n",
            "Epoch 128/499:\n",
            "train Loss: 0.6089 Acc: 0.9222\n",
            "\n",
            "val Loss: 0.8390 Acc: 0.5000\n",
            "\n",
            "Epoch 129/499:\n",
            "train Loss: 0.6082 Acc: 0.9244\n",
            "\n",
            "val Loss: 0.8414 Acc: 0.5000\n",
            "\n",
            "Epoch 130/499:\n",
            "train Loss: 0.6074 Acc: 0.9233\n",
            "\n",
            "val Loss: 0.8440 Acc: 0.5000\n",
            "\n",
            "Epoch 131/499:\n",
            "train Loss: 0.6066 Acc: 0.9233\n",
            "\n",
            "val Loss: 0.8465 Acc: 0.5000\n",
            "\n",
            "Epoch 132/499:\n",
            "train Loss: 0.6058 Acc: 0.9244\n",
            "\n",
            "val Loss: 0.8491 Acc: 0.5000\n",
            "\n",
            "Epoch 133/499:\n",
            "train Loss: 0.6050 Acc: 0.9244\n",
            "\n",
            "val Loss: 0.8517 Acc: 0.5000\n",
            "\n",
            "Epoch 134/499:\n",
            "train Loss: 0.6043 Acc: 0.9244\n",
            "\n",
            "val Loss: 0.8542 Acc: 0.5000\n",
            "\n",
            "Epoch 135/499:\n",
            "train Loss: 0.6035 Acc: 0.9244\n",
            "\n",
            "val Loss: 0.8568 Acc: 0.5000\n",
            "\n",
            "Epoch 136/499:\n",
            "train Loss: 0.6027 Acc: 0.9244\n",
            "\n",
            "val Loss: 0.8594 Acc: 0.5000\n",
            "\n",
            "Epoch 137/499:\n",
            "train Loss: 0.6019 Acc: 0.9244\n",
            "\n",
            "val Loss: 0.8621 Acc: 0.5000\n",
            "\n",
            "Epoch 138/499:\n",
            "train Loss: 0.6012 Acc: 0.9256\n",
            "\n",
            "val Loss: 0.8648 Acc: 0.5000\n",
            "\n",
            "Epoch 139/499:\n",
            "train Loss: 0.6004 Acc: 0.9278\n",
            "\n",
            "val Loss: 0.8675 Acc: 0.5000\n",
            "\n",
            "Epoch 140/499:\n",
            "train Loss: 0.5996 Acc: 0.9267\n",
            "\n",
            "val Loss: 0.8701 Acc: 0.5000\n",
            "\n",
            "Epoch 141/499:\n",
            "train Loss: 0.5988 Acc: 0.9267\n",
            "\n",
            "val Loss: 0.8728 Acc: 0.5000\n",
            "\n",
            "Epoch 142/499:\n",
            "train Loss: 0.5981 Acc: 0.9278\n",
            "\n",
            "val Loss: 0.8755 Acc: 0.5000\n",
            "\n",
            "Epoch 143/499:\n",
            "train Loss: 0.5973 Acc: 0.9300\n",
            "\n",
            "val Loss: 0.8783 Acc: 0.5000\n",
            "\n",
            "Epoch 144/499:\n",
            "train Loss: 0.5965 Acc: 0.9311\n",
            "\n",
            "val Loss: 0.8810 Acc: 0.5000\n",
            "\n",
            "Epoch 145/499:\n",
            "train Loss: 0.5957 Acc: 0.9311\n",
            "\n",
            "val Loss: 0.8838 Acc: 0.5000\n",
            "\n",
            "Epoch 146/499:\n",
            "train Loss: 0.5950 Acc: 0.9322\n",
            "\n",
            "val Loss: 0.8866 Acc: 0.5000\n",
            "\n",
            "Epoch 147/499:\n",
            "train Loss: 0.5942 Acc: 0.9322\n",
            "\n",
            "val Loss: 0.8894 Acc: 0.5000\n",
            "\n",
            "Epoch 148/499:\n",
            "train Loss: 0.5934 Acc: 0.9356\n",
            "\n",
            "val Loss: 0.8922 Acc: 0.5000\n",
            "\n",
            "Epoch 149/499:\n",
            "train Loss: 0.5927 Acc: 0.9356\n",
            "\n",
            "val Loss: 0.8950 Acc: 0.5000\n",
            "\n",
            "Epoch 150/499:\n",
            "train Loss: 0.5919 Acc: 0.9356\n",
            "\n",
            "val Loss: 0.8979 Acc: 0.5000\n",
            "\n",
            "Epoch 151/499:\n",
            "train Loss: 0.5911 Acc: 0.9356\n",
            "\n",
            "val Loss: 0.9008 Acc: 0.5000\n",
            "\n",
            "Epoch 152/499:\n",
            "train Loss: 0.5903 Acc: 0.9356\n",
            "\n",
            "val Loss: 0.9036 Acc: 0.5000\n",
            "\n",
            "Epoch 153/499:\n",
            "train Loss: 0.5896 Acc: 0.9378\n",
            "\n",
            "val Loss: 0.9065 Acc: 0.5000\n",
            "\n",
            "Epoch 154/499:\n",
            "train Loss: 0.5888 Acc: 0.9400\n",
            "\n",
            "val Loss: 0.9094 Acc: 0.5000\n",
            "\n",
            "Epoch 155/499:\n",
            "train Loss: 0.5880 Acc: 0.9400\n",
            "\n",
            "val Loss: 0.9123 Acc: 0.5000\n",
            "\n",
            "Epoch 156/499:\n",
            "train Loss: 0.5873 Acc: 0.9411\n",
            "\n",
            "val Loss: 0.9153 Acc: 0.5000\n",
            "\n",
            "Epoch 157/499:\n",
            "train Loss: 0.5865 Acc: 0.9422\n",
            "\n",
            "val Loss: 0.9182 Acc: 0.5000\n",
            "\n",
            "Epoch 158/499:\n",
            "train Loss: 0.5857 Acc: 0.9433\n",
            "\n",
            "val Loss: 0.9212 Acc: 0.5000\n",
            "\n",
            "Epoch 159/499:\n",
            "train Loss: 0.5850 Acc: 0.9422\n",
            "\n",
            "val Loss: 0.9242 Acc: 0.5000\n",
            "\n",
            "Epoch 160/499:\n",
            "train Loss: 0.5842 Acc: 0.9422\n",
            "\n",
            "val Loss: 0.9271 Acc: 0.5000\n",
            "\n",
            "Epoch 161/499:\n",
            "train Loss: 0.5834 Acc: 0.9422\n",
            "\n",
            "val Loss: 0.9301 Acc: 0.5000\n",
            "\n",
            "Epoch 162/499:\n",
            "train Loss: 0.5827 Acc: 0.9433\n",
            "\n",
            "val Loss: 0.9332 Acc: 0.5000\n",
            "\n",
            "Epoch 163/499:\n",
            "train Loss: 0.5819 Acc: 0.9444\n",
            "\n",
            "val Loss: 0.9362 Acc: 0.5000\n",
            "\n",
            "Epoch 164/499:\n",
            "train Loss: 0.5811 Acc: 0.9444\n",
            "\n",
            "val Loss: 0.9392 Acc: 0.5000\n",
            "\n",
            "Epoch 165/499:\n",
            "train Loss: 0.5804 Acc: 0.9444\n",
            "\n",
            "val Loss: 0.9423 Acc: 0.5000\n",
            "\n",
            "Epoch 166/499:\n",
            "train Loss: 0.5796 Acc: 0.9444\n",
            "\n",
            "val Loss: 0.9454 Acc: 0.5000\n",
            "\n",
            "Epoch 167/499:\n",
            "train Loss: 0.5789 Acc: 0.9444\n",
            "\n",
            "val Loss: 0.9484 Acc: 0.5000\n",
            "\n",
            "Epoch 168/499:\n",
            "train Loss: 0.5781 Acc: 0.9456\n",
            "\n",
            "val Loss: 0.9515 Acc: 0.5000\n",
            "\n",
            "Epoch 169/499:\n",
            "train Loss: 0.5773 Acc: 0.9456\n",
            "\n",
            "val Loss: 0.9546 Acc: 0.5000\n",
            "\n",
            "Epoch 170/499:\n",
            "train Loss: 0.5766 Acc: 0.9456\n",
            "\n",
            "val Loss: 0.9578 Acc: 0.5000\n",
            "\n",
            "Epoch 171/499:\n",
            "train Loss: 0.5758 Acc: 0.9467\n",
            "\n",
            "val Loss: 0.9609 Acc: 0.5000\n",
            "\n",
            "Epoch 172/499:\n",
            "train Loss: 0.5751 Acc: 0.9467\n",
            "\n",
            "val Loss: 0.9640 Acc: 0.5000\n",
            "\n",
            "Epoch 173/499:\n",
            "train Loss: 0.5743 Acc: 0.9467\n",
            "\n",
            "val Loss: 0.9672 Acc: 0.5000\n",
            "\n",
            "Epoch 174/499:\n",
            "train Loss: 0.5735 Acc: 0.9467\n",
            "\n",
            "val Loss: 0.9704 Acc: 0.5000\n",
            "\n",
            "Epoch 175/499:\n",
            "train Loss: 0.5728 Acc: 0.9478\n",
            "\n",
            "val Loss: 0.9736 Acc: 0.5000\n",
            "\n",
            "Epoch 176/499:\n",
            "train Loss: 0.5720 Acc: 0.9478\n",
            "\n",
            "val Loss: 0.9768 Acc: 0.5000\n",
            "\n",
            "Epoch 177/499:\n",
            "train Loss: 0.5713 Acc: 0.9478\n",
            "\n",
            "val Loss: 0.9800 Acc: 0.5000\n",
            "\n",
            "Epoch 178/499:\n",
            "train Loss: 0.5705 Acc: 0.9478\n",
            "\n",
            "val Loss: 0.9832 Acc: 0.5000\n",
            "\n",
            "Epoch 179/499:\n",
            "train Loss: 0.5698 Acc: 0.9478\n",
            "\n",
            "val Loss: 0.9864 Acc: 0.5000\n",
            "\n",
            "Epoch 180/499:\n",
            "train Loss: 0.5690 Acc: 0.9478\n",
            "\n",
            "val Loss: 0.9897 Acc: 0.5000\n",
            "\n",
            "Epoch 181/499:\n",
            "train Loss: 0.5683 Acc: 0.9489\n",
            "\n",
            "val Loss: 0.9929 Acc: 0.5000\n",
            "\n",
            "Epoch 182/499:\n",
            "train Loss: 0.5675 Acc: 0.9489\n",
            "\n",
            "val Loss: 0.9962 Acc: 0.5000\n",
            "\n",
            "Epoch 183/499:\n",
            "train Loss: 0.5668 Acc: 0.9489\n",
            "\n",
            "val Loss: 0.9995 Acc: 0.5000\n",
            "\n",
            "Epoch 184/499:\n",
            "train Loss: 0.5660 Acc: 0.9489\n",
            "\n",
            "val Loss: 1.0027 Acc: 0.5000\n",
            "\n",
            "Epoch 185/499:\n",
            "train Loss: 0.5653 Acc: 0.9489\n",
            "\n",
            "val Loss: 1.0060 Acc: 0.5000\n",
            "\n",
            "Epoch 186/499:\n",
            "train Loss: 0.5645 Acc: 0.9489\n",
            "\n",
            "val Loss: 1.0093 Acc: 0.5000\n",
            "\n",
            "Epoch 187/499:\n",
            "train Loss: 0.5638 Acc: 0.9500\n",
            "\n",
            "val Loss: 1.0127 Acc: 0.5000\n",
            "\n",
            "Epoch 188/499:\n",
            "train Loss: 0.5631 Acc: 0.9500\n",
            "\n",
            "val Loss: 1.0160 Acc: 0.5000\n",
            "\n",
            "Epoch 189/499:\n",
            "train Loss: 0.5623 Acc: 0.9500\n",
            "\n",
            "val Loss: 1.0194 Acc: 0.5000\n",
            "\n",
            "Epoch 190/499:\n",
            "train Loss: 0.5616 Acc: 0.9500\n",
            "\n",
            "val Loss: 1.0227 Acc: 0.5000\n",
            "\n",
            "Epoch 191/499:\n",
            "train Loss: 0.5608 Acc: 0.9500\n",
            "\n",
            "val Loss: 1.0261 Acc: 0.5000\n",
            "\n",
            "Epoch 192/499:\n",
            "train Loss: 0.5601 Acc: 0.9500\n",
            "\n",
            "val Loss: 1.0294 Acc: 0.5000\n",
            "\n",
            "Epoch 193/499:\n",
            "train Loss: 0.5593 Acc: 0.9500\n",
            "\n",
            "val Loss: 1.0328 Acc: 0.5000\n",
            "\n",
            "Epoch 194/499:\n",
            "train Loss: 0.5586 Acc: 0.9500\n",
            "\n",
            "val Loss: 1.0362 Acc: 0.5000\n",
            "\n",
            "Epoch 195/499:\n",
            "train Loss: 0.5579 Acc: 0.9500\n",
            "\n",
            "val Loss: 1.0396 Acc: 0.5000\n",
            "\n",
            "Epoch 196/499:\n",
            "train Loss: 0.5571 Acc: 0.9500\n",
            "\n",
            "val Loss: 1.0430 Acc: 0.5000\n",
            "\n",
            "Epoch 197/499:\n",
            "train Loss: 0.5564 Acc: 0.9500\n",
            "\n",
            "val Loss: 1.0465 Acc: 0.5000\n",
            "\n",
            "Epoch 198/499:\n",
            "train Loss: 0.5557 Acc: 0.9511\n",
            "\n",
            "val Loss: 1.0499 Acc: 0.5000\n",
            "\n",
            "Epoch 199/499:\n",
            "train Loss: 0.5549 Acc: 0.9511\n",
            "\n",
            "val Loss: 1.0534 Acc: 0.5000\n",
            "\n",
            "Epoch 200/499:\n",
            "train Loss: 0.5542 Acc: 0.9511\n",
            "\n",
            "val Loss: 1.0568 Acc: 0.5000\n",
            "\n",
            "Epoch 201/499:\n",
            "train Loss: 0.5535 Acc: 0.9511\n",
            "\n",
            "val Loss: 1.0603 Acc: 0.5000\n",
            "\n",
            "Epoch 202/499:\n",
            "train Loss: 0.5527 Acc: 0.9511\n",
            "\n",
            "val Loss: 1.0638 Acc: 0.5000\n",
            "\n",
            "Epoch 203/499:\n",
            "train Loss: 0.5520 Acc: 0.9511\n",
            "\n",
            "val Loss: 1.0672 Acc: 0.5000\n",
            "\n",
            "Epoch 204/499:\n",
            "train Loss: 0.5513 Acc: 0.9511\n",
            "\n",
            "val Loss: 1.0707 Acc: 0.5000\n",
            "\n",
            "Epoch 205/499:\n",
            "train Loss: 0.5505 Acc: 0.9511\n",
            "\n",
            "val Loss: 1.0742 Acc: 0.5000\n",
            "\n",
            "Epoch 206/499:\n",
            "train Loss: 0.5498 Acc: 0.9522\n",
            "\n",
            "val Loss: 1.0778 Acc: 0.5000\n",
            "\n",
            "Epoch 207/499:\n",
            "train Loss: 0.5491 Acc: 0.9533\n",
            "\n",
            "val Loss: 1.0813 Acc: 0.5000\n",
            "\n",
            "Epoch 208/499:\n",
            "train Loss: 0.5484 Acc: 0.9533\n",
            "\n",
            "val Loss: 1.0848 Acc: 0.5000\n",
            "\n",
            "Epoch 209/499:\n",
            "train Loss: 0.5476 Acc: 0.9533\n",
            "\n",
            "val Loss: 1.0883 Acc: 0.5000\n",
            "\n",
            "Epoch 210/499:\n",
            "train Loss: 0.5469 Acc: 0.9533\n",
            "\n",
            "val Loss: 1.0919 Acc: 0.5000\n",
            "\n",
            "Epoch 211/499:\n",
            "train Loss: 0.5462 Acc: 0.9533\n",
            "\n",
            "val Loss: 1.0955 Acc: 0.5000\n",
            "\n",
            "Epoch 212/499:\n",
            "train Loss: 0.5455 Acc: 0.9533\n",
            "\n",
            "val Loss: 1.0990 Acc: 0.5000\n",
            "\n",
            "Epoch 213/499:\n",
            "train Loss: 0.5448 Acc: 0.9544\n",
            "\n",
            "val Loss: 1.1026 Acc: 0.5000\n",
            "\n",
            "Epoch 214/499:\n",
            "train Loss: 0.5440 Acc: 0.9556\n",
            "\n",
            "val Loss: 1.1062 Acc: 0.5000\n",
            "\n",
            "Epoch 215/499:\n",
            "train Loss: 0.5433 Acc: 0.9544\n",
            "\n",
            "val Loss: 1.1098 Acc: 0.5000\n",
            "\n",
            "Epoch 216/499:\n",
            "train Loss: 0.5426 Acc: 0.9544\n",
            "\n",
            "val Loss: 1.1134 Acc: 0.5000\n",
            "\n",
            "Epoch 217/499:\n",
            "train Loss: 0.5419 Acc: 0.9544\n",
            "\n",
            "val Loss: 1.1170 Acc: 0.5000\n",
            "\n",
            "Epoch 218/499:\n",
            "train Loss: 0.5412 Acc: 0.9544\n",
            "\n",
            "val Loss: 1.1206 Acc: 0.5000\n",
            "\n",
            "Epoch 219/499:\n",
            "train Loss: 0.5405 Acc: 0.9544\n",
            "\n",
            "val Loss: 1.1242 Acc: 0.5000\n",
            "\n",
            "Epoch 220/499:\n",
            "train Loss: 0.5397 Acc: 0.9544\n",
            "\n",
            "val Loss: 1.1279 Acc: 0.5000\n",
            "\n",
            "Epoch 221/499:\n",
            "train Loss: 0.5390 Acc: 0.9544\n",
            "\n",
            "val Loss: 1.1315 Acc: 0.5000\n",
            "\n",
            "Epoch 222/499:\n",
            "train Loss: 0.5383 Acc: 0.9544\n",
            "\n",
            "val Loss: 1.1352 Acc: 0.5000\n",
            "\n",
            "Epoch 223/499:\n",
            "train Loss: 0.5376 Acc: 0.9544\n",
            "\n",
            "val Loss: 1.1388 Acc: 0.5000\n",
            "\n",
            "Epoch 224/499:\n",
            "train Loss: 0.5369 Acc: 0.9544\n",
            "\n",
            "val Loss: 1.1425 Acc: 0.5000\n",
            "\n",
            "Epoch 225/499:\n",
            "train Loss: 0.5362 Acc: 0.9544\n",
            "\n",
            "val Loss: 1.1462 Acc: 0.5000\n",
            "\n",
            "Epoch 226/499:\n",
            "train Loss: 0.5355 Acc: 0.9544\n",
            "\n",
            "val Loss: 1.1499 Acc: 0.5000\n",
            "\n",
            "Epoch 227/499:\n",
            "train Loss: 0.5348 Acc: 0.9544\n",
            "\n",
            "val Loss: 1.1536 Acc: 0.5000\n",
            "\n",
            "Epoch 228/499:\n",
            "train Loss: 0.5341 Acc: 0.9544\n",
            "\n",
            "val Loss: 1.1573 Acc: 0.5000\n",
            "\n",
            "Epoch 229/499:\n",
            "train Loss: 0.5334 Acc: 0.9544\n",
            "\n",
            "val Loss: 1.1610 Acc: 0.5000\n",
            "\n",
            "Epoch 230/499:\n",
            "train Loss: 0.5327 Acc: 0.9544\n",
            "\n",
            "val Loss: 1.1647 Acc: 0.5000\n",
            "\n",
            "Epoch 231/499:\n",
            "train Loss: 0.5320 Acc: 0.9544\n",
            "\n",
            "val Loss: 1.1684 Acc: 0.5000\n",
            "\n",
            "Epoch 232/499:\n",
            "train Loss: 0.5313 Acc: 0.9544\n",
            "\n",
            "val Loss: 1.1721 Acc: 0.5000\n",
            "\n",
            "Epoch 233/499:\n",
            "train Loss: 0.5306 Acc: 0.9544\n",
            "\n",
            "val Loss: 1.1759 Acc: 0.5000\n",
            "\n",
            "Epoch 234/499:\n",
            "train Loss: 0.5299 Acc: 0.9544\n",
            "\n",
            "val Loss: 1.1796 Acc: 0.5000\n",
            "\n",
            "Epoch 235/499:\n",
            "train Loss: 0.5292 Acc: 0.9544\n",
            "\n",
            "val Loss: 1.1834 Acc: 0.5000\n",
            "\n",
            "Epoch 236/499:\n",
            "train Loss: 0.5285 Acc: 0.9544\n",
            "\n",
            "val Loss: 1.1871 Acc: 0.5000\n",
            "\n",
            "Epoch 237/499:\n",
            "train Loss: 0.5278 Acc: 0.9556\n",
            "\n",
            "val Loss: 1.1909 Acc: 0.5000\n",
            "\n",
            "Epoch 238/499:\n",
            "train Loss: 0.5271 Acc: 0.9556\n",
            "\n",
            "val Loss: 1.1947 Acc: 0.5000\n",
            "\n",
            "Epoch 239/499:\n",
            "train Loss: 0.5264 Acc: 0.9556\n",
            "\n",
            "val Loss: 1.1984 Acc: 0.5000\n",
            "\n",
            "Epoch 240/499:\n",
            "train Loss: 0.5257 Acc: 0.9556\n",
            "\n",
            "val Loss: 1.2022 Acc: 0.5000\n",
            "\n",
            "Epoch 241/499:\n",
            "train Loss: 0.5250 Acc: 0.9556\n",
            "\n",
            "val Loss: 1.2060 Acc: 0.5000\n",
            "\n",
            "Epoch 242/499:\n",
            "train Loss: 0.5243 Acc: 0.9544\n",
            "\n",
            "val Loss: 1.2098 Acc: 0.5000\n",
            "\n",
            "Epoch 243/499:\n",
            "train Loss: 0.5236 Acc: 0.9556\n",
            "\n",
            "val Loss: 1.2136 Acc: 0.5000\n",
            "\n",
            "Epoch 244/499:\n",
            "train Loss: 0.5229 Acc: 0.9556\n",
            "\n",
            "val Loss: 1.2174 Acc: 0.5000\n",
            "\n",
            "Epoch 245/499:\n",
            "train Loss: 0.5223 Acc: 0.9567\n",
            "\n",
            "val Loss: 1.2213 Acc: 0.5000\n",
            "\n",
            "Epoch 246/499:\n",
            "train Loss: 0.5216 Acc: 0.9567\n",
            "\n",
            "val Loss: 1.2251 Acc: 0.5000\n",
            "\n",
            "Epoch 247/499:\n",
            "train Loss: 0.5209 Acc: 0.9567\n",
            "\n",
            "val Loss: 1.2289 Acc: 0.5000\n",
            "\n",
            "Epoch 248/499:\n",
            "train Loss: 0.5202 Acc: 0.9567\n",
            "\n",
            "val Loss: 1.2328 Acc: 0.5000\n",
            "\n",
            "Epoch 249/499:\n",
            "train Loss: 0.5195 Acc: 0.9567\n",
            "\n",
            "val Loss: 1.2366 Acc: 0.5000\n",
            "\n",
            "Epoch 250/499:\n",
            "train Loss: 0.5188 Acc: 0.9567\n",
            "\n",
            "val Loss: 1.2405 Acc: 0.5000\n",
            "\n",
            "Epoch 251/499:\n",
            "train Loss: 0.5182 Acc: 0.9567\n",
            "\n",
            "val Loss: 1.2443 Acc: 0.5000\n",
            "\n",
            "Epoch 252/499:\n",
            "train Loss: 0.5175 Acc: 0.9578\n",
            "\n",
            "val Loss: 1.2482 Acc: 0.5000\n",
            "\n",
            "Epoch 253/499:\n",
            "train Loss: 0.5168 Acc: 0.9578\n",
            "\n",
            "val Loss: 1.2521 Acc: 0.5000\n",
            "\n",
            "Epoch 254/499:\n",
            "train Loss: 0.5161 Acc: 0.9578\n",
            "\n",
            "val Loss: 1.2559 Acc: 0.5000\n",
            "\n",
            "Epoch 255/499:\n",
            "train Loss: 0.5155 Acc: 0.9578\n",
            "\n",
            "val Loss: 1.2598 Acc: 0.5000\n",
            "\n",
            "Epoch 256/499:\n",
            "train Loss: 0.5148 Acc: 0.9589\n",
            "\n",
            "val Loss: 1.2637 Acc: 0.5000\n",
            "\n",
            "Epoch 257/499:\n",
            "train Loss: 0.5141 Acc: 0.9589\n",
            "\n",
            "val Loss: 1.2676 Acc: 0.5000\n",
            "\n",
            "Epoch 258/499:\n",
            "train Loss: 0.5134 Acc: 0.9589\n",
            "\n",
            "val Loss: 1.2715 Acc: 0.5000\n",
            "\n",
            "Epoch 259/499:\n",
            "train Loss: 0.5128 Acc: 0.9589\n",
            "\n",
            "val Loss: 1.2754 Acc: 0.5000\n",
            "\n",
            "Epoch 260/499:\n",
            "train Loss: 0.5121 Acc: 0.9589\n",
            "\n",
            "val Loss: 1.2793 Acc: 0.5000\n",
            "\n",
            "Epoch 261/499:\n",
            "train Loss: 0.5114 Acc: 0.9589\n",
            "\n",
            "val Loss: 1.2833 Acc: 0.5000\n",
            "\n",
            "Epoch 262/499:\n",
            "train Loss: 0.5108 Acc: 0.9589\n",
            "\n",
            "val Loss: 1.2872 Acc: 0.5000\n",
            "\n",
            "Epoch 263/499:\n",
            "train Loss: 0.5101 Acc: 0.9589\n",
            "\n",
            "val Loss: 1.2911 Acc: 0.5000\n",
            "\n",
            "Epoch 264/499:\n",
            "train Loss: 0.5094 Acc: 0.9589\n",
            "\n",
            "val Loss: 1.2950 Acc: 0.5000\n",
            "\n",
            "Epoch 265/499:\n",
            "train Loss: 0.5088 Acc: 0.9589\n",
            "\n",
            "val Loss: 1.2990 Acc: 0.5000\n",
            "\n",
            "Epoch 266/499:\n",
            "train Loss: 0.5081 Acc: 0.9589\n",
            "\n",
            "val Loss: 1.3029 Acc: 0.5000\n",
            "\n",
            "Epoch 267/499:\n",
            "train Loss: 0.5074 Acc: 0.9589\n",
            "\n",
            "val Loss: 1.3069 Acc: 0.5000\n",
            "\n",
            "Epoch 268/499:\n",
            "train Loss: 0.5068 Acc: 0.9589\n",
            "\n",
            "val Loss: 1.3108 Acc: 0.5000\n",
            "\n",
            "Epoch 269/499:\n",
            "train Loss: 0.5061 Acc: 0.9589\n",
            "\n",
            "val Loss: 1.3148 Acc: 0.5000\n",
            "\n",
            "Epoch 270/499:\n",
            "train Loss: 0.5055 Acc: 0.9600\n",
            "\n",
            "val Loss: 1.3188 Acc: 0.5000\n",
            "\n",
            "Epoch 271/499:\n",
            "train Loss: 0.5048 Acc: 0.9600\n",
            "\n",
            "val Loss: 1.3227 Acc: 0.5000\n",
            "\n",
            "Epoch 272/499:\n",
            "train Loss: 0.5041 Acc: 0.9600\n",
            "\n",
            "val Loss: 1.3267 Acc: 0.5000\n",
            "\n",
            "Epoch 273/499:\n",
            "train Loss: 0.5035 Acc: 0.9600\n",
            "\n",
            "val Loss: 1.3307 Acc: 0.5000\n",
            "\n",
            "Epoch 274/499:\n",
            "train Loss: 0.5028 Acc: 0.9600\n",
            "\n",
            "val Loss: 1.3347 Acc: 0.5000\n",
            "\n",
            "Epoch 275/499:\n",
            "train Loss: 0.5022 Acc: 0.9600\n",
            "\n",
            "val Loss: 1.3387 Acc: 0.5000\n",
            "\n",
            "Epoch 276/499:\n",
            "train Loss: 0.5015 Acc: 0.9600\n",
            "\n",
            "val Loss: 1.3427 Acc: 0.5000\n",
            "\n",
            "Epoch 277/499:\n",
            "train Loss: 0.5009 Acc: 0.9600\n",
            "\n",
            "val Loss: 1.3467 Acc: 0.5000\n",
            "\n",
            "Epoch 278/499:\n",
            "train Loss: 0.5002 Acc: 0.9600\n",
            "\n",
            "val Loss: 1.3507 Acc: 0.5000\n",
            "\n",
            "Epoch 279/499:\n",
            "train Loss: 0.4996 Acc: 0.9600\n",
            "\n",
            "val Loss: 1.3547 Acc: 0.5000\n",
            "\n",
            "Epoch 280/499:\n",
            "train Loss: 0.4989 Acc: 0.9611\n",
            "\n",
            "val Loss: 1.3587 Acc: 0.5000\n",
            "\n",
            "Epoch 281/499:\n",
            "train Loss: 0.4983 Acc: 0.9611\n",
            "\n",
            "val Loss: 1.3627 Acc: 0.5000\n",
            "\n",
            "Epoch 282/499:\n",
            "train Loss: 0.4976 Acc: 0.9611\n",
            "\n",
            "val Loss: 1.3668 Acc: 0.5000\n",
            "\n",
            "Epoch 283/499:\n",
            "train Loss: 0.4970 Acc: 0.9611\n",
            "\n",
            "val Loss: 1.3708 Acc: 0.5000\n",
            "\n",
            "Epoch 284/499:\n",
            "train Loss: 0.4963 Acc: 0.9611\n",
            "\n",
            "val Loss: 1.3748 Acc: 0.5000\n",
            "\n",
            "Epoch 285/499:\n",
            "train Loss: 0.4957 Acc: 0.9611\n",
            "\n",
            "val Loss: 1.3789 Acc: 0.5000\n",
            "\n",
            "Epoch 286/499:\n",
            "train Loss: 0.4951 Acc: 0.9611\n",
            "\n",
            "val Loss: 1.3829 Acc: 0.5000\n",
            "\n",
            "Epoch 287/499:\n",
            "train Loss: 0.4944 Acc: 0.9611\n",
            "\n",
            "val Loss: 1.3869 Acc: 0.5000\n",
            "\n",
            "Epoch 288/499:\n",
            "train Loss: 0.4938 Acc: 0.9611\n",
            "\n",
            "val Loss: 1.3910 Acc: 0.5000\n",
            "\n",
            "Epoch 289/499:\n",
            "train Loss: 0.4931 Acc: 0.9611\n",
            "\n",
            "val Loss: 1.3950 Acc: 0.5000\n",
            "\n",
            "Epoch 290/499:\n",
            "train Loss: 0.4925 Acc: 0.9600\n",
            "\n",
            "val Loss: 1.3991 Acc: 0.5000\n",
            "\n",
            "Epoch 291/499:\n",
            "train Loss: 0.4919 Acc: 0.9600\n",
            "\n",
            "val Loss: 1.4032 Acc: 0.5000\n",
            "\n",
            "Epoch 292/499:\n",
            "train Loss: 0.4912 Acc: 0.9600\n",
            "\n",
            "val Loss: 1.4072 Acc: 0.5000\n",
            "\n",
            "Epoch 293/499:\n",
            "train Loss: 0.4906 Acc: 0.9600\n",
            "\n",
            "val Loss: 1.4113 Acc: 0.5000\n",
            "\n",
            "Epoch 294/499:\n",
            "train Loss: 0.4900 Acc: 0.9600\n",
            "\n",
            "val Loss: 1.4154 Acc: 0.5000\n",
            "\n",
            "Epoch 295/499:\n",
            "train Loss: 0.4893 Acc: 0.9611\n",
            "\n",
            "val Loss: 1.4195 Acc: 0.5000\n",
            "\n",
            "Epoch 296/499:\n",
            "train Loss: 0.4887 Acc: 0.9600\n",
            "\n",
            "val Loss: 1.4235 Acc: 0.5000\n",
            "\n",
            "Epoch 297/499:\n",
            "train Loss: 0.4881 Acc: 0.9600\n",
            "\n",
            "val Loss: 1.4276 Acc: 0.5000\n",
            "\n",
            "Epoch 298/499:\n",
            "train Loss: 0.4875 Acc: 0.9600\n",
            "\n",
            "val Loss: 1.4317 Acc: 0.5000\n",
            "\n",
            "Epoch 299/499:\n",
            "train Loss: 0.4868 Acc: 0.9600\n",
            "\n",
            "val Loss: 1.4358 Acc: 0.5000\n",
            "\n",
            "Epoch 300/499:\n",
            "train Loss: 0.4862 Acc: 0.9600\n",
            "\n",
            "val Loss: 1.4399 Acc: 0.5000\n",
            "\n",
            "Epoch 301/499:\n",
            "train Loss: 0.4856 Acc: 0.9600\n",
            "\n",
            "val Loss: 1.4440 Acc: 0.5000\n",
            "\n",
            "Epoch 302/499:\n",
            "train Loss: 0.4850 Acc: 0.9600\n",
            "\n",
            "val Loss: 1.4481 Acc: 0.5000\n",
            "\n",
            "Epoch 303/499:\n",
            "train Loss: 0.4843 Acc: 0.9600\n",
            "\n",
            "val Loss: 1.4522 Acc: 0.5000\n",
            "\n",
            "Epoch 304/499:\n",
            "train Loss: 0.4837 Acc: 0.9600\n",
            "\n",
            "val Loss: 1.4563 Acc: 0.5000\n",
            "\n",
            "Epoch 305/499:\n",
            "train Loss: 0.4831 Acc: 0.9600\n",
            "\n",
            "val Loss: 1.4605 Acc: 0.5000\n",
            "\n",
            "Epoch 306/499:\n",
            "train Loss: 0.4825 Acc: 0.9600\n",
            "\n",
            "val Loss: 1.4646 Acc: 0.5000\n",
            "\n",
            "Epoch 307/499:\n",
            "train Loss: 0.4819 Acc: 0.9600\n",
            "\n",
            "val Loss: 1.4687 Acc: 0.5000\n",
            "\n",
            "Epoch 308/499:\n",
            "train Loss: 0.4812 Acc: 0.9600\n",
            "\n",
            "val Loss: 1.4728 Acc: 0.5000\n",
            "\n",
            "Epoch 309/499:\n",
            "train Loss: 0.4806 Acc: 0.9600\n",
            "\n",
            "val Loss: 1.4770 Acc: 0.5000\n",
            "\n",
            "Epoch 310/499:\n",
            "train Loss: 0.4800 Acc: 0.9600\n",
            "\n",
            "val Loss: 1.4811 Acc: 0.5000\n",
            "\n",
            "Epoch 311/499:\n",
            "train Loss: 0.4794 Acc: 0.9600\n",
            "\n",
            "val Loss: 1.4852 Acc: 0.5000\n",
            "\n",
            "Epoch 312/499:\n",
            "train Loss: 0.4788 Acc: 0.9600\n",
            "\n",
            "val Loss: 1.4894 Acc: 0.5000\n",
            "\n",
            "Epoch 313/499:\n",
            "train Loss: 0.4782 Acc: 0.9600\n",
            "\n",
            "val Loss: 1.4935 Acc: 0.5000\n",
            "\n",
            "Epoch 314/499:\n",
            "train Loss: 0.4776 Acc: 0.9600\n",
            "\n",
            "val Loss: 1.4977 Acc: 0.5000\n",
            "\n",
            "Epoch 315/499:\n",
            "train Loss: 0.4770 Acc: 0.9600\n",
            "\n",
            "val Loss: 1.5018 Acc: 0.5000\n",
            "\n",
            "Epoch 316/499:\n",
            "train Loss: 0.4763 Acc: 0.9600\n",
            "\n",
            "val Loss: 1.5060 Acc: 0.5000\n",
            "\n",
            "Epoch 317/499:\n",
            "train Loss: 0.4757 Acc: 0.9600\n",
            "\n",
            "val Loss: 1.5101 Acc: 0.5000\n",
            "\n",
            "Epoch 318/499:\n",
            "train Loss: 0.4751 Acc: 0.9600\n",
            "\n",
            "val Loss: 1.5143 Acc: 0.5000\n",
            "\n",
            "Epoch 319/499:\n",
            "train Loss: 0.4745 Acc: 0.9600\n",
            "\n",
            "val Loss: 1.5184 Acc: 0.5000\n",
            "\n",
            "Epoch 320/499:\n",
            "train Loss: 0.4739 Acc: 0.9611\n",
            "\n",
            "val Loss: 1.5226 Acc: 0.5000\n",
            "\n",
            "Epoch 321/499:\n",
            "train Loss: 0.4733 Acc: 0.9611\n",
            "\n",
            "val Loss: 1.5268 Acc: 0.5000\n",
            "\n",
            "Epoch 322/499:\n",
            "train Loss: 0.4727 Acc: 0.9611\n",
            "\n",
            "val Loss: 1.5309 Acc: 0.5000\n",
            "\n",
            "Epoch 323/499:\n",
            "train Loss: 0.4721 Acc: 0.9622\n",
            "\n",
            "val Loss: 1.5351 Acc: 0.5000\n",
            "\n",
            "Epoch 324/499:\n",
            "train Loss: 0.4715 Acc: 0.9622\n",
            "\n",
            "val Loss: 1.5393 Acc: 0.5000\n",
            "\n",
            "Epoch 325/499:\n",
            "train Loss: 0.4709 Acc: 0.9622\n",
            "\n",
            "val Loss: 1.5435 Acc: 0.5000\n",
            "\n",
            "Epoch 326/499:\n",
            "train Loss: 0.4703 Acc: 0.9622\n",
            "\n",
            "val Loss: 1.5476 Acc: 0.5000\n",
            "\n",
            "Epoch 327/499:\n",
            "train Loss: 0.4697 Acc: 0.9622\n",
            "\n",
            "val Loss: 1.5518 Acc: 0.5000\n",
            "\n",
            "Epoch 328/499:\n",
            "train Loss: 0.4691 Acc: 0.9622\n",
            "\n",
            "val Loss: 1.5560 Acc: 0.5000\n",
            "\n",
            "Epoch 329/499:\n",
            "train Loss: 0.4685 Acc: 0.9633\n",
            "\n",
            "val Loss: 1.5602 Acc: 0.5000\n",
            "\n",
            "Epoch 330/499:\n",
            "train Loss: 0.4679 Acc: 0.9633\n",
            "\n",
            "val Loss: 1.5644 Acc: 0.5000\n",
            "\n",
            "Epoch 331/499:\n",
            "train Loss: 0.4674 Acc: 0.9633\n",
            "\n",
            "val Loss: 1.5686 Acc: 0.5000\n",
            "\n",
            "Epoch 332/499:\n",
            "train Loss: 0.4668 Acc: 0.9633\n",
            "\n",
            "val Loss: 1.5728 Acc: 0.5000\n",
            "\n",
            "Epoch 333/499:\n",
            "train Loss: 0.4662 Acc: 0.9633\n",
            "\n",
            "val Loss: 1.5770 Acc: 0.5000\n",
            "\n",
            "Epoch 334/499:\n",
            "train Loss: 0.4656 Acc: 0.9633\n",
            "\n",
            "val Loss: 1.5812 Acc: 0.5000\n",
            "\n",
            "Epoch 335/499:\n",
            "train Loss: 0.4650 Acc: 0.9633\n",
            "\n",
            "val Loss: 1.5854 Acc: 0.5000\n",
            "\n",
            "Epoch 336/499:\n",
            "train Loss: 0.4644 Acc: 0.9633\n",
            "\n",
            "val Loss: 1.5896 Acc: 0.5000\n",
            "\n",
            "Epoch 337/499:\n",
            "train Loss: 0.4638 Acc: 0.9633\n",
            "\n",
            "val Loss: 1.5938 Acc: 0.5000\n",
            "\n",
            "Epoch 338/499:\n",
            "train Loss: 0.4632 Acc: 0.9633\n",
            "\n",
            "val Loss: 1.5980 Acc: 0.5000\n",
            "\n",
            "Epoch 339/499:\n",
            "train Loss: 0.4627 Acc: 0.9633\n",
            "\n",
            "val Loss: 1.6022 Acc: 0.5000\n",
            "\n",
            "Epoch 340/499:\n",
            "train Loss: 0.4621 Acc: 0.9633\n",
            "\n",
            "val Loss: 1.6064 Acc: 0.5000\n",
            "\n",
            "Epoch 341/499:\n",
            "train Loss: 0.4615 Acc: 0.9622\n",
            "\n",
            "val Loss: 1.6106 Acc: 0.5000\n",
            "\n",
            "Epoch 342/499:\n",
            "train Loss: 0.4609 Acc: 0.9622\n",
            "\n",
            "val Loss: 1.6148 Acc: 0.5000\n",
            "\n",
            "Epoch 343/499:\n",
            "train Loss: 0.4603 Acc: 0.9622\n",
            "\n",
            "val Loss: 1.6191 Acc: 0.5000\n",
            "\n",
            "Epoch 344/499:\n",
            "train Loss: 0.4597 Acc: 0.9633\n",
            "\n",
            "val Loss: 1.6233 Acc: 0.5000\n",
            "\n",
            "Epoch 345/499:\n",
            "train Loss: 0.4592 Acc: 0.9633\n",
            "\n",
            "val Loss: 1.6275 Acc: 0.5000\n",
            "\n",
            "Epoch 346/499:\n",
            "train Loss: 0.4586 Acc: 0.9633\n",
            "\n",
            "val Loss: 1.6317 Acc: 0.5000\n",
            "\n",
            "Epoch 347/499:\n",
            "train Loss: 0.4580 Acc: 0.9633\n",
            "\n",
            "val Loss: 1.6360 Acc: 0.5000\n",
            "\n",
            "Epoch 348/499:\n",
            "train Loss: 0.4574 Acc: 0.9633\n",
            "\n",
            "val Loss: 1.6402 Acc: 0.5000\n",
            "\n",
            "Epoch 349/499:\n",
            "train Loss: 0.4569 Acc: 0.9633\n",
            "\n",
            "val Loss: 1.6444 Acc: 0.5000\n",
            "\n",
            "Epoch 350/499:\n",
            "train Loss: 0.4563 Acc: 0.9633\n",
            "\n",
            "val Loss: 1.6486 Acc: 0.5000\n",
            "\n",
            "Epoch 351/499:\n",
            "train Loss: 0.4557 Acc: 0.9633\n",
            "\n",
            "val Loss: 1.6529 Acc: 0.5000\n",
            "\n",
            "Epoch 352/499:\n",
            "train Loss: 0.4552 Acc: 0.9633\n",
            "\n",
            "val Loss: 1.6571 Acc: 0.5000\n",
            "\n",
            "Epoch 353/499:\n",
            "train Loss: 0.4546 Acc: 0.9644\n",
            "\n",
            "val Loss: 1.6614 Acc: 0.5000\n",
            "\n",
            "Epoch 354/499:\n",
            "train Loss: 0.4540 Acc: 0.9644\n",
            "\n",
            "val Loss: 1.6656 Acc: 0.5000\n",
            "\n",
            "Epoch 355/499:\n",
            "train Loss: 0.4535 Acc: 0.9644\n",
            "\n",
            "val Loss: 1.6698 Acc: 0.5000\n",
            "\n",
            "Epoch 356/499:\n",
            "train Loss: 0.4529 Acc: 0.9644\n",
            "\n",
            "val Loss: 1.6741 Acc: 0.5000\n",
            "\n",
            "Epoch 357/499:\n",
            "train Loss: 0.4523 Acc: 0.9644\n",
            "\n",
            "val Loss: 1.6783 Acc: 0.5000\n",
            "\n",
            "Epoch 358/499:\n",
            "train Loss: 0.4518 Acc: 0.9644\n",
            "\n",
            "val Loss: 1.6826 Acc: 0.5000\n",
            "\n",
            "Epoch 359/499:\n",
            "train Loss: 0.4512 Acc: 0.9644\n",
            "\n",
            "val Loss: 1.6868 Acc: 0.5000\n",
            "\n",
            "Epoch 360/499:\n",
            "train Loss: 0.4506 Acc: 0.9644\n",
            "\n",
            "val Loss: 1.6911 Acc: 0.5000\n",
            "\n",
            "Epoch 361/499:\n",
            "train Loss: 0.4501 Acc: 0.9644\n",
            "\n",
            "val Loss: 1.6953 Acc: 0.5000\n",
            "\n",
            "Epoch 362/499:\n",
            "train Loss: 0.4495 Acc: 0.9644\n",
            "\n",
            "val Loss: 1.6996 Acc: 0.5000\n",
            "\n",
            "Epoch 363/499:\n",
            "train Loss: 0.4490 Acc: 0.9644\n",
            "\n",
            "val Loss: 1.7038 Acc: 0.5000\n",
            "\n",
            "Epoch 364/499:\n",
            "train Loss: 0.4484 Acc: 0.9644\n",
            "\n",
            "val Loss: 1.7081 Acc: 0.5000\n",
            "\n",
            "Epoch 365/499:\n",
            "train Loss: 0.4478 Acc: 0.9644\n",
            "\n",
            "val Loss: 1.7123 Acc: 0.5000\n",
            "\n",
            "Epoch 366/499:\n",
            "train Loss: 0.4473 Acc: 0.9644\n",
            "\n",
            "val Loss: 1.7166 Acc: 0.5000\n",
            "\n",
            "Epoch 367/499:\n",
            "train Loss: 0.4467 Acc: 0.9644\n",
            "\n",
            "val Loss: 1.7208 Acc: 0.5000\n",
            "\n",
            "Epoch 368/499:\n",
            "train Loss: 0.4462 Acc: 0.9644\n",
            "\n",
            "val Loss: 1.7251 Acc: 0.5000\n",
            "\n",
            "Epoch 369/499:\n",
            "train Loss: 0.4456 Acc: 0.9644\n",
            "\n",
            "val Loss: 1.7294 Acc: 0.5000\n",
            "\n",
            "Epoch 370/499:\n",
            "train Loss: 0.4451 Acc: 0.9644\n",
            "\n",
            "val Loss: 1.7336 Acc: 0.5000\n",
            "\n",
            "Epoch 371/499:\n",
            "train Loss: 0.4445 Acc: 0.9644\n",
            "\n",
            "val Loss: 1.7379 Acc: 0.5000\n",
            "\n",
            "Epoch 372/499:\n",
            "train Loss: 0.4440 Acc: 0.9644\n",
            "\n",
            "val Loss: 1.7422 Acc: 0.5000\n",
            "\n",
            "Epoch 373/499:\n",
            "train Loss: 0.4434 Acc: 0.9656\n",
            "\n",
            "val Loss: 1.7464 Acc: 0.5000\n",
            "\n",
            "Epoch 374/499:\n",
            "train Loss: 0.4429 Acc: 0.9656\n",
            "\n",
            "val Loss: 1.7507 Acc: 0.5000\n",
            "\n",
            "Epoch 375/499:\n",
            "train Loss: 0.4423 Acc: 0.9656\n",
            "\n",
            "val Loss: 1.7550 Acc: 0.5000\n",
            "\n",
            "Epoch 376/499:\n",
            "train Loss: 0.4418 Acc: 0.9656\n",
            "\n",
            "val Loss: 1.7592 Acc: 0.5000\n",
            "\n",
            "Epoch 377/499:\n",
            "train Loss: 0.4412 Acc: 0.9656\n",
            "\n",
            "val Loss: 1.7635 Acc: 0.5000\n",
            "\n",
            "Epoch 378/499:\n",
            "train Loss: 0.4407 Acc: 0.9656\n",
            "\n",
            "val Loss: 1.7678 Acc: 0.5000\n",
            "\n",
            "Epoch 379/499:\n",
            "train Loss: 0.4402 Acc: 0.9656\n",
            "\n",
            "val Loss: 1.7721 Acc: 0.5000\n",
            "\n",
            "Epoch 380/499:\n",
            "train Loss: 0.4396 Acc: 0.9656\n",
            "\n",
            "val Loss: 1.7763 Acc: 0.5000\n",
            "\n",
            "Epoch 381/499:\n",
            "train Loss: 0.4391 Acc: 0.9656\n",
            "\n",
            "val Loss: 1.7806 Acc: 0.5000\n",
            "\n",
            "Epoch 382/499:\n",
            "train Loss: 0.4385 Acc: 0.9656\n",
            "\n",
            "val Loss: 1.7849 Acc: 0.5000\n",
            "\n",
            "Epoch 383/499:\n",
            "train Loss: 0.4380 Acc: 0.9656\n",
            "\n",
            "val Loss: 1.7892 Acc: 0.5000\n",
            "\n",
            "Epoch 384/499:\n",
            "train Loss: 0.4375 Acc: 0.9656\n",
            "\n",
            "val Loss: 1.7934 Acc: 0.5000\n",
            "\n",
            "Epoch 385/499:\n",
            "train Loss: 0.4369 Acc: 0.9656\n",
            "\n",
            "val Loss: 1.7977 Acc: 0.5000\n",
            "\n",
            "Epoch 386/499:\n",
            "train Loss: 0.4364 Acc: 0.9656\n",
            "\n",
            "val Loss: 1.8020 Acc: 0.5000\n",
            "\n",
            "Epoch 387/499:\n",
            "train Loss: 0.4359 Acc: 0.9656\n",
            "\n",
            "val Loss: 1.8063 Acc: 0.5000\n",
            "\n",
            "Epoch 388/499:\n",
            "train Loss: 0.4353 Acc: 0.9656\n",
            "\n",
            "val Loss: 1.8106 Acc: 0.5000\n",
            "\n",
            "Epoch 389/499:\n",
            "train Loss: 0.4348 Acc: 0.9656\n",
            "\n",
            "val Loss: 1.8148 Acc: 0.5000\n",
            "\n",
            "Epoch 390/499:\n",
            "train Loss: 0.4343 Acc: 0.9656\n",
            "\n",
            "val Loss: 1.8191 Acc: 0.5000\n",
            "\n",
            "Epoch 391/499:\n",
            "train Loss: 0.4337 Acc: 0.9656\n",
            "\n",
            "val Loss: 1.8234 Acc: 0.5000\n",
            "\n",
            "Epoch 392/499:\n",
            "train Loss: 0.4332 Acc: 0.9656\n",
            "\n",
            "val Loss: 1.8277 Acc: 0.5000\n",
            "\n",
            "Epoch 393/499:\n",
            "train Loss: 0.4327 Acc: 0.9656\n",
            "\n",
            "val Loss: 1.8320 Acc: 0.5000\n",
            "\n",
            "Epoch 394/499:\n",
            "train Loss: 0.4321 Acc: 0.9656\n",
            "\n",
            "val Loss: 1.8363 Acc: 0.5000\n",
            "\n",
            "Epoch 395/499:\n",
            "train Loss: 0.4316 Acc: 0.9656\n",
            "\n",
            "val Loss: 1.8405 Acc: 0.5000\n",
            "\n",
            "Epoch 396/499:\n",
            "train Loss: 0.4311 Acc: 0.9656\n",
            "\n",
            "val Loss: 1.8448 Acc: 0.5000\n",
            "\n",
            "Epoch 397/499:\n",
            "train Loss: 0.4306 Acc: 0.9656\n",
            "\n",
            "val Loss: 1.8491 Acc: 0.5000\n",
            "\n",
            "Epoch 398/499:\n",
            "train Loss: 0.4300 Acc: 0.9656\n",
            "\n",
            "val Loss: 1.8534 Acc: 0.5000\n",
            "\n",
            "Epoch 399/499:\n",
            "train Loss: 0.4295 Acc: 0.9656\n",
            "\n",
            "val Loss: 1.8577 Acc: 0.5000\n",
            "\n",
            "Epoch 400/499:\n",
            "train Loss: 0.4290 Acc: 0.9656\n",
            "\n",
            "val Loss: 1.8620 Acc: 0.5000\n",
            "\n",
            "Epoch 401/499:\n",
            "train Loss: 0.4285 Acc: 0.9656\n",
            "\n",
            "val Loss: 1.8663 Acc: 0.5000\n",
            "\n",
            "Epoch 402/499:\n",
            "train Loss: 0.4279 Acc: 0.9656\n",
            "\n",
            "val Loss: 1.8706 Acc: 0.5000\n",
            "\n",
            "Epoch 403/499:\n",
            "train Loss: 0.4274 Acc: 0.9656\n",
            "\n",
            "val Loss: 1.8749 Acc: 0.5000\n",
            "\n",
            "Epoch 404/499:\n",
            "train Loss: 0.4269 Acc: 0.9656\n",
            "\n",
            "val Loss: 1.8791 Acc: 0.5000\n",
            "\n",
            "Epoch 405/499:\n",
            "train Loss: 0.4264 Acc: 0.9656\n",
            "\n",
            "val Loss: 1.8834 Acc: 0.5000\n",
            "\n",
            "Epoch 406/499:\n",
            "train Loss: 0.4259 Acc: 0.9656\n",
            "\n",
            "val Loss: 1.8877 Acc: 0.5000\n",
            "\n",
            "Epoch 407/499:\n",
            "train Loss: 0.4254 Acc: 0.9656\n",
            "\n",
            "val Loss: 1.8920 Acc: 0.5000\n",
            "\n",
            "Epoch 408/499:\n",
            "train Loss: 0.4248 Acc: 0.9656\n",
            "\n",
            "val Loss: 1.8963 Acc: 0.5000\n",
            "\n",
            "Epoch 409/499:\n",
            "train Loss: 0.4243 Acc: 0.9656\n",
            "\n",
            "val Loss: 1.9006 Acc: 0.5000\n",
            "\n",
            "Epoch 410/499:\n",
            "train Loss: 0.4238 Acc: 0.9656\n",
            "\n",
            "val Loss: 1.9049 Acc: 0.5000\n",
            "\n",
            "Epoch 411/499:\n",
            "train Loss: 0.4233 Acc: 0.9656\n",
            "\n",
            "val Loss: 1.9092 Acc: 0.5000\n",
            "\n",
            "Epoch 412/499:\n",
            "train Loss: 0.4228 Acc: 0.9656\n",
            "\n",
            "val Loss: 1.9135 Acc: 0.5000\n",
            "\n",
            "Epoch 413/499:\n",
            "train Loss: 0.4223 Acc: 0.9656\n",
            "\n",
            "val Loss: 1.9178 Acc: 0.5000\n",
            "\n",
            "Epoch 414/499:\n",
            "train Loss: 0.4218 Acc: 0.9656\n",
            "\n",
            "val Loss: 1.9221 Acc: 0.5000\n",
            "\n",
            "Epoch 415/499:\n",
            "train Loss: 0.4213 Acc: 0.9656\n",
            "\n",
            "val Loss: 1.9264 Acc: 0.5000\n",
            "\n",
            "Epoch 416/499:\n",
            "train Loss: 0.4208 Acc: 0.9656\n",
            "\n",
            "val Loss: 1.9307 Acc: 0.5000\n",
            "\n",
            "Epoch 417/499:\n",
            "train Loss: 0.4203 Acc: 0.9656\n",
            "\n",
            "val Loss: 1.9350 Acc: 0.5000\n",
            "\n",
            "Epoch 418/499:\n",
            "train Loss: 0.4198 Acc: 0.9656\n",
            "\n",
            "val Loss: 1.9393 Acc: 0.5000\n",
            "\n",
            "Epoch 419/499:\n",
            "train Loss: 0.4193 Acc: 0.9656\n",
            "\n",
            "val Loss: 1.9436 Acc: 0.5000\n",
            "\n",
            "Epoch 420/499:\n",
            "train Loss: 0.4187 Acc: 0.9656\n",
            "\n",
            "val Loss: 1.9479 Acc: 0.5000\n",
            "\n",
            "Epoch 421/499:\n",
            "train Loss: 0.4182 Acc: 0.9656\n",
            "\n",
            "val Loss: 1.9522 Acc: 0.5000\n",
            "\n",
            "Epoch 422/499:\n",
            "train Loss: 0.4177 Acc: 0.9656\n",
            "\n",
            "val Loss: 1.9565 Acc: 0.5000\n",
            "\n",
            "Epoch 423/499:\n",
            "train Loss: 0.4172 Acc: 0.9656\n",
            "\n",
            "val Loss: 1.9608 Acc: 0.5000\n",
            "\n",
            "Epoch 424/499:\n",
            "train Loss: 0.4167 Acc: 0.9656\n",
            "\n",
            "val Loss: 1.9650 Acc: 0.5000\n",
            "\n",
            "Epoch 425/499:\n",
            "train Loss: 0.4162 Acc: 0.9667\n",
            "\n",
            "val Loss: 1.9693 Acc: 0.5000\n",
            "\n",
            "Epoch 426/499:\n",
            "train Loss: 0.4157 Acc: 0.9667\n",
            "\n",
            "val Loss: 1.9736 Acc: 0.5000\n",
            "\n",
            "Epoch 427/499:\n",
            "train Loss: 0.4153 Acc: 0.9667\n",
            "\n",
            "val Loss: 1.9779 Acc: 0.5000\n",
            "\n",
            "Epoch 428/499:\n",
            "train Loss: 0.4148 Acc: 0.9667\n",
            "\n",
            "val Loss: 1.9822 Acc: 0.5000\n",
            "\n",
            "Epoch 429/499:\n",
            "train Loss: 0.4143 Acc: 0.9667\n",
            "\n",
            "val Loss: 1.9865 Acc: 0.5000\n",
            "\n",
            "Epoch 430/499:\n",
            "train Loss: 0.4138 Acc: 0.9667\n",
            "\n",
            "val Loss: 1.9908 Acc: 0.5000\n",
            "\n",
            "Epoch 431/499:\n",
            "train Loss: 0.4133 Acc: 0.9667\n",
            "\n",
            "val Loss: 1.9951 Acc: 0.5000\n",
            "\n",
            "Epoch 432/499:\n",
            "train Loss: 0.4128 Acc: 0.9667\n",
            "\n",
            "val Loss: 1.9994 Acc: 0.5000\n",
            "\n",
            "Epoch 433/499:\n",
            "train Loss: 0.4123 Acc: 0.9678\n",
            "\n",
            "val Loss: 2.0037 Acc: 0.5000\n",
            "\n",
            "Epoch 434/499:\n",
            "train Loss: 0.4118 Acc: 0.9678\n",
            "\n",
            "val Loss: 2.0080 Acc: 0.5000\n",
            "\n",
            "Epoch 435/499:\n",
            "train Loss: 0.4113 Acc: 0.9678\n",
            "\n",
            "val Loss: 2.0123 Acc: 0.5000\n",
            "\n",
            "Epoch 436/499:\n",
            "train Loss: 0.4108 Acc: 0.9678\n",
            "\n",
            "val Loss: 2.0166 Acc: 0.5000\n",
            "\n",
            "Epoch 437/499:\n",
            "train Loss: 0.4103 Acc: 0.9678\n",
            "\n",
            "val Loss: 2.0209 Acc: 0.5000\n",
            "\n",
            "Epoch 438/499:\n",
            "train Loss: 0.4098 Acc: 0.9678\n",
            "\n",
            "val Loss: 2.0252 Acc: 0.5000\n",
            "\n",
            "Epoch 439/499:\n",
            "train Loss: 0.4094 Acc: 0.9678\n",
            "\n",
            "val Loss: 2.0295 Acc: 0.5000\n",
            "\n",
            "Epoch 440/499:\n",
            "train Loss: 0.4089 Acc: 0.9678\n",
            "\n",
            "val Loss: 2.0338 Acc: 0.5000\n",
            "\n",
            "Epoch 441/499:\n",
            "train Loss: 0.4084 Acc: 0.9678\n",
            "\n",
            "val Loss: 2.0381 Acc: 0.5000\n",
            "\n",
            "Epoch 442/499:\n",
            "train Loss: 0.4079 Acc: 0.9678\n",
            "\n",
            "val Loss: 2.0424 Acc: 0.5000\n",
            "\n",
            "Epoch 443/499:\n",
            "train Loss: 0.4074 Acc: 0.9689\n",
            "\n",
            "val Loss: 2.0467 Acc: 0.5000\n",
            "\n",
            "Epoch 444/499:\n",
            "train Loss: 0.4069 Acc: 0.9689\n",
            "\n",
            "val Loss: 2.0510 Acc: 0.5000\n",
            "\n",
            "Epoch 445/499:\n",
            "train Loss: 0.4065 Acc: 0.9689\n",
            "\n",
            "val Loss: 2.0553 Acc: 0.5000\n",
            "\n",
            "Epoch 446/499:\n",
            "train Loss: 0.4060 Acc: 0.9689\n",
            "\n",
            "val Loss: 2.0596 Acc: 0.5000\n",
            "\n",
            "Epoch 447/499:\n",
            "train Loss: 0.4055 Acc: 0.9689\n",
            "\n",
            "val Loss: 2.0639 Acc: 0.5000\n",
            "\n",
            "Epoch 448/499:\n",
            "train Loss: 0.4050 Acc: 0.9689\n",
            "\n",
            "val Loss: 2.0682 Acc: 0.5000\n",
            "\n",
            "Epoch 449/499:\n",
            "train Loss: 0.4045 Acc: 0.9689\n",
            "\n",
            "val Loss: 2.0725 Acc: 0.5000\n",
            "\n",
            "Epoch 450/499:\n",
            "train Loss: 0.4041 Acc: 0.9689\n",
            "\n",
            "val Loss: 2.0768 Acc: 0.5000\n",
            "\n",
            "Epoch 451/499:\n",
            "train Loss: 0.4036 Acc: 0.9689\n",
            "\n",
            "val Loss: 2.0811 Acc: 0.5000\n",
            "\n",
            "Epoch 452/499:\n",
            "train Loss: 0.4031 Acc: 0.9689\n",
            "\n",
            "val Loss: 2.0854 Acc: 0.5000\n",
            "\n",
            "Epoch 453/499:\n",
            "train Loss: 0.4026 Acc: 0.9689\n",
            "\n",
            "val Loss: 2.0896 Acc: 0.5000\n",
            "\n",
            "Epoch 454/499:\n",
            "train Loss: 0.4022 Acc: 0.9689\n",
            "\n",
            "val Loss: 2.0939 Acc: 0.5000\n",
            "\n",
            "Epoch 455/499:\n",
            "train Loss: 0.4017 Acc: 0.9689\n",
            "\n",
            "val Loss: 2.0982 Acc: 0.5000\n",
            "\n",
            "Epoch 456/499:\n",
            "train Loss: 0.4012 Acc: 0.9689\n",
            "\n",
            "val Loss: 2.1025 Acc: 0.5000\n",
            "\n",
            "Epoch 457/499:\n",
            "train Loss: 0.4008 Acc: 0.9689\n",
            "\n",
            "val Loss: 2.1068 Acc: 0.5000\n",
            "\n",
            "Epoch 458/499:\n",
            "train Loss: 0.4003 Acc: 0.9689\n",
            "\n",
            "val Loss: 2.1111 Acc: 0.5000\n",
            "\n",
            "Epoch 459/499:\n",
            "train Loss: 0.3998 Acc: 0.9689\n",
            "\n",
            "val Loss: 2.1154 Acc: 0.5000\n",
            "\n",
            "Epoch 460/499:\n",
            "train Loss: 0.3994 Acc: 0.9689\n",
            "\n",
            "val Loss: 2.1197 Acc: 0.5000\n",
            "\n",
            "Epoch 461/499:\n",
            "train Loss: 0.3989 Acc: 0.9689\n",
            "\n",
            "val Loss: 2.1240 Acc: 0.5000\n",
            "\n",
            "Epoch 462/499:\n",
            "train Loss: 0.3984 Acc: 0.9689\n",
            "\n",
            "val Loss: 2.1283 Acc: 0.5000\n",
            "\n",
            "Epoch 463/499:\n",
            "train Loss: 0.3980 Acc: 0.9689\n",
            "\n",
            "val Loss: 2.1326 Acc: 0.5000\n",
            "\n",
            "Epoch 464/499:\n",
            "train Loss: 0.3975 Acc: 0.9689\n",
            "\n",
            "val Loss: 2.1369 Acc: 0.5000\n",
            "\n",
            "Epoch 465/499:\n",
            "train Loss: 0.3970 Acc: 0.9689\n",
            "\n",
            "val Loss: 2.1411 Acc: 0.5000\n",
            "\n",
            "Epoch 466/499:\n",
            "train Loss: 0.3966 Acc: 0.9689\n",
            "\n",
            "val Loss: 2.1454 Acc: 0.5000\n",
            "\n",
            "Epoch 467/499:\n",
            "train Loss: 0.3961 Acc: 0.9689\n",
            "\n",
            "val Loss: 2.1497 Acc: 0.5000\n",
            "\n",
            "Epoch 468/499:\n",
            "train Loss: 0.3956 Acc: 0.9689\n",
            "\n",
            "val Loss: 2.1540 Acc: 0.5000\n",
            "\n",
            "Epoch 469/499:\n",
            "train Loss: 0.3952 Acc: 0.9689\n",
            "\n",
            "val Loss: 2.1583 Acc: 0.5000\n",
            "\n",
            "Epoch 470/499:\n",
            "train Loss: 0.3947 Acc: 0.9689\n",
            "\n",
            "val Loss: 2.1626 Acc: 0.5000\n",
            "\n",
            "Epoch 471/499:\n",
            "train Loss: 0.3943 Acc: 0.9689\n",
            "\n",
            "val Loss: 2.1669 Acc: 0.5000\n",
            "\n",
            "Epoch 472/499:\n",
            "train Loss: 0.3938 Acc: 0.9689\n",
            "\n",
            "val Loss: 2.1712 Acc: 0.5000\n",
            "\n",
            "Epoch 473/499:\n",
            "train Loss: 0.3934 Acc: 0.9689\n",
            "\n",
            "val Loss: 2.1754 Acc: 0.5000\n",
            "\n",
            "Epoch 474/499:\n",
            "train Loss: 0.3929 Acc: 0.9689\n",
            "\n",
            "val Loss: 2.1797 Acc: 0.5000\n",
            "\n",
            "Epoch 475/499:\n",
            "train Loss: 0.3924 Acc: 0.9689\n",
            "\n",
            "val Loss: 2.1840 Acc: 0.5000\n",
            "\n",
            "Epoch 476/499:\n",
            "train Loss: 0.3920 Acc: 0.9689\n",
            "\n",
            "val Loss: 2.1883 Acc: 0.5000\n",
            "\n",
            "Epoch 477/499:\n",
            "train Loss: 0.3915 Acc: 0.9689\n",
            "\n",
            "val Loss: 2.1926 Acc: 0.5000\n",
            "\n",
            "Epoch 478/499:\n",
            "train Loss: 0.3911 Acc: 0.9689\n",
            "\n",
            "val Loss: 2.1969 Acc: 0.5000\n",
            "\n",
            "Epoch 479/499:\n",
            "train Loss: 0.3906 Acc: 0.9689\n",
            "\n",
            "val Loss: 2.2011 Acc: 0.5000\n",
            "\n",
            "Epoch 480/499:\n",
            "train Loss: 0.3902 Acc: 0.9689\n",
            "\n",
            "val Loss: 2.2054 Acc: 0.5000\n",
            "\n",
            "Epoch 481/499:\n",
            "train Loss: 0.3897 Acc: 0.9689\n",
            "\n",
            "val Loss: 2.2097 Acc: 0.5000\n",
            "\n",
            "Epoch 482/499:\n",
            "train Loss: 0.3893 Acc: 0.9689\n",
            "\n",
            "val Loss: 2.2140 Acc: 0.5000\n",
            "\n",
            "Epoch 483/499:\n",
            "train Loss: 0.3888 Acc: 0.9689\n",
            "\n",
            "val Loss: 2.2183 Acc: 0.5000\n",
            "\n",
            "Epoch 484/499:\n",
            "train Loss: 0.3884 Acc: 0.9689\n",
            "\n",
            "val Loss: 2.2225 Acc: 0.5000\n",
            "\n",
            "Epoch 485/499:\n",
            "train Loss: 0.3879 Acc: 0.9689\n",
            "\n",
            "val Loss: 2.2268 Acc: 0.5000\n",
            "\n",
            "Epoch 486/499:\n",
            "train Loss: 0.3875 Acc: 0.9689\n",
            "\n",
            "val Loss: 2.2311 Acc: 0.5000\n",
            "\n",
            "Epoch 487/499:\n",
            "train Loss: 0.3871 Acc: 0.9689\n",
            "\n",
            "val Loss: 2.2354 Acc: 0.5000\n",
            "\n",
            "Epoch 488/499:\n",
            "train Loss: 0.3866 Acc: 0.9689\n",
            "\n",
            "val Loss: 2.2397 Acc: 0.5000\n",
            "\n",
            "Epoch 489/499:\n",
            "train Loss: 0.3862 Acc: 0.9689\n",
            "\n",
            "val Loss: 2.2439 Acc: 0.5000\n",
            "\n",
            "Epoch 490/499:\n",
            "train Loss: 0.3857 Acc: 0.9689\n",
            "\n",
            "val Loss: 2.2482 Acc: 0.5000\n",
            "\n",
            "Epoch 491/499:\n",
            "train Loss: 0.3853 Acc: 0.9700\n",
            "\n",
            "val Loss: 2.2525 Acc: 0.5000\n",
            "\n",
            "Epoch 492/499:\n",
            "train Loss: 0.3848 Acc: 0.9700\n",
            "\n",
            "val Loss: 2.2568 Acc: 0.5000\n",
            "\n",
            "Epoch 493/499:\n",
            "train Loss: 0.3844 Acc: 0.9700\n",
            "\n",
            "val Loss: 2.2610 Acc: 0.5000\n",
            "\n",
            "Epoch 494/499:\n",
            "train Loss: 0.3840 Acc: 0.9700\n",
            "\n",
            "val Loss: 2.2653 Acc: 0.5000\n",
            "\n",
            "Epoch 495/499:\n",
            "train Loss: 0.3835 Acc: 0.9700\n",
            "\n",
            "val Loss: 2.2696 Acc: 0.5000\n",
            "\n",
            "Epoch 496/499:\n",
            "train Loss: 0.3831 Acc: 0.9700\n",
            "\n",
            "val Loss: 2.2738 Acc: 0.5000\n",
            "\n",
            "Epoch 497/499:\n",
            "train Loss: 0.3826 Acc: 0.9700\n",
            "\n",
            "val Loss: 2.2781 Acc: 0.5000\n",
            "\n",
            "Epoch 498/499:\n",
            "train Loss: 0.3822 Acc: 0.9700\n",
            "\n",
            "val Loss: 2.2824 Acc: 0.5000\n",
            "\n",
            "Epoch 499/499:\n",
            "train Loss: 0.3818 Acc: 0.9700\n",
            "\n",
            "val Loss: 2.2866 Acc: 0.5000\n",
            "\n",
            "CPU times: user 8.97 s, sys: 3.63 s, total: 12.6 s\n",
            "Wall time: 11.8 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OK38TywePKUU",
        "colab_type": "code",
        "outputId": "aedcc05c-5e6c-4aac-924f-99433821b3b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "# Loss\n",
        "plt.plot(range(num_epochs), train_loss, val_loss)\n",
        "plt.legend(['train', 'val'])\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('cross entropy loss')\n",
        "plt.title('loss')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'loss')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oE2UJmSmPKUX",
        "colab_type": "code",
        "outputId": "bd726356-fab2-4051-965d-51d02d10ae0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "# accuracy\n",
        "plt.plot(range(num_epochs), train_acc, val_acc)\n",
        "plt.legend(['train', 'val'])\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.title('accuracy')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'accuracy')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEqB1t96GI-O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f47q_lMXGJEv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}